{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0f96336",
   "metadata": {},
   "source": [
    "# Исследование гибридных методов решения симметричной задачи коммивояжера для оптимизации подбора параметров технологической системы\n",
    "Студент группы ВМ-223: Баринов Даниил Сергеевич"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ce3140",
   "metadata": {},
   "source": [
    "## Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa13ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import networkx as nx\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "import os\n",
    "\n",
    "import signal\n",
    "import gzip\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Optional, Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac54b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45421459",
   "metadata": {},
   "source": [
    "## Логирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59568800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_logger(level='INFO', logfile=None):\n",
    "    \"\"\"\n",
    "    Configures the logger with the specified log level and log file.\n",
    "\n",
    "    Args:\n",
    "        level (str): Log level ('DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL').\n",
    "        logfile (str): Path to the log file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"console\")\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    if not logger.handlers:\n",
    "        if not logfile:\n",
    "            console_handler = logging.StreamHandler()\n",
    "            console_handler.setLevel(level)\n",
    "            logger.addHandler(console_handler)\n",
    "        else:\n",
    "            file_handler = logging.FileHandler(logfile)\n",
    "            file_handler.setLevel(level)\n",
    "            logger.addHandler(file_handler)\n",
    "\n",
    "\n",
    "def log(message, level='INFO'):\n",
    "    \"\"\"\n",
    "    Logs a message with the specified log level.\n",
    "\n",
    "    Args:\n",
    "        message (str): Message to be logged.\n",
    "        level (str): Log level ('DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(\"console\")\n",
    "\n",
    "    match level:\n",
    "        case 'DEBUG':\n",
    "            logger.debug(message)\n",
    "        case 'INFO':\n",
    "            logger.info(message)\n",
    "        case 'WARNING':\n",
    "            logger.warning(message)\n",
    "        case 'ERROR':\n",
    "            logger.error(message)\n",
    "        case 'CRITICAL':\n",
    "            logger.critical(message)\n",
    "        case _:\n",
    "            logger.debug(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc9e29c",
   "metadata": {},
   "source": [
    "## Конфигурация бенчмарков"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f735d0",
   "metadata": {},
   "source": [
    "Handler для таймаутов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f7014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeoutException(Exception):   # Custom exception class\n",
    "    pass\n",
    "\n",
    "def handle_alarm(signal_number, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "signal.signal(signal.SIGALRM, handle_alarm)\n",
    "\n",
    "signal_time = 300\n",
    "\n",
    "times_info = {}\n",
    "shortest_paths = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1fc1c7",
   "metadata": {},
   "source": [
    "TSP Benchmark класс для TSPLIB и гибридных методов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d07599",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSPBenchmark:\n",
    "    def __init__(self, data_dir: str = \"tsp_data\"):\n",
    "        \"\"\"\n",
    "        Initialize the TSP benchmark environment.\n",
    "\n",
    "        Args:\n",
    "            data_dir: Directory to store/load the TSP datasets\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.data_dir.mkdir(exist_ok=True)\n",
    "        self.instances = {}  # Maps instance name to metadata\n",
    "        self.optimal_tours = {}  # Maps instance name to optimal tour\n",
    "\n",
    "    def download_instance(self, instance_name: str, url: str) -> bool:\n",
    "        \"\"\"\n",
    "        Download a TSPLIB instance if not already present.\n",
    "        Handles both plain files and gzipped files (.gz).\n",
    "\n",
    "        Args:\n",
    "            instance_name: Name of the instance (e.g., 'a280')\n",
    "            url: Base URL to download from (will be modified for .gz files)\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        instance_dir = self.data_dir / instance_name\n",
    "        instance_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        tsp_file = instance_dir / f\"{instance_name}.tsp\"\n",
    "        opt_file = instance_dir / f\"{instance_name}.opt.tour\"\n",
    "\n",
    "        # Handle gzipped files\n",
    "        gz_url = url + \".gz\"  # Append .gz to the URL\n",
    "\n",
    "        # Download .tsp file if not exists\n",
    "        if not tsp_file.exists():\n",
    "            try:\n",
    "                print(f\"Downloading {instance_name}.tsp.gz...\")\n",
    "                # Download to a temporary file\n",
    "                temp_gz_file = instance_dir / f\"{instance_name}.tsp.gz\"\n",
    "                urllib.request.urlretrieve(gz_url, temp_gz_file)\n",
    "\n",
    "                # Extract the gzipped file\n",
    "                with gzip.open(temp_gz_file, 'rb') as f_in:\n",
    "                    with open(tsp_file, 'wb') as f_out:\n",
    "                        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "                # Remove the temporary file\n",
    "                temp_gz_file.unlink()\n",
    "                print(f\"Successfully downloaded and extracted {instance_name}.tsp\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {gz_url}: {e}\")\n",
    "\n",
    "                # Try without .gz extension as fallback\n",
    "                try:\n",
    "                    print(f\"Trying without .gz extension...\")\n",
    "                    urllib.request.urlretrieve(url, tsp_file)\n",
    "                    print(f\"Successfully downloaded {instance_name}.tsp\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"Error downloading {url}: {e2}\")\n",
    "                    return False\n",
    "\n",
    "        # Try to download optimal tour if available\n",
    "        opt_url = url.replace(\".tsp\", \".opt.tour\") + \".gz\"\n",
    "        if not opt_file.exists():\n",
    "            try:\n",
    "                print(f\"Downloading {instance_name}.opt.tour.gz...\")\n",
    "                # Download to a temporary file\n",
    "                temp_gz_file = instance_dir / f\"{instance_name}.opt.tour.gz\"\n",
    "                urllib.request.urlretrieve(opt_url, temp_gz_file)\n",
    "\n",
    "                # Extract the gzipped file\n",
    "                with gzip.open(temp_gz_file, 'rb') as f_in:\n",
    "                    with open(opt_file, 'wb') as f_out:\n",
    "                        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "                # Remove the temporary file\n",
    "                temp_gz_file.unlink()\n",
    "                print(f\"Successfully downloaded and extracted optimal tour for {instance_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Optimal tour not available for {instance_name}: {e}\")\n",
    "\n",
    "                # Try without .gz extension as fallback\n",
    "                try:\n",
    "                    print(f\"Trying without .gz extension...\")\n",
    "                    opt_url_plain = url.replace(\".tsp\", \".opt.tour\")\n",
    "                    urllib.request.urlretrieve(opt_url_plain, opt_file)\n",
    "                    print(f\"Successfully downloaded optimal tour for {instance_name}\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"Optimal tour not available (tried plain file): {e2}\")\n",
    "                    # This is not a fatal error, as some instances might not have published optimal tours\n",
    "\n",
    "        return True\n",
    "\n",
    "    def download_tsplib_set(self, size_range: Tuple[int, int] = (100, 1000)) -> None:\n",
    "        \"\"\"\n",
    "        Download a set of TSPLIB instances within a size range.\n",
    "\n",
    "        Args:\n",
    "            size_range: Tuple of (min_cities, max_cities)\n",
    "        \"\"\"\n",
    "        # TSPLIB index URL\n",
    "        tsplib_index = \"http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/tsp/\"\n",
    "\n",
    "        # Download a few popular instances\n",
    "        instances = {\n",
    "            \"berlin52\": \"http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/tsp/berlin52.tsp\",\n",
    "            \"eil101\": \"http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/tsp/eil101.tsp\",\n",
    "            \"ch130\": \"http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/tsp/ch130.tsp\",\n",
    "            \"ch150\": \"http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/tsp/ch150.tsp\",\n",
    "            # \"brg180\": \"http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/tsp/brg180.tsp\",\n",
    "            \"a280\": \"http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/tsp/a280.tsp\",\n",
    "            \"pcb442\": \"http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/tsp/pcb442.tsp\",\n",
    "            # \"pr1002\": \"http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/tsp/pr1002.tsp\"\n",
    "        }\n",
    "\n",
    "        for name, url in instances.items():\n",
    "            self.download_instance(name, url)\n",
    "\n",
    "    # The rest of the class remains unchanged\n",
    "    def load_instance(self, instance_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Load a TSP instance and parse its metadata.\n",
    "\n",
    "        Args:\n",
    "            instance_name: Name of the instance to load\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with instance metadata and distance matrix\n",
    "        \"\"\"\n",
    "        tsp_file = self.data_dir / instance_name / f\"{instance_name}.tsp\"\n",
    "\n",
    "        if not tsp_file.exists():\n",
    "            raise FileNotFoundError(f\"Instance file {tsp_file} not found\")\n",
    "\n",
    "        metadata = {\n",
    "            \"name\": instance_name,\n",
    "            \"coords\": [],\n",
    "            \"dimension\": 0,\n",
    "            \"edge_weight_type\": \"\",\n",
    "            \"comment\": \"\"\n",
    "        }\n",
    "\n",
    "        # Parse the .tsp file\n",
    "        reading_coords = False\n",
    "        with open(tsp_file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "\n",
    "                if line == \"EOF\":\n",
    "                    break\n",
    "\n",
    "                if reading_coords:\n",
    "                    if line:\n",
    "                        parts = line.split()\n",
    "                        # Skip the index (first part) and parse the coordinates\n",
    "                        coords = [float(p) for p in parts[1:3]]\n",
    "                        metadata[\"coords\"].append(coords)\n",
    "                    continue\n",
    "\n",
    "                if \":\" in line:\n",
    "                    key, value = [p.strip() for p in line.split(\":\", 1)]\n",
    "                    if key == \"DIMENSION\":\n",
    "                        metadata[\"dimension\"] = int(value)\n",
    "                    elif key == \"EDGE_WEIGHT_TYPE\":\n",
    "                        metadata[\"edge_weight_type\"] = value\n",
    "                    elif key == \"COMMENT\":\n",
    "                        metadata[\"comment\"] = value\n",
    "\n",
    "                if line == \"NODE_COORD_SECTION\":\n",
    "                    reading_coords = True\n",
    "\n",
    "        # Convert coordinates to a NumPy array\n",
    "        metadata[\"coords\"] = np.array(metadata[\"coords\"])\n",
    "\n",
    "        # Calculate distance matrix\n",
    "        n = metadata[\"dimension\"]\n",
    "        distances = np.zeros((n, n))\n",
    "        if metadata[\"edge_weight_type\"] == \"EUC_2D\":\n",
    "            # Euclidean distance (rounded to nearest integer as per TSPLIB standard)\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    if i != j:\n",
    "                        dx = metadata[\"coords\"][i][0] - metadata[\"coords\"][j][0]\n",
    "                        dy = metadata[\"coords\"][i][1] - metadata[\"coords\"][j][1]\n",
    "                        distances[i][j] = round(math.sqrt(dx*dx + dy*dy))\n",
    "        elif metadata[\"edge_weight_type\"] == \"GEO\":\n",
    "            # Geographical distance (latitude/longitude)\n",
    "            # Implement GEO distance calculation if needed\n",
    "            pass\n",
    "        else:\n",
    "            # Default to Euclidean distance (not rounded)\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    if i != j:\n",
    "                        distances[i][j] = np.linalg.norm(metadata[\"coords\"][i] - metadata[\"coords\"][j])\n",
    "\n",
    "        metadata[\"distances\"] = distances\n",
    "\n",
    "        # Try to load optimal tour if available\n",
    "        opt_file = self.data_dir / instance_name / f\"{instance_name}.opt.tour\"\n",
    "        if opt_file.exists():\n",
    "            optimal_tour = self.load_optimal_tour(opt_file)\n",
    "            metadata[\"optimal_tour\"] = optimal_tour\n",
    "            metadata[\"optimal_length\"] = self.calculate_tour_length(\n",
    "                optimal_tour, distances\n",
    "            )\n",
    "            print(f\"Loaded optimal tour with length {metadata['optimal_length']}\")\n",
    "        else:\n",
    "            print(f\"No optimal tour available for {instance_name}\")\n",
    "\n",
    "        self.instances[instance_name] = metadata\n",
    "        return metadata\n",
    "\n",
    "    def load_optimal_tour(self, tour_file: Path) -> List[int]:\n",
    "        \"\"\"\n",
    "        Load an optimal tour from a .tour file.\n",
    "\n",
    "        Args:\n",
    "            tour_file: Path to the .tour file\n",
    "\n",
    "        Returns:\n",
    "            List of city indices forming the optimal tour\n",
    "        \"\"\"\n",
    "        tour = []\n",
    "        reading_tour = False\n",
    "\n",
    "        with open(tour_file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "\n",
    "                if line == \"EOF\":\n",
    "                    break\n",
    "\n",
    "                if reading_tour:\n",
    "                    if line and line.isdigit():\n",
    "                        # TSPLIB tour files use 1-based indexing, convert to 0-based\n",
    "                        tour.append(int(line) - 1)\n",
    "                    continue\n",
    "\n",
    "                if line == \"TOUR_SECTION\":\n",
    "                    reading_tour = True\n",
    "\n",
    "        return tour\n",
    "\n",
    "    def calculate_tour_length(self, tour: List[int], distances: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the length of a tour.\n",
    "\n",
    "        Args:\n",
    "            tour: List of city indices\n",
    "            distances: Distance matrix\n",
    "\n",
    "        Returns:\n",
    "            Total tour length\n",
    "        \"\"\"\n",
    "        length = 0\n",
    "        for i in range(len(tour)):\n",
    "            length += distances[tour[i]][tour[(i + 1) % len(tour)]]\n",
    "        return length\n",
    "\n",
    "    def benchmark_solver(self,\n",
    "                         instance_name: str,\n",
    "                         solver_class,\n",
    "                         solver_name = None,\n",
    "                         solver_params: Dict[str, Any] = None,\n",
    "                         runs: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Benchmark a TSP solver on a specific instance.\n",
    "\n",
    "        Args:\n",
    "            instance_name: Name of the instance to benchmark on\n",
    "            solver_class: Class of the solver to benchmark\n",
    "            solver_params: Parameters to pass to the solver\n",
    "            runs: Number of runs to perform\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with benchmark results\n",
    "        \"\"\"\n",
    "        if instance_name not in self.instances:\n",
    "            self.load_instance(instance_name)\n",
    "\n",
    "        instance = self.instances[instance_name]\n",
    "\n",
    "        if solver_params is None:\n",
    "            solver_params = {}\n",
    "\n",
    "        results = {\n",
    "            \"instance\": instance_name,\n",
    "            \"dimension\": instance[\"dimension\"],\n",
    "            \"runs\": [],\n",
    "            \"best_length\": float('inf'),\n",
    "            \"worst_length\": 0,\n",
    "            \"mean_length\": 0,\n",
    "            \"mean_time\": 0\n",
    "        }\n",
    "\n",
    "        total_length = 0\n",
    "        total_time = 0\n",
    "\n",
    "        for run in range(runs):\n",
    "            print(f\"Run {run+1}/{runs}...\")\n",
    "\n",
    "            # Initialize the solver\n",
    "            if solver_class:\n",
    "                solver = solver_class(instance[\"distances\"])\n",
    "            \n",
    "\n",
    "            # Time the solution process\n",
    "            \n",
    "\n",
    "            # Call the appropriate solve method based on the solver class\n",
    "            if solver_name == \"TSP_2opt_SA_naive\":\n",
    "                start_time = time.time()\n",
    "                tour, length = solver.solve_hybrid(**solver_params)\n",
    "                solve_time = time.time() - start_time\n",
    "            elif solver_name == \"TSP_LKH_Hybrid_naive\":\n",
    "                start_time = time.time()\n",
    "                tour, length = solver.solve(**solver_params)\n",
    "                solve_time = time.time() - start_time\n",
    "            elif solver_name == \"TSP_2opt_SA_outer\":\n",
    "                start_time = time.time()\n",
    "                tour, length = solver.solve(**solver_params)\n",
    "                solve_time = time.time() - start_time\n",
    "            elif solver_name == \"TSP_2opt_3opt_SA\":\n",
    "                start_time = time.time()\n",
    "                tour, length = solver.solve(**solver_params)\n",
    "                solve_time = time.time() - start_time\n",
    "            elif solver_name == \"TSP_LKH_Hybrid_optimized\":\n",
    "                start_time = time.time()\n",
    "                tour, length = solver.solve(**solver_params)\n",
    "                solve_time = time.time() - start_time\n",
    "            elif solver_name == \"GNN_v1_BS\":\n",
    "                gnn_model_v1 = solver_params[\"model\"]\n",
    "                config = solver_params[\"config\"]\n",
    "                beam_size_gnn_pure = solver_params[\"beam_size_gnn_pure\"]\n",
    "                dtypeFloat = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "                dtypeLong = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor\n",
    "                coords = np.array(instance[\"coords\"])\n",
    "                start_time = time.time()\n",
    "                tour, length, _ = TSPComparison.pure_gnn_beam_search_solve(\n",
    "                    coords, gnn_model_v1, config, beam_size_gnn_pure, dtypeFloat, dtypeLong\n",
    "                )\n",
    "                solve_time = time.time() - start_time\n",
    "            elif solver_name == \"GNN_v2_BS\":\n",
    "                gnn_model_v2 = solver_params[\"model\"]\n",
    "                config = solver_params[\"config\"]\n",
    "                beam_size_gnn_pure = solver_params[\"beam_size_gnn_pure\"]\n",
    "                dtypeFloat = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "                dtypeLong = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor\n",
    "                coords = np.array(instance[\"coords\"])\n",
    "                start_time = time.time()\n",
    "                tour, length, _ = TSPComparison.pure_gnn_beam_search_solve(\n",
    "                    coords, gnn_model_v2, config, beam_size_gnn_pure, dtypeFloat, dtypeLong\n",
    "                )\n",
    "                solve_time = time.time() - start_time\n",
    "            elif solver_name == \"GNN_v1_BC\":\n",
    "                gnn_solver_v1 = solver_params[\"gnn_solver\"]\n",
    "                coords = np.array(instance[\"coords\"])\n",
    "                start_time = time.time()\n",
    "                tour, length, _ = gnn_solver_v1.solve_tsp(coords)\n",
    "                solve_time = time.time() - start_time\n",
    "            else:\n",
    "                # Generic solve method\n",
    "                start_time = time.time()\n",
    "                tour, length = solver.solve(**solver_params)\n",
    "                solve_time = time.time() - start_time\n",
    "\n",
    "\n",
    "            # Record results\n",
    "            run_result = {\n",
    "                \"tour\": tour,\n",
    "                \"length\": length,\n",
    "                \"time\": solve_time\n",
    "            }\n",
    "\n",
    "            results[\"runs\"].append(run_result)\n",
    "            total_length += length\n",
    "            total_time += solve_time\n",
    "\n",
    "            # Update best and worst\n",
    "            if length < results[\"best_length\"]:\n",
    "                results[\"best_length\"] = length\n",
    "                results[\"best_tour\"] = tour\n",
    "            if length > results[\"worst_length\"]:\n",
    "                results[\"worst_length\"] = length\n",
    "\n",
    "        # Calculate means\n",
    "        results[\"mean_length\"] = total_length / runs\n",
    "        results[\"mean_time\"] = total_time / runs\n",
    "\n",
    "        # Calculate accuracy if optimal tour is available\n",
    "        if \"optimal_length\" in instance:\n",
    "            optimal_length = instance[\"optimal_length\"]\n",
    "            results[\"optimal_length\"] = optimal_length\n",
    "            results[\"best_gap\"] = (results[\"best_length\"] - optimal_length) / optimal_length * 100\n",
    "            results[\"mean_gap\"] = (results[\"mean_length\"] - optimal_length) / optimal_length * 100\n",
    "\n",
    "            print(f\"Optimal tour length: {optimal_length}\")\n",
    "            print(f\"Best tour length: {results['best_length']} (gap: {results['best_gap']:.2f}%)\")\n",
    "            print(f\"Mean tour length: {results['mean_length']} (gap: {results['mean_gap']:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"Best tour length: {results['best_length']}\")\n",
    "            print(f\"Mean tour length: {results['mean_length']}\")\n",
    "\n",
    "        print(f\"Mean solution time: {results['mean_time']:.2f} seconds\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def benchmark_all(self,\n",
    "                      solvers: List[Tuple[str, Any, Dict[str, Any]]],\n",
    "                      instances: List[str] = None,\n",
    "                      runs: int = 3) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Benchmark multiple solvers on multiple instances.\n",
    "\n",
    "        Args:\n",
    "            solvers: List of (solver_name, solver_class, solver_params) tuples\n",
    "            instances: List of instance names to benchmark on (if None, use all loaded instances)\n",
    "            runs: Number of runs per solver per instance\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping (instance_name, solver_name) to benchmark results\n",
    "        \"\"\"\n",
    "        if instances is None:\n",
    "            # Use all instances that have been loaded\n",
    "            instances = list(self.instances.keys())\n",
    "\n",
    "            # If no instances are loaded, download and load some standard ones\n",
    "            if not instances:\n",
    "                self.download_tsplib_set()\n",
    "                instances = [\"berlin52\", \"a280\", \"pcb442\"]\n",
    "                for instance in instances:\n",
    "                    self.load_instance(instance)\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        for instance_name in instances:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Benchmarking on {instance_name}\")\n",
    "            print(f\"{'='*50}\")\n",
    "\n",
    "            for solver_name, solver_class, solver_params in solvers:\n",
    "                print(f\"\\n{'-'*40}\")\n",
    "                print(f\"Using solver: {solver_name}\")\n",
    "                print(f\"{'-'*40}\")\n",
    "\n",
    "                key = (instance_name, solver_name)\n",
    "                results[key] = self.benchmark_solver(\n",
    "                    instance_name,\n",
    "                    solver_class,\n",
    "                    solver_name,\n",
    "                    solver_params,\n",
    "                    runs\n",
    "                )\n",
    "\n",
    "                # Add solver information\n",
    "                results[key][\"solver\"] = solver_name\n",
    "                results[key][\"params\"] = solver_params\n",
    "\n",
    "        # Print summary\n",
    "        self.print_benchmark_summary(results)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def print_benchmark_summary(self, results: Dict[Tuple[str, str], Dict[str, Any]]) -> None:\n",
    "        \"\"\"\n",
    "        Print a summary of benchmark results.\n",
    "\n",
    "        Args:\n",
    "            results: Dictionary mapping (instance_name, solver_name) to benchmark results\n",
    "        \"\"\"\n",
    "        print(\"\\n\\n\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"BENCHMARK SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # Group by instance\n",
    "        by_instance = {}\n",
    "        for (instance, solver), result in results.items():\n",
    "            if instance not in by_instance:\n",
    "                by_instance[instance] = []\n",
    "            by_instance[instance].append((solver, result))\n",
    "\n",
    "        # Print results for each instance\n",
    "        for instance, solver_results in by_instance.items():\n",
    "            print(f\"\\nInstance: {instance} ({self.instances[instance]['dimension']} cities)\")\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"{'Solver':<20} {'Best Length':<15} {'Mean Length':<15} {'Mean Time (s)':<15} {'Gap (%)':<10}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            optimal_length = None\n",
    "            if \"optimal_length\" in self.instances[instance]:\n",
    "                optimal_length = self.instances[instance][\"optimal_length\"]\n",
    "\n",
    "            for solver, result in sorted(solver_results, key=lambda x: x[1][\"best_length\"]):\n",
    "                if optimal_length is not None:\n",
    "                    gap = (result[\"best_length\"] - optimal_length) / optimal_length * 100\n",
    "                    gap_str = f\"{gap:.2f}%\"\n",
    "                else:\n",
    "                    gap_str = \"N/A\"\n",
    "\n",
    "                print(f\"{solver:<20} {result['best_length']:<15.2f} {result['mean_length']:<15.2f} {result['mean_time']:<15.2f} {gap_str:<10}\")\n",
    "\n",
    "            if optimal_length is not None:\n",
    "                print(f\"\\nOptimal tour length: {optimal_length}\")\n",
    "\n",
    "    def visualize_tour(self, instance_name: str, tour: List[int], title: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Visualize a tour for a given instance.\n",
    "\n",
    "        Args:\n",
    "            instance_name: Name of the instance\n",
    "            tour: List of city indices representing the tour\n",
    "            title: Title for the plot\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "\n",
    "            if instance_name not in self.instances:\n",
    "                self.load_instance(instance_name)\n",
    "\n",
    "            instance = self.instances[instance_name]\n",
    "            coords = instance[\"coords\"]\n",
    "\n",
    "            # Create a plot\n",
    "            plt.figure(figsize=(10, 8))\n",
    "\n",
    "            # Plot cities\n",
    "            plt.scatter(coords[:, 0], coords[:, 1], c='blue', s=20)\n",
    "\n",
    "            # Plot tour\n",
    "            for i in range(len(tour)):\n",
    "                city1 = tour[i]\n",
    "                city2 = tour[(i + 1) % len(tour)]\n",
    "                plt.plot([coords[city1, 0], coords[city2, 0]],\n",
    "                         [coords[city1, 1], coords[city2, 1]],\n",
    "                         'r-', alpha=0.7)\n",
    "\n",
    "            if title:\n",
    "                plt.title(title)\n",
    "            else:\n",
    "                plt.title(f\"Tour for {instance_name}\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save the plot to a file\n",
    "            output_dir = self.data_dir / \"plots\"\n",
    "            output_dir.mkdir(exist_ok=True)\n",
    "            plt.savefig(output_dir / f\"{instance_name}_{int(time.time())}.png\")\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"Matplotlib is required for visualization.\")\n",
    "            print(\"Install it with: pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9871bb7",
   "metadata": {},
   "source": [
    "## Визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c234f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsp(p, x_coord, W, W_val, W_target, title=\"default\"):\n",
    "    \"\"\"\n",
    "    Helper function to plot TSP tours.\n",
    "\n",
    "    Args:\n",
    "        p: Matplotlib figure/subplot axis (e.g., plt.gca()).\n",
    "        x_coord: Coordinates of nodes (num_nodes, 2).\n",
    "        W: Edge adjacency matrix (ignored if plotting a tour path).\n",
    "        W_val: Edge values (distance) matrix (used by nx.Graph).\n",
    "        W_target: One-hot matrix with 1s on edges to plot (e.g., tour edges).\n",
    "        title: Title of figure/subplot.\n",
    "\n",
    "    Returns:\n",
    "        p: Updated figure/subplot axis.\n",
    "    \"\"\"\n",
    "\n",
    "    def _edges_to_node_pairs(W_target_matrix):\n",
    "        \"\"\"Helper function to convert edge matrix into pairs of adjacent nodes.\"\"\"\n",
    "        pairs = []\n",
    "        rows, cols = np.where(W_target_matrix == 1)\n",
    "        # Avoid duplicates for undirected graphs, take only (i, j) where i < j\n",
    "        for r, c in zip(rows, cols):\n",
    "            if r < c:\n",
    "                pairs.append((r, c))\n",
    "        return pairs\n",
    "\n",
    "    # Используем nx.Graph() вместо DiGraph, т.к. TSP тур неориентированный\n",
    "    G = nx.Graph(W_val) # Передаем матрицу расстояний для информации о графе\n",
    "    pos = dict(zip(range(len(x_coord)), x_coord.tolist())) # Позиции узлов\n",
    "\n",
    "    # Получаем пары ребер из матрицы W_target\n",
    "    target_pairs = _edges_to_node_pairs(W_target)\n",
    "\n",
    "    colors = ['g'] + ['b'] * (len(x_coord) - 1) # Зеленый для 0, синий для остальных\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=colors, node_size=50, ax=p)\n",
    "\n",
    "    # Рисуем только ребра тура\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=target_pairs, alpha=1, width=1.5, edge_color='r', ax=p)\n",
    "\n",
    "    # Добавляем подписи узлов\n",
    "    labels = {i: str(i) for i in range(len(x_coord))}\n",
    "    nx.draw_networkx_labels(G, pos, labels=labels, font_size=8, ax=p)\n",
    "\n",
    "    p.set_title(title)\n",
    "    p.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True) # Показываем оси\n",
    "    p.set_xlabel(\"X Coordinate\")\n",
    "    p.set_ylabel(\"Y Coordinate\")\n",
    "    p.grid(True, linestyle='--', alpha=0.6)\n",
    "    p.axis('equal') # Равные масштабы\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "# --- Основная функция plot_solution ---\n",
    "def plot_solution(nodes_coord: np.ndarray,\n",
    "                  tour: List[int],\n",
    "                  title: str,\n",
    "                  tour_length: float,\n",
    "                  ax: Optional[plt.Axes] = None):\n",
    "    \"\"\"\n",
    "    Визуализирует один маршрут TSP на заданной оси matplotlib.\n",
    "\n",
    "    Args:\n",
    "        nodes_coord: Координаты узлов (N, 2).\n",
    "        tour: Список индексов узлов в порядке обхода (0-based).\n",
    "        tour_length: Длина маршрута.\n",
    "        title: Заголовок для графика.\n",
    "        ax: Ось matplotlib для рисования. Если None, создается новая фигура/ось.\n",
    "    \"\"\"\n",
    "    n = nodes_coord.shape[0]\n",
    "    if not tour:\n",
    "        print(f\"Warning: Cannot plot empty tour for '{title}'.\")\n",
    "        if ax: ax.set_title(f\"{title}\\n(Empty Tour)\")\n",
    "        return\n",
    "\n",
    "    if len(tour) != n:\n",
    "         print(f\"Warning: Tour length ({len(tour)}) != num_nodes ({n}) for '{title}'. Plotting partial.\")\n",
    "\n",
    "    if ax is None:\n",
    "        # Если ось не передана, создаем новую фигуру и ось\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # Создаем матрицу W_target для ребер тура\n",
    "    W_tour = np.zeros((n, n))\n",
    "    for i in range(len(tour)):\n",
    "        u = tour[i]\n",
    "        v = tour[(i + 1) % len(tour)]\n",
    "        if 0 <= u < n and 0 <= v < n:\n",
    "            W_tour[u, v] = 1\n",
    "            W_tour[v, u] = 1\n",
    "        else:\n",
    "            print(f\"Error: Invalid node index in tour for plotting. u={u}, v={v}\")\n",
    "            ax.set_title(f\"{title}\\n(Invalid Tour)\")\n",
    "            return\n",
    "\n",
    "    # Создаем фиктивные матрицы для plot_tsp\n",
    "    dist_matrix_dummy = squareform(pdist(nodes_coord))\n",
    "    W_dummy = np.ones((n,n))\n",
    "    np.fill_diagonal(W_dummy, 0)\n",
    "\n",
    "    # Используем plot_tsp для рисования на переданной оси 'ax'\n",
    "    plot_tsp(ax, nodes_coord, W_dummy, dist_matrix_dummy, W_tour,\n",
    "             title=f\"{title}\\nLength: {tour_length:.4f}\")\n",
    "\n",
    "    # Выделяем начальную точку\n",
    "    start_node_idx = tour[0]\n",
    "    # Используем plot вместо scatter на оси ax для лучшего контроля\n",
    "    ax.plot(nodes_coord[start_node_idx, 0], nodes_coord[start_node_idx, 1],\n",
    "            marker='*', color='yellow', markersize=15, markeredgecolor='black', zorder=4, linestyle='None')\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Новая функция `plot_comparison` (для двух графиков)\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_comparison(nodes_coord: np.ndarray,\n",
    "                    found_tour: List[int],\n",
    "                    found_length: float,\n",
    "                    optimal_tour: List[int],\n",
    "                    optimal_length: float,\n",
    "                    instance_name: str,\n",
    "                    method_name: str,\n",
    "                    found_color: str = 'red',\n",
    "                    optimal_color: str = 'green',\n",
    "                    found_linestyle: str = '-',\n",
    "                    optimal_linestyle: str = '--',\n",
    "                    found_alpha: float = 0.7,\n",
    "                    optimal_alpha: float = 0.9,\n",
    "                    start_node_color='yellow'):\n",
    "    \"\"\"\n",
    "    Создает фигуру с двумя графиками: оптимальный тур слева, найденный тур справа.\n",
    "    Не вызывает plot_solution, рисует все напрямую.\n",
    "\n",
    "    Args:\n",
    "        nodes_coord: Координаты узлов (N, 2).\n",
    "        found_tour: Найденный маршрут (список индексов 0-based).\n",
    "        found_length: Длина найденного маршрута.\n",
    "        optimal_tour: Оптимальный маршрут (список индексов 0-based).\n",
    "        optimal_length: Длина оптимального маршрута.\n",
    "        instance_name: Имя экземпляра TSP.\n",
    "        method_name: Имя метода, который нашел `found_tour`.\n",
    "        # ... (остальные параметры для настройки цветов и т.д.)\n",
    "    \"\"\"\n",
    "    n = nodes_coord.shape[0]\n",
    "\n",
    "    # --- Создаем фигуру с двумя осями ---\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 8)) # Немного увеличим ширину\n",
    "\n",
    "    # --- Функция для отрисовки одного тура на оси ---\n",
    "    def draw_single_tour(ax, tour, length, title, tour_color, linestyle, alpha, start_node_color):\n",
    "        # 1. Проверка тура\n",
    "        if not tour:\n",
    "             ax.set_title(f\"{title}\\n(Empty Tour Provided)\")\n",
    "             ax.text(0.5, 0.5, \"No tour data\", ha='center', va='center', transform=ax.transAxes)\n",
    "             ax.set_xticks([]); ax.set_yticks([]) # Убираем оси для пустого графика\n",
    "             return\n",
    "        if len(tour) != n:\n",
    "             print(f\"Warning: Tour length ({len(tour)}) != num_nodes ({n}) for '{title}'. Plotting partial.\")\n",
    "             # Мы все равно нарисуем то, что есть\n",
    "\n",
    "        # 2. Рисуем узлы\n",
    "        ax.scatter(nodes_coord[:, 0], nodes_coord[:, 1], c='blue', s=50, zorder=3, label='Cities')\n",
    "        # ---> ИСПРАВЛЕНИЕ: Уменьшен отступ для текста <---\n",
    "        text_offset = (np.max(nodes_coord[:,1]) - np.min(nodes_coord[:,1])) * 0.02 # Маленький % от диапазона Y\n",
    "        for i, (x, y) in enumerate(nodes_coord):\n",
    "            ax.text(x, y + text_offset, str(i), fontsize=9, ha='center', va='bottom') # Сдвиг вверх\n",
    "\n",
    "        # 3. Рисуем линии тура\n",
    "        tour_len_to_plot = len(tour)\n",
    "        route_coords = nodes_coord[tour]\n",
    "        # Замыкаем тур только если он полный\n",
    "        route_coords_closed = np.vstack([route_coords, route_coords[0]]) if len(tour) == n else route_coords\n",
    "        ax.plot(route_coords_closed[:, 0], route_coords_closed[:, 1],\n",
    "                 color=tour_color, linestyle=linestyle, alpha=alpha,\n",
    "                 lw=1.5, zorder=1, label=f'Length: {length:.4f}') # Увеличена точность длины\n",
    "\n",
    "        # 4. Выделяем начальную точку\n",
    "        start_node_idx = tour[0]\n",
    "        if 0 <= start_node_idx < n:\n",
    "            ax.plot(nodes_coord[start_node_idx, 0], nodes_coord[start_node_idx, 1],\n",
    "                    marker='*', color=start_node_color, markersize=15,\n",
    "                    markeredgecolor='black', zorder=4, linestyle='None',\n",
    "                    label='Start Node (0)') # Добавим индекс в легенду\n",
    "\n",
    "        # 5. Настройки осей\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('X Coordinate')\n",
    "        ax.set_ylabel('Y Coordinate')\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        # ---> ИСПРАВЛЕНИЕ: Устанавливаем пределы осей ПОСЛЕ отрисовки <---\n",
    "        # Добавляем небольшой отступ вокруг крайних точек\n",
    "        margin_x = (np.max(nodes_coord[:, 0]) - np.min(nodes_coord[:, 0])) * 0.1\n",
    "        margin_y = (np.max(nodes_coord[:, 1]) - np.min(nodes_coord[:, 1])) * 0.1\n",
    "        ax.set_xlim(np.min(nodes_coord[:, 0]) - margin_x, np.max(nodes_coord[:, 0]) + margin_x)\n",
    "        ax.set_ylim(np.min(nodes_coord[:, 1]) - margin_y, np.max(nodes_coord[:, 1]) + margin_y)\n",
    "        ax.set_aspect('equal', adjustable='box') # <--- Используем set_aspect вместо axis('equal')\n",
    "        ax.legend(loc='best', fontsize='small')\n",
    "\n",
    "    # --- Рисуем на левой оси (Оптимальный) ---\n",
    "    if optimal_tour:\n",
    "        draw_single_tour(ax=axes[0], tour=optimal_tour, length=optimal_length,\n",
    "                         title=f\"Optimal Solution: {instance_name}\",\n",
    "                         tour_color=optimal_color, linestyle=optimal_linestyle,\n",
    "                         alpha=optimal_alpha, start_node_color=start_node_color)\n",
    "    else:\n",
    "         axes[0].set_title(f\"Optimal Solution: {instance_name}\\n(Not Available)\")\n",
    "         axes[0].text(0.5, 0.5, \"Optimal tour data missing\", ha='center', va='center', transform=axes[0].transAxes)\n",
    "         axes[0].set_xticks([]); axes[0].set_yticks([])\n",
    "\n",
    "    # --- Рисуем на правой оси (Найденный) ---\n",
    "    if found_tour:\n",
    "        draw_single_tour(ax=axes[1], tour=found_tour, length=found_length,\n",
    "                         title=f\"Found ({method_name}): {instance_name}\",\n",
    "                         tour_color=found_color, linestyle=found_linestyle,\n",
    "                         alpha=found_alpha, start_node_color=start_node_color)\n",
    "    else:\n",
    "         axes[1].set_title(f\"Found ({method_name}): {instance_name}\\n(Not Available)\")\n",
    "         axes[1].text(0.5, 0.5, \"Found tour data missing\", ha='center', va='center', transform=axes[1].transAxes)\n",
    "         axes[1].set_xticks([]); axes[1].set_yticks([])\n",
    "\n",
    "    # Общий заголовок\n",
    "    gap = float('inf')\n",
    "    gap_str = \"N/A\"\n",
    "    if optimal_length > 0 and found_length != float('inf'):\n",
    "        gap = (found_length - optimal_length) / optimal_length * 100\n",
    "        gap_str = f\"{gap:.2f}%\"\n",
    "\n",
    "    fig.suptitle(f\"TSP Solution Comparison: {instance_name}\\nMethod: {method_name} | Gap: {gap_str}\", fontsize=16)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.93])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0e613",
   "metadata": {},
   "source": [
    "# Графовые нейронные сети для решения задачи коммивояжера"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db0c799",
   "metadata": {},
   "source": [
    "Реализация архитектуры графовой нейронной сети (базовая, без интеграции с Branch&Cut) принадлежит авторам данной работы:\n",
    "\n",
    "Isik S., Atkin M. Tackling the Traveling Salesman Problem with Graph Neural Networks. — 2023. — URL: https://medium.com/stanford-cs224w/tackling-the-traveling-salesman-problem-with-graph-neural-networks-b86ef4300c6e\n",
    "\n",
    "В свою очередь, предложенная авторами архитектура вдохновлена следующими работами:\n",
    "\n",
    "Joshi C. K., Laurent T., Bresson X. An Efficient Graph Convolutional Network Technique for the Travelling Salesman Problem. — 2019. — arXiv: 1906.01227 [cs.LG]. - URL: https://arxiv.org/abs/1906.01227\n",
    "\n",
    "Bresson X., Laurent T. The Transformer Network for the Traveling Salesman Problem. — 2021. — arXiv: 2103.03012 [cs.LG]. - URL: https://arxiv.org/abs/2103.03012\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e85330",
   "metadata": {},
   "source": [
    "## TSP Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6d4f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Dataloader definitions\n",
    "class DotDict(dict):\n",
    "    \"\"\"Wrapper around in-built dict class to access members through the dot operation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwds):\n",
    "        self.update(kwds)\n",
    "        self.__dict__ = self\n",
    "\n",
    "\n",
    "class GoogleTSPReader(object):\n",
    "    \"\"\"Iterator that reads TSP dataset files and yields mini-batches.\n",
    "\n",
    "    Format expected as in Vinyals et al., 2015: https://arxiv.org/abs/1506.03134, http://goo.gl/NDcOIG\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_nodes, num_neighbors, batch_size, filepath):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_nodes: Number of nodes in TSP tours\n",
    "            num_neighbors: Number of neighbors to consider for each node in graph\n",
    "            batch_size: Batch size\n",
    "            filepath: Path to dataset file (.txt file)\n",
    "        \"\"\"\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_neighbors = num_neighbors\n",
    "        self.batch_size = batch_size\n",
    "        self.filepath = filepath\n",
    "        self.filedata = shuffle(open(filepath, \"r\").readlines())  # Always shuffle upon reading data\n",
    "        self.max_iter = (len(self.filedata) // batch_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in range(self.max_iter):\n",
    "            start_idx = batch * self.batch_size\n",
    "            end_idx = (batch + 1) * self.batch_size\n",
    "            yield self.process_batch(self.filedata[start_idx:end_idx])\n",
    "\n",
    "    def process_batch(self, lines):\n",
    "        \"\"\"Helper function to convert raw lines into a mini-batch as a DotDict.\n",
    "        \"\"\"\n",
    "        batch_edges = []\n",
    "        batch_edges_values = []\n",
    "        batch_edges_target = []  # Binary classification targets (0/1)\n",
    "        batch_nodes = []\n",
    "        batch_nodes_target = []  # Multi-class classification targets (`num_nodes` classes)\n",
    "        batch_nodes_coord = []\n",
    "        batch_tour_nodes = []\n",
    "        batch_tour_len = []\n",
    "\n",
    "        for line_num, line in enumerate(lines):\n",
    "            line = line.split(\" \")  # Split into list\n",
    "\n",
    "            # Compute signal on nodes\n",
    "            nodes = np.ones(self.num_nodes)  # All 1s for TSP...\n",
    "\n",
    "            # Convert node coordinates to required format\n",
    "            nodes_coord = []\n",
    "            for idx in range(0, 2 * self.num_nodes, 2):\n",
    "                nodes_coord.append([float(line[idx]), float(line[idx + 1])])\n",
    "\n",
    "            # Compute distance matrix\n",
    "            W_val = squareform(pdist(nodes_coord, metric='euclidean'))\n",
    "\n",
    "            # Compute adjacency matrix\n",
    "            if self.num_neighbors == -1:\n",
    "                W = np.ones((self.num_nodes, self.num_nodes))  # Graph is fully connected\n",
    "            else:\n",
    "                W = np.zeros((self.num_nodes, self.num_nodes))\n",
    "                # Determine k-nearest neighbors for each node\n",
    "                knns = np.argpartition(W_val, kth=self.num_neighbors, axis=-1)[:, self.num_neighbors::-1]\n",
    "                # Make connections\n",
    "                for idx in range(self.num_nodes):\n",
    "                    W[idx][knns[idx]] = 1\n",
    "            np.fill_diagonal(W, 2)  # Special token for self-connections\n",
    "\n",
    "            # Convert tour nodes to required format\n",
    "            # Don't add final connection for tour/cycle\n",
    "            tour_nodes = [int(node) - 1 for node in line[line.index('output') + 1:-1]][:-1]\n",
    "\n",
    "            # Compute node and edge representation of tour + tour_len\n",
    "            tour_len = 0\n",
    "            nodes_target = np.zeros(self.num_nodes)\n",
    "            edges_target = np.zeros((self.num_nodes, self.num_nodes))\n",
    "            for idx in range(len(tour_nodes) - 1):\n",
    "                i = tour_nodes[idx]\n",
    "                j = tour_nodes[idx + 1]\n",
    "                nodes_target[i] = idx  # node targets: ordering of nodes in tour\n",
    "                edges_target[i][j] = 1\n",
    "                edges_target[j][i] = 1\n",
    "                tour_len += W_val[i][j]\n",
    "\n",
    "            # Add final connection of tour in edge target\n",
    "            nodes_target[j] = len(tour_nodes) - 1\n",
    "            edges_target[j][tour_nodes[0]] = 1\n",
    "            edges_target[tour_nodes[0]][j] = 1\n",
    "            tour_len += W_val[j][tour_nodes[0]]\n",
    "\n",
    "            # Concatenate the data\n",
    "            batch_edges.append(W)\n",
    "            batch_edges_values.append(W_val)\n",
    "            batch_edges_target.append(edges_target)\n",
    "            batch_nodes.append(nodes)\n",
    "            batch_nodes_target.append(nodes_target)\n",
    "            batch_nodes_coord.append(nodes_coord)\n",
    "            batch_tour_nodes.append(tour_nodes)\n",
    "            batch_tour_len.append(tour_len)\n",
    "\n",
    "        # From list to tensors as a DotDict\n",
    "        batch = DotDict()\n",
    "        batch.edges = np.stack(batch_edges, axis=0)\n",
    "        batch.edges_values = np.stack(batch_edges_values, axis=0)\n",
    "        batch.edges_target = np.stack(batch_edges_target, axis=0)\n",
    "        batch.nodes = np.stack(batch_nodes, axis=0)\n",
    "        batch.nodes_target = np.stack(batch_nodes_target, axis=0)\n",
    "        batch.nodes_coord = np.stack(batch_nodes_coord, axis=0)\n",
    "        batch.tour_nodes = np.stack(batch_tour_nodes, axis=0)\n",
    "        batch.tour_len = np.stack(batch_tour_len, axis=0)\n",
    "        return batch\n",
    "\n",
    "dtypeFloat = torch.cuda.FloatTensor if device.type == 'cuda' else torch.FloatTensor\n",
    "dtypeLong = torch.cuda.LongTensor if device.type == 'cuda' else torch.LongTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e4e0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 20\n",
    "num_neighbors = -1    # when set to -1, it considers all the connections instead of k nearest neighbors\n",
    "train_filepath = f\"./tsp_data/tsp{num_nodes}_train_concorde.txt\"\n",
    "\n",
    "batch_size = 1\n",
    "dataset = GoogleTSPReader(num_nodes, num_neighbors, batch_size, train_filepath)\n",
    "\n",
    "t = time.time()\n",
    "batch = next(iter(dataset))  # Generate a batch of TSPs\n",
    "print(\"Batch generation took: {:.3f} sec\".format(time.time() - t))\n",
    "\n",
    "print(\"edges:\", batch.edges.shape)\n",
    "print(\"edges_values:\", batch.edges_values.shape)\n",
    "print(\"edges_targets:\", batch.edges_target.shape)\n",
    "print(\"nodes:\", batch.nodes.shape)\n",
    "print(\"nodes_target:\", batch.nodes_target.shape)\n",
    "print(\"nodes_coord:\", batch.nodes_coord.shape)\n",
    "print(\"tour_nodes:\", batch.tour_nodes.shape)\n",
    "print(\"tour_len:\", batch.tour_len.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e8d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plotting helper function\n",
    "\n",
    "def plot_tsp(p, x_coord, W, W_val, W_target, title=\"default\"):\n",
    "    \"\"\"\n",
    "    Helper function to plot TSP tours.\n",
    "\n",
    "    Args:\n",
    "        p: Matplotlib figure/subplot\n",
    "        x_coord: Coordinates of nodes\n",
    "        W: Edge adjacency matrix\n",
    "        W_val: Edge values (distance) matrix\n",
    "        W_target: One-hot matrix with 1s on groundtruth/predicted edges\n",
    "        title: Title of figure/subplot\n",
    "\n",
    "    Returns:\n",
    "        p: Updated figure/subplot\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def _edges_to_node_pairs(W):\n",
    "        \"\"\"Helper function to convert edge matrix into pairs of adjacent nodes.\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        for r in range(len(W)):\n",
    "            for c in range(len(W)):\n",
    "                if W[r][c] == 1:\n",
    "                    pairs.append((r, c))\n",
    "        return pairs\n",
    "\n",
    "    G = nx.DiGraph(W_val)\n",
    "    pos = dict(zip(range(len(x_coord)), x_coord.tolist()))\n",
    "    adj_pairs = _edges_to_node_pairs(W)\n",
    "    target_pairs = _edges_to_node_pairs(W_target)\n",
    "    colors = ['g'] + ['b'] * (len(x_coord) - 1)  # Green for 0th node, blue for others\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=colors, node_size=50)\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=adj_pairs, alpha=0.3, width=0.5)\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=target_pairs, alpha=1, width=1, edge_color='r')\n",
    "    p.set_title(title)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2b105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "f = plt.figure(figsize=(5, 5))\n",
    "a = f.add_subplot(111)\n",
    "plot_tsp(a, batch.nodes_coord[idx], batch.edges[idx], batch.edges_values[idx], batch.edges_target[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b7af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Batch normalization layers\n",
    "class BatchNormNode(nn.Module):\n",
    "    \"\"\"Batch normalization for node features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(BatchNormNode, self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim, track_running_stats=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features (batch_size, num_nodes, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            x_bn: Node features after batch normalization (batch_size, num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        x_trans = x.transpose(1, 2).contiguous()  # Reshape input: (batch_size, hidden_dim, num_nodes)\n",
    "        x_trans_bn = self.batch_norm(x_trans)\n",
    "        x_bn = x_trans_bn.transpose(1, 2).contiguous()  # Reshape to original shape\n",
    "        return x_bn\n",
    "\n",
    "\n",
    "class BatchNormEdge(nn.Module):\n",
    "    \"\"\"Batch normalization for edge features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(BatchNormEdge, self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm2d(hidden_dim, track_running_stats=False)\n",
    "\n",
    "    def forward(self, e):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            e: Edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            e_bn: Edge features after batch normalization (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        e_trans = e.transpose(1, 3).contiguous()  # Reshape input: (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "        e_trans_bn = self.batch_norm(e_trans)\n",
    "        e_bn = e_trans_bn.transpose(1, 3).contiguous()  # Reshape to original\n",
    "        return e_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef8efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title MLP layer\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi-layer Perceptron for output prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, output_dim, L=2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.L = L\n",
    "        U = []\n",
    "        for layer in range(self.L - 1):\n",
    "            U.append(nn.Linear(hidden_dim, hidden_dim, True))\n",
    "        self.U = nn.ModuleList(U)\n",
    "        self.V = nn.Linear(hidden_dim, output_dim, True)\n",
    "        self.dropout_mlp = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input features (batch_size, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            y: Output predictions (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        Ux = x\n",
    "        for U_i in self.U:\n",
    "            Ux = U_i(Ux)  # B x H\n",
    "            Ux = F.relu(Ux)  # B x H\n",
    "            Ux = self.dropout_mlp(Ux)\n",
    "        y = self.V(Ux)  # B x O\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9765b0",
   "metadata": {},
   "source": [
    "## Эмбеддинги вершин и ребер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b6f684",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeFeatures(nn.Module):\n",
    "    \"\"\"Convnet features for nodes.\n",
    "\n",
    "    Using `sum` aggregation:\n",
    "        x_i = U*x_i +  sum_j [ gate_ij * (V*x_j) ]\n",
    "\n",
    "    Using `mean` aggregation:\n",
    "        x_i = U*x_i + ( sum_j [ gate_ij * (V*x_j) ] / sum_j [ gate_ij] )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, aggregation=\"mean\"):\n",
    "        super(NodeFeatures, self).__init__()\n",
    "        self.aggregation = aggregation\n",
    "        self.U = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "        self.V = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "\n",
    "    def forward(self, x, edge_gate):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features (batch_size, num_nodes, hidden_dim)\n",
    "            edge_gate: Edge gate values (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            x_new: Convolved node features (batch_size, num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        Ux = self.U(x)  # B x V x H\n",
    "        Vx = self.V(x)  # B x V x H\n",
    "        Vx = Vx.unsqueeze(1)  # extend Vx from \"B x V x H\" to \"B x 1 x V x H\"\n",
    "        gateVx = edge_gate * Vx  # B x V x V x H\n",
    "        if self.aggregation==\"mean\":\n",
    "            x_new = Ux + torch.sum(gateVx, dim=2) / (1e-20 + torch.sum(edge_gate, dim=2))  # B x V x H\n",
    "        elif self.aggregation==\"sum\":\n",
    "            x_new = Ux + torch.sum(gateVx, dim=2)  # B x V x H\n",
    "        return x_new\n",
    "\n",
    "\n",
    "class EdgeFeatures(nn.Module):\n",
    "    \"\"\"Convnet features for edges.\n",
    "\n",
    "    e_ij = U*e_ij + V*(x_i + x_j)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(EdgeFeatures, self).__init__()\n",
    "        self.U = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "        self.V = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features (batch_size, num_nodes, hidden_dim)\n",
    "            e: Edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            e_new: Convolved edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        Ue = self.U(e)\n",
    "        Vx = self.V(x)\n",
    "        Wx = Vx.unsqueeze(1)  # Extend Vx from \"B x V x H\" to \"B x V x 1 x H\"\n",
    "        Vx = Vx.unsqueeze(2)  # extend Vx from \"B x V x H\" to \"B x 1 x V x H\"\n",
    "        e_new = Ue + Vx + Wx\n",
    "        return e_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b768bde9",
   "metadata": {},
   "source": [
    "## Слой Residual Gated GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualGatedGCNLayer(nn.Module):\n",
    "    \"\"\"Convnet layer with gating and residual connection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, aggregation=\"sum\"):\n",
    "        super(ResidualGatedGCNLayer, self).__init__()\n",
    "        self.node_feat = NodeFeatures(hidden_dim, aggregation)\n",
    "        self.edge_feat = EdgeFeatures(hidden_dim)\n",
    "        self.bn_node = BatchNormNode(hidden_dim)\n",
    "        self.bn_edge = BatchNormEdge(hidden_dim)\n",
    "        # self.dropout_layer = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features (batch_size, num_nodes, hidden_dim)\n",
    "            e: Edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            x_new: Convolved node features (batch_size, num_nodes, hidden_dim)\n",
    "            e_new: Convolved edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        e_in = e\n",
    "        x_in = x\n",
    "        # Edge convolution\n",
    "        e_tmp = self.edge_feat(x_in, e_in)  # B x V x V x H\n",
    "        # Compute edge gates\n",
    "        edge_gate = torch.sigmoid(e_tmp)\n",
    "        # Node convolution\n",
    "        x_tmp = self.node_feat(x_in, edge_gate)\n",
    "        # Batch normalization\n",
    "        e_tmp = self.bn_edge(e_tmp)\n",
    "        x_tmp = self.bn_node(x_tmp)\n",
    "        # ReLU Activation\n",
    "        e = F.relu(e_tmp)\n",
    "        x = F.relu(x_tmp)\n",
    "        # Dropout (Optional)\n",
    "        # x = self.dropout_layer(x)\n",
    "        # e = self.dropout_layer(e)\n",
    "        # Residual connection\n",
    "        x_new = x_in + x\n",
    "        e_new = e_in + e\n",
    "        return x_new, e_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63570606",
   "metadata": {},
   "source": [
    "## Модель Residual Gated GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c27055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import TransformerConv\n",
    "\n",
    "class ResidualGatedGCNModel(nn.Module):\n",
    "    \"\"\"Residual Gated GCN Model for outputting predictions as edge adjacency matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, dtypeFloat, dtypeLong):\n",
    "        super(ResidualGatedGCNModel, self).__init__()\n",
    "        self.dtypeFloat = dtypeFloat\n",
    "        self.dtypeLong = dtypeLong\n",
    "        # Define net parameters\n",
    "        self.num_nodes = config['num_nodes']\n",
    "        self.node_dim = config['node_dim']\n",
    "        self.voc_nodes_in = config['voc_nodes_in']\n",
    "        self.voc_nodes_out = config['num_nodes']\n",
    "        self.voc_edges_in = config['voc_edges_in']\n",
    "        self.voc_edges_out = config['voc_edges_out']\n",
    "        self.hidden_dim = config['hidden_dim']\n",
    "        self.num_layers = config['num_layers']\n",
    "        self.mlp_layers = config['mlp_layers']\n",
    "        self.aggregation = config['aggregation']\n",
    "        # Node and edge embedding layers/lookups\n",
    "\n",
    "        # We are using TransformerConv layer from torch geometric library!\n",
    "        self.nodes_coord_embedding = TransformerConv(self.node_dim, self.hidden_dim)\n",
    "\n",
    "        self.edges_values_embedding = nn.Linear(1, self.hidden_dim//2, bias=False)\n",
    "        self.edges_embedding = nn.Embedding(self.voc_edges_in, self.hidden_dim//2)\n",
    "        # Define GCN Layers\n",
    "        gcn_layers = []\n",
    "        for layer in range(self.num_layers):\n",
    "            gcn_layers.append(ResidualGatedGCNLayer(self.hidden_dim, self.aggregation))\n",
    "        self.gcn_layers = nn.ModuleList(gcn_layers)\n",
    "        # Define MLP classifiers\n",
    "        self.mlp_edges = MLP(self.hidden_dim, self.voc_edges_out, self.mlp_layers)\n",
    "\n",
    "    def loss_edges(self, y_pred_edges, y_edges, edge_cw):\n",
    "        \"\"\"\n",
    "        Loss function for edge predictions.\n",
    "\n",
    "        Args:\n",
    "            y_pred_edges: Predictions for edges (batch_size, num_nodes, num_nodes)\n",
    "            y_edges: Targets for edges (batch_size, num_nodes, num_nodes)\n",
    "            edge_cw: Class weights for edges loss\n",
    "\n",
    "        Returns:\n",
    "            loss_edges: Value of loss function\n",
    "\n",
    "        \"\"\"\n",
    "        # Edge loss\n",
    "        y = F.log_softmax(y_pred_edges, dim=3)  # B x V x V x voc_edges\n",
    "        y = y.permute(0, 3, 1, 2)  # B x voc_edges x V x V\n",
    "        loss_edges = nn.NLLLoss(edge_cw)\n",
    "        loss = loss_edges(y.contiguous(), y_edges)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_edges: Input edge adjacency matrix (batch_size, num_nodes, num_nodes)\n",
    "            x_edges_values: Input edge distance matrix (batch_size, num_nodes, num_nodes)\n",
    "            x_nodes: Input nodes (batch_size, num_nodes)\n",
    "            x_nodes_coord: Input node coordinates (batch_size, num_nodes, node_dim)\n",
    "            y_edges: Targets for edges (batch_size, num_nodes, num_nodes)\n",
    "            edge_cw: Class weights for edges loss\n",
    "            # y_nodes: Targets for nodes (batch_size, num_nodes, num_nodes)\n",
    "            # node_cw: Class weights for nodes loss\n",
    "\n",
    "        Returns:\n",
    "            y_pred_edges: Predictions for edges (batch_size, num_nodes, num_nodes)\n",
    "            # y_pred_nodes: Predictions for nodes (batch_size, num_nodes)\n",
    "            loss: Value of loss function\n",
    "        \"\"\"\n",
    "        # Node and edge embedding\n",
    "        edge_index = torch.squeeze(x_edges).nonzero().t().contiguous()\n",
    "        x = self.nodes_coord_embedding(torch.squeeze(x_nodes_coord), edge_index)\n",
    "        x = torch.unsqueeze(x, 0)\n",
    "        e_vals = self.edges_values_embedding(x_edges_values.unsqueeze(3))  # B x V x V x H\n",
    "        e_tags = self.edges_embedding(x_edges)  # B x V x V x H\n",
    "        e = torch.cat((e_vals, e_tags), dim=3)\n",
    "        # GCN layers\n",
    "        for layer in range(self.num_layers):\n",
    "            x, e = self.gcn_layers[layer](x, e)  # B x V x H, B x V x V x H\n",
    "        # MLP classifier\n",
    "        y_pred_edges = self.mlp_edges(e)  # B x V x V x voc_edges_out\n",
    "\n",
    "        # Compute loss\n",
    "        edge_cw = torch.Tensor(edge_cw).type(self.dtypeFloat)  # Convert to tensors\n",
    "        loss = self.loss_edges(y_pred_edges.cuda(), y_edges.cuda(), edge_cw)\n",
    "\n",
    "        return y_pred_edges, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f344d4",
   "metadata": {},
   "source": [
    "## Утилитарные функции (сохранение и загрузка моделей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83032e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Функция для сохранения состояния модели\n",
    "def save_model(net, filepath):\n",
    "    \"\"\"Save model state.\n",
    "\n",
    "    Args:\n",
    "        net: PyTorch model\n",
    "        filepath: Path to save model\n",
    "    \"\"\"\n",
    "    torch.save(net.state_dict(), filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "# Функция для загрузки состояния модели\n",
    "def load_model(model_class, config, filepath, dtypeFloat, dtypeLong): # <-- Принимает model_class\n",
    "    \"\"\"\n",
    "    Load model state into an instance of the specified model_class.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Model file not found: {filepath}\")\n",
    "        return None\n",
    "    try:\n",
    "        # Создаем экземпляр ПРАВИЛЬНОГО класса\n",
    "        model_instance = model_class(config, dtypeFloat, dtypeLong)\n",
    "        # Оборачиваем в DataParallel ПОСЛЕ создания\n",
    "        net = nn.DataParallel(model_instance)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            net.cuda() # Сначала перемещаем обертку и модель на GPU\n",
    "            # Загружаем state_dict с явным указанием map_location на случай,\n",
    "            # если модель сохранялась на другом устройстве\n",
    "            state_dict = torch.load(filepath, map_location=device)\n",
    "            net.load_state_dict(state_dict)\n",
    "        else:\n",
    "            # Загружаем на CPU\n",
    "            state_dict = torch.load(filepath, map_location=torch.device('cpu'))\n",
    "            net.load_state_dict(state_dict)\n",
    "\n",
    "        net.eval() # Переводим в режим оценки\n",
    "        print(f\"Model of type {model_class.__name__} loaded successfully from {filepath}\")\n",
    "        return net\n",
    "    except AttributeError as ae:\n",
    "         # Отлавливаем ошибку несоответствия ключей, которая может возникнуть при load_state_dict\n",
    "         print(f\"Error loading model state from {filepath}: AttributeError likely due to architecture mismatch - {ae}\")\n",
    "         return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {filepath}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Печатаем полный traceback для диагностики\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe133da",
   "metadata": {},
   "source": [
    "## Настройка гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79efe5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Hyperparameters\n",
    "\n",
    "num_nodes = 20 #@param # Could also be 10, 20, or 30!\n",
    "num_neighbors = -1 # Could increase it!\n",
    "train_filepath = f\"./tsp_data/tsp{num_nodes}_train_concorde.txt\"\n",
    "test_filepath = f\"./tsp_data/tsp{num_nodes}_test_concorde.txt\"\n",
    "val_filepath = f\"./tsp_data/tsp{num_nodes}_val_concorde.txt\"\n",
    "hidden_dim = 300 #@param\n",
    "num_layers = 5 #@param\n",
    "mlp_layers = 2 #@param\n",
    "learning_rate = 0.001 #@param\n",
    "max_epochs = 30 #@param\n",
    "batches_per_epoch = 2000\n",
    "\n",
    "variables = {'train_filepath': train_filepath,\n",
    "             'val_filepath': val_filepath,\n",
    "             'test_filepath': test_filepath,\n",
    "             'num_nodes': num_nodes,\n",
    "             'num_neighbors': num_neighbors,\n",
    "             'node_dim': 2 ,\n",
    "             'voc_nodes_in': 2,\n",
    "             'voc_nodes_out': 2,\n",
    "             'voc_edges_in': 3,\n",
    "             'voc_edges_out': 2,\n",
    "             'hidden_dim': hidden_dim,\n",
    "             'num_layers': num_layers,\n",
    "             'mlp_layers': mlp_layers,\n",
    "             'aggregation': 'mean',\n",
    "             'max_epochs': max_epochs,\n",
    "             'val_every': 3,\n",
    "             'test_every': 3,\n",
    "             'batches_per_epoch': batches_per_epoch,\n",
    "             'accumulation_steps': 1,\n",
    "             'learning_rate': learning_rate,\n",
    "             'decay_rate': 1.01,\n",
    "             'batch_size': 1\n",
    "             }\n",
    "net = nn.DataParallel(ResidualGatedGCNModel(variables,  torch.cuda.FloatTensor, torch.cuda.LongTensor))\n",
    "net.cuda()\n",
    "\n",
    "# Compute number of network parameters\n",
    "nb_param = 0\n",
    "for param in net.parameters():\n",
    "    nb_param += np.prod(list(param.data.size()))\n",
    "print('Number of parameters:', nb_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef2182",
   "metadata": {},
   "source": [
    "## Цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d32ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(net, optimizer, config):\n",
    "    # Set training mode\n",
    "    net.train()\n",
    "\n",
    "    # Assign parameters\n",
    "    num_nodes = config['num_nodes']\n",
    "    num_neighbors = config['num_neighbors']\n",
    "    batches_per_epoch = config['batches_per_epoch']\n",
    "    accumulation_steps = config['accumulation_steps']\n",
    "    train_filepath = config['train_filepath']\n",
    "    batch_size = config['batch_size']\n",
    "\n",
    "    # Load TSP data\n",
    "    dataset = GoogleTSPReader(num_nodes, num_neighbors, batch_size, train_filepath)\n",
    "    if batches_per_epoch != -1:\n",
    "        batches_per_epoch = min(batches_per_epoch, dataset.max_iter)\n",
    "    else:\n",
    "        batches_per_epoch = dataset.max_iter\n",
    "\n",
    "    # Convert dataset to iterable\n",
    "    dataset = iter(dataset)\n",
    "\n",
    "    # Initially set loss class weights as None\n",
    "    edge_cw = None\n",
    "\n",
    "    # Initialize running data\n",
    "    running_loss = 0.0\n",
    "    running_nb_data = 0\n",
    "\n",
    "    start_epoch = time.time()\n",
    "    for batch_num in range(batches_per_epoch):\n",
    "        # Generate a batch of TSPs\n",
    "        try:\n",
    "            batch = next(dataset)\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "        # Convert batch to torch Variables\n",
    "        x_edges = Variable(torch.tensor(batch.edges, dtype=torch.int64, device='cuda'))\n",
    "        x_edges_values = Variable(torch.tensor(batch.edges_values, dtype=torch.float32, device='cuda'))\n",
    "        x_nodes = Variable(torch.tensor(batch.nodes, dtype=torch.int64, device='cuda'))\n",
    "        x_nodes_coord = Variable(torch.tensor(batch.nodes_coord, dtype=torch.float32, device='cuda'))\n",
    "        y_edges = Variable(torch.tensor(batch.edges_target, dtype=torch.int64, device='cuda'))\n",
    "        y_nodes = Variable(torch.tensor(batch.nodes_target, dtype=torch.int64, device='cuda'))\n",
    "\n",
    "        # Compute class weights (if uncomputed)\n",
    "        if type(edge_cw) != torch.Tensor:\n",
    "            edge_labels = y_edges.cpu().numpy().flatten()\n",
    "            edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels)\n",
    "\n",
    "        # Forward pass\n",
    "        y_preds, loss = net.forward(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw)\n",
    "        loss = loss.mean()  # Take mean of loss across multiple GPUs\n",
    "        loss = loss / accumulation_steps  # Scale loss by accumulation steps\n",
    "        loss.backward()\n",
    "\n",
    "        # Backward pass\n",
    "        if (batch_num+1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Update running data\n",
    "        running_nb_data += 1\n",
    "        running_loss += loss.data.item()* accumulation_steps  # Re-scale loss\n",
    "\n",
    "    # Compute statistics for full epoch\n",
    "    loss = running_loss/ running_nb_data\n",
    "\n",
    "    return time.time()-start_epoch, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5350e9f4",
   "metadata": {},
   "source": [
    "## Цикл тестирования (валидации)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, config, mode='test'):\n",
    "    # Set evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Assign parameters\n",
    "    num_nodes = config['num_nodes']\n",
    "    num_neighbors = config['num_neighbors']\n",
    "    batches_per_epoch = 1 # config['batches_per_epoch']\n",
    "    val_filepath = config['val_filepath']\n",
    "    test_filepath = config['test_filepath']\n",
    "    batch_size = config['batch_size']\n",
    "\n",
    "    # Load TSP data\n",
    "    if mode == 'val':\n",
    "        dataset = GoogleTSPReader(num_nodes, num_neighbors, batch_size, filepath=val_filepath)\n",
    "    elif mode == 'test':\n",
    "        dataset = GoogleTSPReader(num_nodes, num_neighbors, batch_size, filepath=test_filepath)\n",
    "\n",
    "    # Convert dataset to iterable\n",
    "    dataset = iter(dataset)\n",
    "\n",
    "    # Initially set loss class weights as None\n",
    "    edge_cw = None\n",
    "\n",
    "    # Initialize running data\n",
    "    running_loss = 0.0\n",
    "    running_nb_data = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_test = time.time()\n",
    "        for batch_num in range(batches_per_epoch):\n",
    "            # Generate a batch of TSPs\n",
    "            try:\n",
    "                batch = next(dataset)\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "            # Convert batch to torch Variables\n",
    "            x_edges = Variable(torch.tensor(batch.edges, dtype=torch.int64, device='cuda'))\n",
    "            x_edges_values = Variable(torch.tensor(batch.edges_values, dtype=torch.float32, device='cuda'))\n",
    "            x_nodes = Variable(torch.tensor(batch.nodes, dtype=torch.int64, device='cuda'))\n",
    "            x_nodes_coord = Variable(torch.tensor(batch.nodes_coord, dtype=torch.float32, device='cuda'))\n",
    "            y_edges = Variable(torch.tensor(batch.edges_target, dtype=torch.int64, device='cuda'))\n",
    "            y_nodes = Variable(torch.tensor(batch.nodes_target, dtype=torch.int64, device='cuda'))\n",
    "\n",
    "            # Compute class weights (if uncomputed)\n",
    "            if type(edge_cw) != torch.Tensor:\n",
    "                edge_labels = y_edges.cpu().numpy().flatten()\n",
    "                edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels)\n",
    "\n",
    "            # Forward pass\n",
    "            y_preds, loss = net.forward(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges, edge_cw)\n",
    "            loss = loss.mean()  # Take mean of loss across multiple GPUs\n",
    "\n",
    "            # Update running data\n",
    "            running_nb_data += 1\n",
    "            running_loss += loss.data.item()\n",
    "\n",
    "    # Compute statistics for full epoch\n",
    "    loss = running_loss/ running_nb_data\n",
    "\n",
    "    return time.time()-start_test, loss\n",
    "\n",
    "def update_learning_rate(optimizer, lr):\n",
    "  \"\"\"\n",
    "  Updates learning rate for given optimizer.\n",
    "\n",
    "  Args:\n",
    "      optimizer: Optimizer object\n",
    "      lr: New learning rate\n",
    "\n",
    "  Returns:\n",
    "      optimizer: Updated optimizer objects\n",
    "  \"\"\"\n",
    "  for param_group in optimizer.param_groups:\n",
    "      param_group['lr'] = lr\n",
    "  return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2ed572",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53880702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=variables[\"learning_rate\"])\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=variables['learning_rate'], weight_decay=1e-5)\n",
    "val_loss_old = None\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    # Train\n",
    "    train_time, train_loss = train_one_epoch(net, optimizer, variables)\n",
    "    # Print metrics\n",
    "    train_losses.append(train_loss)\n",
    "    print(f\"Epoch: {epoch}, Train Loss: {train_loss}\")\n",
    "\n",
    "    if epoch % variables[\"val_every\"] == 0 or epoch == variables[\"max_epochs\"]-1:\n",
    "        # Validate\n",
    "        val_time, val_loss = test(net, variables, mode='val')\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Epoch: {epoch}, Val Loss; {val_loss}\")\n",
    "\n",
    "        # Update learning rate\n",
    "        if val_loss_old != None and val_loss > 0.99 * val_loss_old:\n",
    "            variables[\"learning_rate\"] /= variables[\"decay_rate\"]\n",
    "            optimizer = update_learning_rate(optimizer, variables[\"learning_rate\"])\n",
    "\n",
    "        val_loss_old = val_loss  # Update old validation loss\n",
    "\n",
    "    if epoch % variables[\"test_every\"] == 0 or epoch == variables[\"max_epochs\"]-1:\n",
    "        # Test\n",
    "        test_time, test_loss = test(net, variables, mode='test')\n",
    "        test_losses.append(test_loss)\n",
    "        print(f\"Epoch: {epoch}, Test Loss; {test_loss}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a3c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"./tsp_gnn_model_naive.pt\"\n",
    "torch.save(net.state_dict(), filepath)\n",
    "print(f\"Model saved to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e5512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plotting helper functions\n",
    "\n",
    "def plot_loss_curve(train_loss, val_loss, test_loss, config):\n",
    "    \"\"\"\n",
    "    Plot training, validation, and test loss curves.\n",
    "\n",
    "    Parameters:\n",
    "    - train_loss: List of training losses for each epoch\n",
    "    - val_loss: List of validation losses\n",
    "    - test_loss: List of test losses\n",
    "    - config: Dictionary containing plotting configuration\n",
    "    \"\"\"\n",
    "    # Create a figure with a specific size\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Plot training loss (typically at every epoch)\n",
    "    plt.plot(train_loss, color='green', label='Train Loss')\n",
    "\n",
    "    # Plot validation loss (at specified intervals)\n",
    "    val_every = config.get(\"val_every\", 1)\n",
    "    val_x = [i * val_every for i in range(len(val_loss))]\n",
    "    plt.plot(val_x, val_loss, color='orange', label='Val Loss')\n",
    "\n",
    "    # Plot test loss (at specified intervals)\n",
    "    test_every = config.get(\"test_every\", 1)\n",
    "    test_x = [i * test_every for i in range(len(test_loss))]\n",
    "    plt.plot(test_x, test_loss, color='purple', label='Test Loss')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "def plot_tsp_heatmap(p, x_coord, W_val, W_pred, title=\"default\"):\n",
    "    \"\"\"\n",
    "    Helper function to plot predicted TSP tours with edge strength denoting confidence of prediction.\n",
    "\n",
    "    Args:\n",
    "        p: Matplotlib figure/subplot\n",
    "        x_coord: Coordinates of nodes\n",
    "        W_val: Edge values (distance) matrix\n",
    "        W_pred: Edge predictions matrix\n",
    "        title: Title of figure/subplot\n",
    "\n",
    "    Returns:\n",
    "        p: Updated figure/subplot\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def _edges_to_node_pairs(W):\n",
    "        \"\"\"Helper function to convert edge matrix into pairs of adjacent nodes.\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        edge_preds = []\n",
    "        for r in range(len(W)):\n",
    "            for c in range(len(W)):\n",
    "                if W[r][c] > 0.25:\n",
    "                    pairs.append((r, c))\n",
    "                    edge_preds.append(W[r][c])\n",
    "        return pairs, edge_preds\n",
    "\n",
    "    G = nx.Graph(W_val)\n",
    "    pos = dict(zip(range(len(x_coord)), x_coord.tolist()))\n",
    "    node_pairs, edge_color = _edges_to_node_pairs(W_pred)\n",
    "    node_color = ['g'] + ['b'] * (len(x_coord) - 1)  # Green for 0th node, blue for others\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_color, node_size=50)\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=node_pairs, edge_color=edge_color, edge_cmap=plt.cm.Reds, width=0.75)\n",
    "    p.set_title(title)\n",
    "    return p\n",
    "\n",
    "\n",
    "def plot_predictions(x_nodes_coord, x_edges, x_edges_values, y_edges, y_pred_edges, num_plots=3):\n",
    "    \"\"\"\n",
    "    Plots groundtruth TSP tour vs. predicted tours (with beamsearch).\n",
    "\n",
    "    Args:\n",
    "        x_nodes_coord: Input node coordinates (batch_size, num_nodes, node_dim)\n",
    "        x_edges: Input edge adjacency matrix (batch_size, num_nodes, num_nodes)\n",
    "        x_edges_values: Input edge distance matrix (batch_size, num_nodes, num_nodes)\n",
    "        y_edges: Groundtruth labels for edges (batch_size, num_nodes, num_nodes)\n",
    "        y_pred_edges: Predictions for edges (batch_size, num_nodes, num_nodes)\n",
    "        num_plots: Number of figures to plot\n",
    "\n",
    "    \"\"\"\n",
    "    y = F.softmax(y_pred_edges, dim=3)  # B x V x V x voc_edges\n",
    "    y_bins = y.argmax(dim=3)  # Binary predictions: B x V x V\n",
    "    y_probs = y[:,:,:,1]  # Prediction probabilities: B x V x V\n",
    "    for f_idx, idx in enumerate(np.random.choice(len(y), num_plots, replace=False)):\n",
    "        f = plt.figure(f_idx, figsize=(15, 5))\n",
    "        x_coord = x_nodes_coord[idx].cpu().numpy()\n",
    "        W = x_edges[idx].cpu().numpy()\n",
    "        W_val = x_edges_values[idx].cpu().numpy()\n",
    "        W_target = y_edges[idx].cpu().numpy()\n",
    "        W_sol_bins = y_bins[idx].cpu().numpy()\n",
    "        W_sol_probs = y_probs[idx].cpu().numpy()\n",
    "        plt1 = f.add_subplot(131)\n",
    "        plot_tsp(plt1, x_coord.squeeze(), W.squeeze(), W_val.squeeze(), W_target.squeeze(), 'Groundtruth')\n",
    "        plt2 = f.add_subplot(132)\n",
    "        plot_tsp_heatmap(plt2, x_coord.squeeze(), W_val.squeeze(), W_sol_probs.squeeze(), 'Prediction Heatmap')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c8b71c",
   "metadata": {},
   "source": [
    "## Тестирование и визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4c0380",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_ = True\n",
    "dtypeFloat = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "dtypeLong = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor\n",
    "if load_:\n",
    "    MODEL_PATH_V1=\"./tsp_gnn_model_naive.pt\"\n",
    "\n",
    "    if os.path.exists(MODEL_PATH_V1):\n",
    "        try:\n",
    "            # Используем имя класса первой модели\n",
    "            net = load_model(ResidualGatedGCNModel,variables, MODEL_PATH_V1, dtypeFloat, dtypeLong) # Убедитесь, что load_model использует ResidualGatedGCNModel\n",
    "            print(f\"Model 1 ({MODEL_PATH_V1}) loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Model 1: {e}\")\n",
    "    else:\n",
    "        print(f\"Model file not found: {MODEL_PATH_V1}\")\n",
    "\n",
    "net.eval()\n",
    "\n",
    "num_samples = 10\n",
    "num_nodes = variables['num_nodes']\n",
    "num_neighbors = variables['num_neighbors']\n",
    "test_filepath = variables['test_filepath']\n",
    "dataset = iter(GoogleTSPReader(num_nodes, num_neighbors, 1, test_filepath))\n",
    "\n",
    "\n",
    "x_edges = []\n",
    "x_edges_values = []\n",
    "x_nodes = []\n",
    "x_nodes_coord = []\n",
    "y_edges = []\n",
    "y_nodes = []\n",
    "y_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_samples):\n",
    "        sample = next(dataset)\n",
    "        # Convert batch to torch Variables\n",
    "        x_edges.append(Variable(torch.LongTensor(sample.edges).type(dtypeLong), requires_grad=False))\n",
    "        x_edges_values.append(Variable(torch.FloatTensor(sample.edges_values).type(dtypeFloat), requires_grad=False))\n",
    "        x_nodes.append(Variable(torch.LongTensor(sample.nodes).type(dtypeLong), requires_grad=False))\n",
    "        x_nodes_coord.append(Variable(torch.FloatTensor(sample.nodes_coord).type(dtypeFloat), requires_grad=False))\n",
    "        y_edges.append(Variable(torch.LongTensor(sample.edges_target).type(dtypeLong), requires_grad=False))\n",
    "        y_nodes.append(Variable(torch.LongTensor(sample.nodes_target).type(dtypeLong), requires_grad=False))\n",
    "\n",
    "        # Compute class weights\n",
    "        edge_labels = (y_edges[-1].cpu().numpy().flatten())\n",
    "        edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred, loss = net.forward(x_edges[-1], x_edges_values[-1], x_nodes[-1], x_nodes_coord[-1], y_edges[-1], edge_cw)\n",
    "        y_preds.append(y_pred)\n",
    "\n",
    "\n",
    "y_preds = torch.squeeze(torch.stack(y_preds))\n",
    "\n",
    "# Plot prediction visualizations\n",
    "plot_predictions(x_nodes_coord, x_edges, x_edges_values, y_edges, y_preds, num_plots=num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf89bfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curve(train_losses, val_losses, test_losses, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc526cf",
   "metadata": {},
   "source": [
    "## Лучевой поиск (Beam Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffcb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Beam search class\n",
    "class Beamsearch(object):\n",
    "    \"\"\"Class for managing internals of beamsearch procedure.\n",
    "\n",
    "    References:\n",
    "        General: https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/translate/beam.py\n",
    "        For TSP: https://github.com/alexnowakvila/QAP_pt/blob/master/src/tsp/beam_search.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, beam_size, batch_size, num_nodes,\n",
    "                 dtypeFloat=torch.cuda.FloatTensor, dtypeLong=torch.cuda.LongTensor,\n",
    "                 probs_type='raw', random_start=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            beam_size: Beam size\n",
    "            batch_size: Batch size\n",
    "            num_nodes: Number of nodes in TSP tours\n",
    "            dtypeFloat: Float data type (for GPU/CPU compatibility)\n",
    "            dtypeLong: Long data type (for GPU/CPU compatibility)\n",
    "            probs_type: Type of probability values being handled by beamsearch (either 'raw'/'logits'/'argmax'(TODO))\n",
    "            random_start: Flag for using fixed (at node 0) vs. random starting points for beamsearch\n",
    "        \"\"\"\n",
    "        # Beamsearch parameters\n",
    "        self.batch_size = batch_size\n",
    "        self.beam_size = beam_size\n",
    "        self.num_nodes = num_nodes\n",
    "        self.probs_type = probs_type\n",
    "        # Set data types\n",
    "        self.dtypeFloat = dtypeFloat\n",
    "        self.dtypeLong = dtypeLong\n",
    "        # Set beamsearch starting nodes\n",
    "        self.start_nodes = torch.zeros(batch_size, beam_size).type(self.dtypeLong)\n",
    "        if random_start == True:\n",
    "            # Random starting nodes\n",
    "            self.start_nodes = torch.randint(0, num_nodes, (batch_size, beam_size)).type(self.dtypeLong)\n",
    "        # Mask for constructing valid hypothesis\n",
    "        self.mask = torch.ones(batch_size, beam_size, num_nodes).type(self.dtypeFloat)\n",
    "        self.update_mask(self.start_nodes)  # Mask the starting node of the beam search\n",
    "        # Score for each translation on the beam\n",
    "        self.scores = torch.zeros(batch_size, beam_size).type(self.dtypeFloat)\n",
    "        self.all_scores = []\n",
    "        # Backpointers at each time-step\n",
    "        self.prev_Ks = []\n",
    "        # Outputs at each time-step\n",
    "        self.next_nodes = [self.start_nodes]\n",
    "\n",
    "    def get_current_state(self):\n",
    "        \"\"\"Get the output of the beam at the current timestep.\n",
    "        \"\"\"\n",
    "        current_state = (self.next_nodes[-1].unsqueeze(2)\n",
    "                         .expand(self.batch_size, self.beam_size, self.num_nodes))\n",
    "        return current_state\n",
    "\n",
    "    def get_current_origin(self):\n",
    "        \"\"\"Get the backpointers for the current timestep.\n",
    "        \"\"\"\n",
    "        return self.prev_Ks[-1]\n",
    "\n",
    "    def advance(self, trans_probs):\n",
    "        \"\"\"Advances the beam based on transition probabilities.\n",
    "\n",
    "        Args:\n",
    "            trans_probs: Probabilities of advancing from the previous step (batch_size, beam_size, num_nodes)\n",
    "        \"\"\"\n",
    "        # Compound the previous scores (summing logits == multiplying probabilities)\n",
    "        if len(self.prev_Ks) > 0:\n",
    "            if self.probs_type == 'raw':\n",
    "                beam_lk = trans_probs * self.scores.unsqueeze(2).expand_as(trans_probs)\n",
    "            elif self.probs_type == 'logits':\n",
    "                beam_lk = trans_probs + self.scores.unsqueeze(2).expand_as(trans_probs)\n",
    "        else:\n",
    "            beam_lk = trans_probs\n",
    "            # Only use the starting nodes from the beam\n",
    "            if self.probs_type == 'raw':\n",
    "                beam_lk[:, 1:] = torch.zeros(beam_lk[:, 1:].size()).type(self.dtypeFloat)\n",
    "            elif self.probs_type == 'logits':\n",
    "                beam_lk[:, 1:] = -1e20 * torch.ones(beam_lk[:, 1:].size()).type(self.dtypeFloat)\n",
    "        # Multiply by mask\n",
    "        beam_lk = beam_lk * self.mask\n",
    "        beam_lk = beam_lk.view(self.batch_size, -1)  # (batch_size, beam_size * num_nodes)\n",
    "        # Get top k scores and indexes (k = beam_size)\n",
    "        bestScores, bestScoresId = beam_lk.topk(self.beam_size, 1, True, True)\n",
    "        # Update scores\n",
    "        self.scores = bestScores\n",
    "        # Update backpointers\n",
    "        prev_k = bestScoresId // self.num_nodes  # integer division\n",
    "        self.prev_Ks.append(prev_k)\n",
    "        # Update outputs\n",
    "        new_nodes = bestScoresId % self.num_nodes  # remainder gives the node index\n",
    "        self.next_nodes.append(new_nodes)\n",
    "        # Re-index mask\n",
    "        perm_mask = prev_k.unsqueeze(2).expand_as(self.mask).type(self.dtypeLong)  # (batch_size, beam_size, num_nodes)\n",
    "        self.mask = self.mask.gather(1, perm_mask)\n",
    "        # Mask newly added nodes\n",
    "        self.update_mask(new_nodes)\n",
    "\n",
    "    def update_mask(self, new_nodes):\n",
    "        \"\"\"Sets new_nodes to zero in mask.\n",
    "        \"\"\"\n",
    "        arr = (torch.arange(0, self.num_nodes).unsqueeze(0).unsqueeze(1)\n",
    "               .expand_as(self.mask).type(self.dtypeLong))\n",
    "        new_nodes = new_nodes.unsqueeze(2).expand_as(self.mask)\n",
    "        update_mask = 1 - torch.eq(arr, new_nodes).type(self.dtypeFloat)\n",
    "        self.mask = self.mask * update_mask\n",
    "        if self.probs_type == 'logits':\n",
    "            # Convert 0s in mask to inf\n",
    "            self.mask[self.mask == 0] = 1e20\n",
    "\n",
    "    def sort_best(self):\n",
    "        \"\"\"Sort the beam.\n",
    "        \"\"\"\n",
    "        return torch.sort(self.scores, 0, True)\n",
    "\n",
    "    def get_best(self):\n",
    "        \"\"\"Get the score and index of the best hypothesis in the beam.\n",
    "        \"\"\"\n",
    "        scores, ids = self.sort_best()\n",
    "        return scores[1], ids[1]\n",
    "\n",
    "    def get_hypothesis(self, k):\n",
    "        \"\"\"Walk back to construct the full hypothesis.\n",
    "\n",
    "        Args:\n",
    "            k: Position in the beam to construct (usually 0s for most probable hypothesis)\n",
    "        \"\"\"\n",
    "        assert self.num_nodes == len(self.prev_Ks) + 1\n",
    "        hyp = -1 * torch.ones(self.batch_size, self.num_nodes).type(self.dtypeLong)\n",
    "        for j in range(len(self.prev_Ks) - 1, -2, -1):\n",
    "            hyp[:, j + 1] = self.next_nodes[j + 1].gather(1, k).view(1, self.batch_size)\n",
    "            k = self.prev_Ks[j].type(self.dtypeLong).gather(1, k)\n",
    "        return hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1557168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Beam search helper functions\n",
    "def W_to_tour_len(W, W_values):\n",
    "    \"\"\"Helper function to calculate tour length from edge adjacency matrix.\n",
    "    \"\"\"\n",
    "    tour_len = 0\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            if W[i][j] == 1:\n",
    "                tour_len += W_values[i][j]\n",
    "    tour_len /= 2  # Divide by 2 because adjacency matrices are symmetric\n",
    "    return tour_len\n",
    "\n",
    "\n",
    "def tour_nodes_to_W(nodes):\n",
    "    \"\"\"Helper function to convert ordered list of tour nodes to edge adjacency matrix.\n",
    "    \"\"\"\n",
    "    W = np.zeros((len(nodes), len(nodes)))\n",
    "    for idx in range(len(nodes) - 1):\n",
    "        i = int(nodes[idx])\n",
    "        j = int(nodes[idx + 1])\n",
    "        W[i][j] = 1\n",
    "        W[j][i] = 1\n",
    "    # Add final connection of tour in edge target\n",
    "    W[j][int(nodes[0])] = 1\n",
    "    W[int(nodes[0])][j] = 1\n",
    "    return W\n",
    "\n",
    "def tour_nodes_to_tour_len(nodes, W_values):\n",
    "    \"\"\"Helper function to calculate tour length from ordered list of tour nodes.\n",
    "    \"\"\"\n",
    "    tour_len = 0\n",
    "    for idx in range(len(nodes) - 1):\n",
    "        i = nodes[idx]\n",
    "        j = nodes[idx + 1]\n",
    "        tour_len += W_values[i][j]\n",
    "    # Add final connection of tour in edge target\n",
    "    tour_len += W_values[j][nodes[0]]\n",
    "    return tour_len\n",
    "\n",
    "\n",
    "def is_valid_tour(nodes, num_nodes):\n",
    "    \"\"\"Sanity check: tour visits all nodes given.\n",
    "    \"\"\"\n",
    "    return sorted(nodes) == [i for i in range(num_nodes)]\n",
    "\n",
    "\n",
    "def mean_tour_len_edges(x_edges_values, y_pred_edges):\n",
    "    \"\"\"\n",
    "    Computes mean tour length for given batch prediction as edge adjacency matrices (for PyTorch tensors).\n",
    "\n",
    "    Args:\n",
    "        x_edges_values: Edge values (distance) matrix (batch_size, num_nodes, num_nodes)\n",
    "        y_pred_edges: Edge predictions (batch_size, num_nodes, num_nodes, voc_edges)\n",
    "\n",
    "    Returns:\n",
    "        mean_tour_len: Mean tour length over batch\n",
    "    \"\"\"\n",
    "    y = F.softmax(y_pred_edges, dim=3)  # B x V x V x voc_edges\n",
    "    y = y.argmax(dim=3)  # B x V x V\n",
    "    # Divide by 2 because edges_values is symmetric\n",
    "    tour_lens = (y.float() * x_edges_values.float()).sum(dim=1).sum(dim=1) / 2\n",
    "    mean_tour_len = tour_lens.sum().to(dtype=torch.float).item() / tour_lens.numel()\n",
    "    return mean_tour_len\n",
    "\n",
    "\n",
    "def mean_tour_len_nodes(x_edges_values, bs_nodes):\n",
    "    \"\"\"\n",
    "    Computes mean tour length for given batch prediction as node ordering after beamsearch (for Pytorch tensors).\n",
    "\n",
    "    Args:\n",
    "        x_edges_values: Edge values (distance) matrix (batch_size, num_nodes, num_nodes)\n",
    "        bs_nodes: Node orderings (batch_size, num_nodes)\n",
    "\n",
    "    Returns:\n",
    "        mean_tour_len: Mean tour length over batch\n",
    "    \"\"\"\n",
    "    y = bs_nodes.cpu().numpy()\n",
    "    W_val = x_edges_values.cpu().numpy()\n",
    "    running_tour_len = 0\n",
    "    for batch_idx in range(y.shape[0]):\n",
    "        for y_idx in range(y[batch_idx].shape[0] - 1):\n",
    "            i = y[batch_idx][y_idx]\n",
    "            j = y[batch_idx][y_idx + 1]\n",
    "            running_tour_len += W_val[batch_idx][i][j]\n",
    "        running_tour_len += W_val[batch_idx][j][0]  # Add final connection to tour/cycle\n",
    "    return running_tour_len / y.shape[0]\n",
    "\n",
    "\n",
    "def beamsearch_tour_nodes(y_pred_edges, beam_size, batch_size, num_nodes, dtypeFloat, dtypeLong, probs_type='raw', random_start=False):\n",
    "    \"\"\"\n",
    "    Performs beamsearch procedure on edge prediction matrices and returns possible TSP tours.\n",
    "\n",
    "    Args:\n",
    "        y_pred_edges: Predictions for edges (batch_size, num_nodes, num_nodes)\n",
    "        beam_size: Beam size\n",
    "        batch_size: Batch size\n",
    "        num_nodes: Number of nodes in TSP tours\n",
    "        dtypeFloat: Float data type (for GPU/CPU compatibility)\n",
    "        dtypeLong: Long data type (for GPU/CPU compatibility)\n",
    "        random_start: Flag for using fixed (at node 0) vs. random starting points for beamsearch\n",
    "\n",
    "    Returns: TSP tours in terms of node ordering (batch_size, num_nodes)\n",
    "\n",
    "    \"\"\"\n",
    "    if probs_type == 'raw':\n",
    "        # Compute softmax over edge prediction matrix\n",
    "        print(\"y_pred_edges shape:\", y_pred_edges.shape)\n",
    "        print(\"y_pred_edges: \", y_pred_edges)\n",
    "        y = F.softmax(y_pred_edges, dim=3)  # B x V x V x voc_edges\n",
    "        print(\"y shape:\", y.shape)\n",
    "        print(\"y: \", y)\n",
    "        # Consider the second dimension only\n",
    "        y = y[:, :, :, 1]  # B x V x V\n",
    "        print(\"y shape:\", y.shape)\n",
    "        print(\"y: \", y)\n",
    "    elif probs_type == 'logits':\n",
    "        # Compute logits over edge prediction matrix\n",
    "        y = F.log_softmax(y_pred_edges, dim=3)  # B x V x V x voc_edges\n",
    "        # Consider the second dimension only\n",
    "        y = y[:, :, :, 1]  # B x V x V\n",
    "        y[y == 0] = -1e-20  # Set 0s (i.e. log(1)s) to very small negative number\n",
    "    # Perform beamsearch\n",
    "    beamsearch = Beamsearch(beam_size, batch_size, num_nodes, dtypeFloat, dtypeLong, probs_type, random_start)\n",
    "    trans_probs = y.gather(1, beamsearch.get_current_state())\n",
    "    print(\"Initial beamsearch state:\", beamsearch.get_current_state())\n",
    "    print(\"Transition probabilities shape:\", trans_probs.shape)\n",
    "    print(\"Transition probabilities:\", trans_probs)\n",
    "    for step in range(num_nodes - 1):\n",
    "        beamsearch.advance(trans_probs)\n",
    "        trans_probs = y.gather(1, beamsearch.get_current_state().type(dtypeLong))\n",
    "        print(f\"Step {step}: current state = {beamsearch.get_current_state()}\")\n",
    "        print(f\"Step {step}: transition probabilities = {trans_probs}\")\n",
    "    # Find TSP tour with highest probability among beam_size candidates\n",
    "    ends = torch.zeros(batch_size, 1).type(dtypeLong)\n",
    "    hyp = beamsearch.get_hypothesis(ends)\n",
    "    print(\"Hypotheses shape:\", hyp.shape)\n",
    "    print(\"Hypotheses:\", hyp)\n",
    "    return hyp\n",
    "\n",
    "\n",
    "def beamsearch_tour_nodes_shortest(y_pred_edges, x_edges_values, beam_size, batch_size, num_nodes,\n",
    "                                   dtypeFloat, dtypeLong, probs_type='raw', random_start=False):\n",
    "    \"\"\"\n",
    "    Performs beamsearch procedure on edge prediction matrices and returns possible TSP tours.\n",
    "\n",
    "    Final predicted tour is the one with the shortest tour length.\n",
    "    (Standard beamsearch returns the one with the highest probability and does not take length into account.)\n",
    "\n",
    "    Args:\n",
    "        y_pred_edges: Predictions for edges (batch_size, num_nodes, num_nodes)\n",
    "        x_edges_values: Input edge distance matrix (batch_size, num_nodes, num_nodes)\n",
    "        beam_size: Beam size\n",
    "        batch_size: Batch size\n",
    "        num_nodes: Number of nodes in TSP tours\n",
    "        dtypeFloat: Float data type (for GPU/CPU compatibility)\n",
    "        dtypeLong: Long data type (for GPU/CPU compatibility)\n",
    "        probs_type: Type of probability values being handled by beamsearch (either 'raw'/'logits'/'argmax'(TODO))\n",
    "        random_start: Flag for using fixed (at node 0) vs. random starting points for beamsearch\n",
    "\n",
    "    Returns:\n",
    "        shortest_tours: TSP tours in terms of node ordering (batch_size, num_nodes)\n",
    "\n",
    "    \"\"\"\n",
    "    if probs_type == 'raw':\n",
    "        # Compute softmax over edge prediction matrix\n",
    "        y = F.softmax(y_pred_edges, dim=3)  # B x V x V x voc_edges\n",
    "        # Consider the second dimension only\n",
    "        y = y[:, :, :, 1]  # B x V x V\n",
    "    elif probs_type == 'logits':\n",
    "        # Compute logits over edge prediction matrix\n",
    "        y = F.log_softmax(y_pred_edges, dim=3)  # B x V x V x voc_edges\n",
    "        # Consider the second dimension only\n",
    "        y = y[:, :, :, 1]  # B x V x V\n",
    "        y[y == 0] = -1e-20  # Set 0s (i.e. log(1)s) to very small negative number\n",
    "    # Perform beamsearch\n",
    "    beamsearch = Beamsearch(beam_size, batch_size, num_nodes, dtypeFloat, dtypeLong, probs_type, random_start)\n",
    "    trans_probs = y.gather(1, beamsearch.get_current_state())\n",
    "    for step in range(num_nodes - 1):\n",
    "        beamsearch.advance(trans_probs)\n",
    "        trans_probs = y.gather(1, beamsearch.get_current_state().type(dtypeLong))\n",
    "    # Initially assign shortest_tours as most probable tours i.e. standard beamsearch\n",
    "    ends = torch.zeros(batch_size, 1).type(dtypeLong)\n",
    "    shortest_tours = beamsearch.get_hypothesis(ends)\n",
    "    # Compute current tour lengths\n",
    "    shortest_lens = [1e6] * len(shortest_tours)\n",
    "    for idx in range(len(shortest_tours)):\n",
    "        shortest_lens[idx] = tour_nodes_to_tour_len(shortest_tours[idx].cpu().numpy(),\n",
    "                                                    x_edges_values[idx].cpu().numpy())\n",
    "    # Iterate over all positions in beam (except position 0 --> highest probability)\n",
    "    for pos in range(1, beam_size):\n",
    "        ends = pos * torch.ones(batch_size, 1).type(dtypeLong)  # New positions\n",
    "        hyp_tours = beamsearch.get_hypothesis(ends)\n",
    "        for idx in range(len(hyp_tours)):\n",
    "            hyp_nodes = hyp_tours[idx].cpu().numpy()\n",
    "            hyp_len = tour_nodes_to_tour_len(hyp_nodes, x_edges_values[idx].cpu().numpy())\n",
    "            # Replace tour in shortest_tours if new length is shorter than current best\n",
    "            if hyp_len < shortest_lens[idx] and is_valid_tour(hyp_nodes, num_nodes):\n",
    "                shortest_tours[idx] = hyp_tours[idx]\n",
    "                shortest_lens[idx] = hyp_len\n",
    "    return shortest_tours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206c6694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_with_beam_search(x_nodes_coord, x_edges, x_edges_values, y_edges, y_pred_edges, beam_search_predictions, num_plots=3):\n",
    "    \"\"\"\n",
    "    Plots groundtruth TSP tour, predicted tours (with beamsearch), and beam search predictions.\n",
    "\n",
    "    Args:\n",
    "        x_nodes_coord: Input node coordinates (batch_size, num_nodes, node_dim)\n",
    "        x_edges: Input edge adjacency matrix (batch_size, num_nodes, num_nodes)\n",
    "        x_edges_values: Input edge distance matrix (batch_size, num_nodes, num_nodes)\n",
    "        y_edges: Groundtruth labels for edges (batch_size, num_nodes, num_nodes)\n",
    "        y_pred_edges: Predictions for edges (batch_size, num_nodes, num_nodes)\n",
    "        beam_search_predictions: Predicted node ordering from beam search (batch_size, num_nodes)\n",
    "        num_plots: Number of figures to plot\n",
    "    \"\"\"\n",
    "    y = F.softmax(y_pred_edges, dim=3)  # B x V x V x voc_edges\n",
    "    y_bins = y.argmax(dim=3)  # Binary predictions: B x V x V\n",
    "    y_probs = y[:,:,:,1]  # Prediction probabilities: B x V x V\n",
    "\n",
    "    for f_idx, idx in enumerate(np.random.choice(len(y), num_plots, replace=False)):\n",
    "        f = plt.figure(f_idx, figsize=(15, 6))\n",
    "\n",
    "        # Extract relevant data for the selected sample\n",
    "        x_coord = x_nodes_coord[idx].cpu().numpy()\n",
    "        W = x_edges[idx].cpu().numpy()\n",
    "        W_val = x_edges_values[idx].cpu().numpy()\n",
    "        W_target = y_edges[idx].cpu().numpy()\n",
    "        W_sol_bins = y_bins[idx].cpu().numpy()\n",
    "        W_sol_probs = y_probs[idx].cpu().numpy()\n",
    "\n",
    "        # Groundtruth plot\n",
    "        plt1 = f.add_subplot(131)\n",
    "        plot_tsp(plt1, x_coord.squeeze(), W.squeeze(), W_val.squeeze(), W_target.squeeze(), 'Groundtruth')\n",
    "\n",
    "        # Prediction heatmap plot\n",
    "        plt2 = f.add_subplot(132)\n",
    "        plot_tsp_heatmap(plt2, x_coord.squeeze(), W_val.squeeze(), W_sol_probs.squeeze(), 'Prediction Heatmap')\n",
    "\n",
    "        # Beam search plot\n",
    "        beam_search_route = beam_search_predictions[idx].cpu().numpy().tolist()  # Convert to list of node indices\n",
    "        W_pred = tour_nodes_to_W(beam_search_route)  # Convert route to edge matrix\n",
    "\n",
    "        # Compute the tour length for the beam search tour\n",
    "        tour_length = tour_nodes_to_tour_len(beam_search_route, W_val.squeeze())\n",
    "\n",
    "        # Dummy target (since we don't have groundtruth for beam search)\n",
    "        dummy_target = np.zeros_like(W_val.squeeze())\n",
    "\n",
    "        # Beam search tour plot\n",
    "        plt3 = f.add_subplot(133)\n",
    "        plot_tsp(plt3, x_coord.squeeze(), W_pred, W_val.squeeze(), dummy_target,\n",
    "                 title=f\"Beam Search Tour (Length: {tour_length:.2f})\")\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define beam search parameters:\n",
    "beam_size = 5  # or any beam size you prefer\n",
    "batch_size = y_preds.shape[0]\n",
    "num_nodes = y_preds.shape[1]\n",
    "dtypeFloat = torch.cuda.FloatTensor if y_preds.is_cuda else torch.FloatTensor\n",
    "dtypeLong = torch.cuda.LongTensor if y_preds.is_cuda else torch.LongTensor\n",
    "\n",
    "# Call the beam search helper function for node ordering based on edge predictions:\n",
    "predicted_tours = beamsearch_tour_nodes(\n",
    "    y_preds,\n",
    "    beam_size,\n",
    "    batch_size,\n",
    "    num_nodes,\n",
    "    dtypeFloat,\n",
    "    dtypeLong,\n",
    "    probs_type='raw',    # or 'logits'\n",
    "    random_start=True\n",
    ")\n",
    "\n",
    "print(\"Predicted TSP tours:\", predicted_tours)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970dddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions_with_beam_search(x_nodes_coord, x_edges, x_edges_values, y_edges, y_preds, predicted_tours, num_plots=num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8685124c",
   "metadata": {},
   "source": [
    "## Модель Residual Gated GNN (Упрощенная модель без TransformerConv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba24304",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualGatedGCNLayer_v2(nn.Module):\n",
    "    \"\"\"Residual Gated GCN layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, aggregation=\"mean\"):\n",
    "        super(ResidualGatedGCNLayer_v2, self).__init__()\n",
    "        self.node_feat = NodeFeatures(hidden_dim, aggregation)\n",
    "        self.edge_feat = EdgeFeatures(hidden_dim)\n",
    "        self.bn_node = BatchNormNode(hidden_dim)\n",
    "        self.bn_edge = BatchNormEdge(hidden_dim)\n",
    "        self.dropout_layer = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features (batch_size, num_nodes, hidden_dim)\n",
    "            e: Edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            x_new: Updated node features (batch_size, num_nodes, hidden_dim)\n",
    "            e_new: Updated edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        # Gate edge features\n",
    "        edge_gate = torch.sigmoid(e)\n",
    "\n",
    "        # Node and edge feature transformation\n",
    "        x_new = self.node_feat(x, edge_gate)\n",
    "        e_new = self.edge_feat(x, e)\n",
    "\n",
    "        # Batch normalization\n",
    "        x_new = self.bn_node(x_new)\n",
    "        e_new = self.bn_edge(e_new)\n",
    "\n",
    "        # Residual connection\n",
    "        x_new = F.relu(x + x_new)\n",
    "        e_new = F.relu(e + e_new)\n",
    "\n",
    "        x_new = self.dropout_layer(x_new)\n",
    "        e_new = self.dropout_layer(e_new)\n",
    "\n",
    "        return x_new, e_new\n",
    "\n",
    "class ResidualGatedGCNModel_v2(nn.Module):\n",
    "    \"\"\"Residual Gated GCN Model for TSP.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, dtypeFloat, dtypeLong):\n",
    "        super(ResidualGatedGCNModel_v2, self).__init__()\n",
    "        self.config = config\n",
    "        self.dtypeFloat = dtypeFloat\n",
    "        self.dtypeLong = dtypeLong\n",
    "\n",
    "        # Initial embedding layers\n",
    "        self.node_embedding = nn.Linear(config['node_dim'], config['hidden_dim'], bias=False)\n",
    "        self.edge_embedding = nn.Linear(1, config['hidden_dim'], bias=False)\n",
    "\n",
    "        # GCN layers\n",
    "        gcn_layers = []\n",
    "        for layer in range(config['num_layers']):\n",
    "            gcn_layers.append(ResidualGatedGCNLayer_v2(config['hidden_dim'], config['aggregation']))\n",
    "        self.gcn_layers = nn.ModuleList(gcn_layers)\n",
    "\n",
    "        # MLP for edge classification\n",
    "        self.mlp_edges = MLP(config['hidden_dim'], config['voc_edges_out'], config['mlp_layers'])\n",
    "\n",
    "    def forward(self, x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges=None, edge_cw=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_edges: Input edge connectivity (batch_size, num_nodes, num_nodes)\n",
    "            x_edges_values: Input edge distances (batch_size, num_nodes, num_nodes)\n",
    "            x_nodes: Input node features (batch_size, num_nodes)\n",
    "            x_nodes_coord: Input node coordinates (batch_size, num_nodes, node_dim)\n",
    "            y_edges: Edge labels (batch_size, num_nodes, num_nodes)\n",
    "            edge_cw: Class weights for edge loss\n",
    "\n",
    "        Returns:\n",
    "            y_pred_edges: Edge predictions (batch_size, num_nodes, num_nodes, voc_edges_out)\n",
    "            loss: Cross-entropy loss\n",
    "        \"\"\"\n",
    "        # Problem size\n",
    "        batch_size, num_nodes = x_nodes.size()\n",
    "\n",
    "        # Initial embeddings\n",
    "        x = self.node_embedding(x_nodes_coord)  # B x V x H\n",
    "\n",
    "        # Create edge features\n",
    "        edge_feat = x_edges_values.unsqueeze(3)  # B x V x V x 1\n",
    "        e = self.edge_embedding(edge_feat)  # B x V x V x H\n",
    "\n",
    "        # Apply GCN layers\n",
    "        for layer in self.gcn_layers:\n",
    "            x, e = layer(x, e)\n",
    "\n",
    "        # MLP classifier for edges\n",
    "        # To apply the MLP on each edge, we reshape the tensor\n",
    "        e_flat = e.reshape(-1, self.config['hidden_dim'])  # (B*V*V) x H\n",
    "        y_pred_edges_flat = self.mlp_edges(e_flat)  # (B*V*V) x 2\n",
    "        y_pred_edges = y_pred_edges_flat.reshape(batch_size, num_nodes, num_nodes, -1)  # B x V x V x 2\n",
    "\n",
    "        # Compute loss if training\n",
    "        if y_edges is not None:\n",
    "            # Convert edge predictions and labels for loss computation\n",
    "            # We need to flatten them to (batch_size*num_nodes*num_nodes, num_classes)\n",
    "            y_pred_edges_flat = y_pred_edges.reshape(-1, self.config['voc_edges_out'])\n",
    "            y_edges_flat = y_edges.reshape(-1)\n",
    "            \n",
    "            device = y_pred_edges_flat.device # Определяем устройство\n",
    "\n",
    "            loss = None # Инициализируем loss\n",
    "            # Compute loss with class weights if provided\n",
    "            if edge_cw is not None:\n",
    "                if not isinstance(edge_cw, torch.Tensor):\n",
    "                     # Если вдруг передали не тензор (хотя должны тензор dummy_edge_cw)\n",
    "                     edge_cw = torch.tensor(edge_cw, dtype=torch.float, device=device)\n",
    "                else:\n",
    "                     # Просто перемещаем на device и проверяем тип\n",
    "                     edge_cw = edge_cw.to(device=device, dtype=torch.float)\n",
    "\n",
    "                loss_fct = nn.CrossEntropyLoss(weight=edge_cw)\n",
    "                loss = loss_fct(y_pred_edges_flat, y_edges_flat)\n",
    "            else:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(y_pred_edges_flat, y_edges_flat)\n",
    "\n",
    "            return y_pred_edges, loss\n",
    "        else:\n",
    "            return y_pred_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ab3b44",
   "metadata": {},
   "source": [
    "# Гибридные методы решения задачи коммивояжера"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db368e85",
   "metadata": {},
   "source": [
    "### Метод имитации отжига + 2-opt (Simulated Annealing + 2-opt outer loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff82176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "class TSP_2opt_SA_outer:\n",
    "    def __init__(self, distance_matrix):\n",
    "        \"\"\"\n",
    "        Initialize the solver with a distance matrix.\n",
    "\n",
    "        Args:\n",
    "            distance_matrix: 2D numpy array where distance_matrix[i][j] is the distance from city i to city j\n",
    "        \"\"\"\n",
    "        self.distances = distance_matrix\n",
    "        self.num_cities = len(distance_matrix)\n",
    "\n",
    "    def calculate_tour_length(self, tour):\n",
    "        \"\"\"Calculate the total length of a tour.\"\"\"\n",
    "        return sum(self.distances[tour[i]][tour[(i + 1) % self.num_cities]] for i in range(self.num_cities))\n",
    "\n",
    "    def two_opt_swap(self, tour, i, j):\n",
    "        \"\"\"\n",
    "        Perform a 2-opt swap by reversing the segment between positions i and j.\n",
    "        Returns a new tour.\n",
    "        \"\"\"\n",
    "        new_tour = tour.copy()\n",
    "        new_tour[i:j+1] = new_tour[i:j+1][::-1]\n",
    "        return new_tour\n",
    "\n",
    "    def delta_two_opt(self, tour, i, j):\n",
    "        \"\"\"\n",
    "        Calculate the cost difference (delta) for a 2-opt swap between positions i and j in O(1) time.\n",
    "        \"\"\"\n",
    "        n = self.num_cities\n",
    "        a, b = tour[i - 1], tour[i]\n",
    "        c, d = tour[j], tour[(j + 1) % n]\n",
    "        delta = (self.distances[a][c] + self.distances[b][d]) - (self.distances[a][b] + self.distances[c][d])\n",
    "        return delta\n",
    "\n",
    "    def get_random_neighbor(self, tour):\n",
    "        \"\"\"\n",
    "        Get a random neighbor by performing a 2-opt swap between two random indices.\n",
    "        Returns the new tour and the cost difference (delta).\n",
    "        \"\"\"\n",
    "        i, j = sorted(random.sample(range(self.num_cities), 2))\n",
    "        new_tour = self.two_opt_swap(tour, i, j)\n",
    "        delta = self.delta_two_opt(tour, i, j)\n",
    "        return new_tour, delta, i, j\n",
    "\n",
    "    def acceptance_probability(self, delta, temperature):\n",
    "        \"\"\"\n",
    "        Calculate the acceptance probability.\n",
    "        Returns 1.0 for improvements.\n",
    "        \"\"\"\n",
    "        return 1.0 if delta < 0 else math.exp(-delta / temperature)\n",
    "\n",
    "    def double_bridge_move(self, tour):\n",
    "        \"\"\"\n",
    "        Perform a double-bridge move to perturb the current tour.\n",
    "        This move breaks the tour into four segments and rearranges them.\n",
    "        \"\"\"\n",
    "        n = self.num_cities\n",
    "        pos1 = random.randint(1, n // 4)\n",
    "        pos2 = random.randint(pos1 + 1, n // 2)\n",
    "        pos3 = random.randint(pos2 + 1, 3 * n // 4)\n",
    "        new_tour = tour[0:pos1] + tour[pos3:] + tour[pos2:pos3] + tour[pos1:pos2]\n",
    "        return new_tour\n",
    "\n",
    "    def solve_sa(self, initial_tour, sa_params, use_perturbation=True):\n",
    "        \"\"\"\n",
    "        Run the SA phase from a given starting tour.\n",
    "        Returns the best tour, its length, number of iterations, and elapsed time.\n",
    "        \"\"\"\n",
    "        current_tour = initial_tour.copy()\n",
    "        current_length = self.calculate_tour_length(current_tour)\n",
    "        best_tour, best_length = current_tour.copy(), current_length\n",
    "\n",
    "        temperature = sa_params.get('initial_temp', 500.0)\n",
    "        cooling_rate = sa_params.get('cooling_rate', 0.9995)\n",
    "        min_temp = sa_params.get('min_temp', 1e-6)\n",
    "        max_iterations = sa_params.get('max_iterations', 10000)\n",
    "        max_no_improve = sa_params.get('max_no_improve', 1000)\n",
    "        perturb_interval = sa_params.get('perturb_interval', 500)\n",
    "        perturb_prob = sa_params.get('perturb_prob', 0.1)\n",
    "\n",
    "        iterations = 0\n",
    "        no_improve = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        while iterations < max_iterations and temperature > min_temp and no_improve < max_no_improve:\n",
    "            iterations += 1\n",
    "            # Optionally apply a perturbation\n",
    "            if use_perturbation and iterations % perturb_interval == 0 and random.random() < perturb_prob:\n",
    "                perturbed = self.double_bridge_move(current_tour)\n",
    "                perturbed_length = self.calculate_tour_length(perturbed)\n",
    "                # Accept perturbation if it improves or with small probability\n",
    "                if perturbed_length < current_length or random.random() < 0.2:\n",
    "                    current_tour = perturbed\n",
    "                    current_length = perturbed_length\n",
    "                    no_improve = 0\n",
    "\n",
    "            neighbor, delta, i_swap, j_swap = self.get_random_neighbor(current_tour)\n",
    "            if self.acceptance_probability(delta, temperature) > random.random():\n",
    "                current_tour = neighbor\n",
    "                current_length += delta\n",
    "                if current_length < best_length:\n",
    "                    best_tour = current_tour.copy()\n",
    "                    best_length = current_length\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "            else:\n",
    "                no_improve += 1\n",
    "            temperature *= cooling_rate\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        return best_tour, best_length, iterations, elapsed\n",
    "\n",
    "    def improve_tour_with_2opt(self, tour, max_iterations=500):\n",
    "        \"\"\"\n",
    "        Run deterministic 2-opt local search.\n",
    "        Returns the improved tour, its length, and the number of iterations.\n",
    "        \"\"\"\n",
    "        best_tour = tour.copy()\n",
    "        best_length = self.calculate_tour_length(best_tour)\n",
    "        iterations = 0\n",
    "        improved = True\n",
    "\n",
    "        while improved and iterations < max_iterations:\n",
    "            improved = False\n",
    "            iterations += 1\n",
    "            for i in range(1, self.num_cities - 1):\n",
    "                for j in range(i + 1, self.num_cities):\n",
    "                    if i == 1 and j == self.num_cities - 1:\n",
    "                        continue  # Avoid trivial full reversal\n",
    "                    delta = self.delta_two_opt(best_tour, i, j)\n",
    "                    if delta < 0:\n",
    "                        best_tour = self.two_opt_swap(best_tour, i, j)\n",
    "                        best_length += delta\n",
    "                        improved = True\n",
    "                        break\n",
    "                if improved:\n",
    "                    break\n",
    "        return best_tour, best_length, iterations\n",
    "\n",
    "    def solve(self, outer_iterations=10, sa_params=None, local_search_after_sa=True, use_perturbation=True):\n",
    "        \"\"\"\n",
    "        Hybrid iterated approach: repeatedly run SA followed by (optional) 2-opt.\n",
    "        This iterated local search allows restarting from perturbed solutions.\n",
    "\n",
    "        Args:\n",
    "            outer_iterations: Number of outer iterations.\n",
    "            sa_params: Parameters for the SA phase.\n",
    "            local_search_after_sa: Whether to perform 2-opt after SA.\n",
    "            use_perturbation: Whether to use the double-bridge perturbation in SA.\n",
    "\n",
    "        Returns:\n",
    "            overall_best_tour, overall_best_length: Best solution found.\n",
    "        \"\"\"\n",
    "        if sa_params is None:\n",
    "            sa_params = {\n",
    "                'initial_temp': 500.0,\n",
    "                'cooling_rate': 0.9995,\n",
    "                'min_temp': 1e-6,\n",
    "                'max_iterations': 10000,\n",
    "                'max_no_improve': 1000,\n",
    "                'perturb_interval': 500,\n",
    "                'perturb_prob': 0.1\n",
    "            }\n",
    "\n",
    "        overall_best_tour = None\n",
    "        overall_best_length = float('inf')\n",
    "        overall_times = []\n",
    "\n",
    "        # Start with a random tour\n",
    "        current_tour = list(range(self.num_cities))\n",
    "        random.shuffle(current_tour)\n",
    "\n",
    "        for outer in range(1, outer_iterations + 1):\n",
    "            outer_start = time.time()\n",
    "            print(f\"Outer iteration {outer}/{outer_iterations}\")\n",
    "            # Run SA from the current tour\n",
    "            sa_tour, sa_length, sa_iters, sa_time = self.solve_sa(current_tour, sa_params, use_perturbation)\n",
    "            print(f\"  SA: {sa_iters} iterations, {sa_time:.4f} seconds, tour length: {sa_length:.2f}\")\n",
    "\n",
    "            # Apply 2-opt improvement if enabled\n",
    "            if local_search_after_sa:\n",
    "                print(\"  Applying deterministic 2-opt local search...\")\n",
    "                opt_tour, opt_length, opt_iters = self.improve_tour_with_2opt(sa_tour)\n",
    "                print(f\"  2-opt: {opt_iters} iterations, final tour length: {opt_length:.2f}\")\n",
    "                candidate_length = opt_length\n",
    "                candidate_tour = opt_tour\n",
    "            else:\n",
    "                candidate_length = sa_length\n",
    "                candidate_tour = sa_tour\n",
    "\n",
    "            total_outer = time.time() - outer_start\n",
    "            print(f\"  Outer iteration time: {total_outer:.4f} seconds\")\n",
    "\n",
    "            # Update overall best if found\n",
    "            if candidate_length < overall_best_length:\n",
    "                overall_best_length = candidate_length\n",
    "                overall_best_tour = candidate_tour.copy()\n",
    "                print(f\"  New best solution found: {overall_best_length:.2f}\")\n",
    "            # Use the candidate as starting point for next outer iteration\n",
    "            current_tour = candidate_tour.copy()\n",
    "            overall_times.append(total_outer)\n",
    "            print(\"\")\n",
    "\n",
    "        avg_time = sum(overall_times)/len(overall_times) if overall_times else 0.0\n",
    "        print(f\"Best solution overall: {overall_best_length:.2f}\")\n",
    "        print(f\"Mean outer iteration time: {avg_time:.4f} seconds\")\n",
    "        return overall_best_tour, overall_best_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e22628c",
   "metadata": {},
   "source": [
    "## Метод ветвей и отсечений и интеграция с графовыми нейронными сетями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9625b2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import networkx as nx\n",
    "import pulp\n",
    "from pulp import LpVariable, LpProblem, lpSum, LpMinimize, value\n",
    "\n",
    "class GNNBranchCutSolver:\n",
    "    \"\"\"\n",
    "    Solver that integrates GNN predictions with Branch & Cut for TSP\n",
    "    \"\"\"\n",
    "    def __init__(self, gnn_model, threshold=0.6, fixing_percentage=0.2, elimination_percentage=0.2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gnn_model: Trained GNN model\n",
    "            threshold: Probability threshold for edge fixing\n",
    "            fixing_percentage: Percentage of highest probability edges to fix\n",
    "            elimination_percentage: Percentage of lowest probability edges to eliminate\n",
    "        \"\"\"\n",
    "        self.gnn_model = gnn_model\n",
    "        self.threshold = threshold\n",
    "        self.fixing_percentage = fixing_percentage\n",
    "        self.elimination_percentage = elimination_percentage\n",
    "        print(self.threshold)\n",
    "        print(self.fixing_percentage)\n",
    "        print(self.elimination_percentage)\n",
    "\n",
    "    def get_edge_probabilities(self, nodes_coord):\n",
    "        # Проверка типа входных данных\n",
    "        if not isinstance(nodes_coord, np.ndarray):\n",
    "             try:\n",
    "                 # Попытка конвертировать, если это тензор или список списков\n",
    "                 nodes_coord = np.array(nodes_coord)\n",
    "             except Exception as e:\n",
    "                 raise TypeError(f\"Input nodes_coord must be a NumPy array or convertible. Error: {e}\")\n",
    "        if nodes_coord.ndim != 2 or nodes_coord.shape[1] != 2:\n",
    "            raise ValueError(f\"Input nodes_coord must have shape (num_nodes, 2), but got {nodes_coord.shape}\")\n",
    "\n",
    "        num_nodes = nodes_coord.shape[0]\n",
    "        batch_size = 1 # Обрабатываем по одному экземпляру\n",
    "\n",
    "        # Подготовка входных тензоров\n",
    "        try:\n",
    "            # 1. Матрица расстояний\n",
    "            dist_matrix = squareform(pdist(nodes_coord, metric='euclidean'))\n",
    "\n",
    "            # 2. Входные тензоры для GNN\n",
    "            # x_edges: Матрица связности (1 - связь, 2 - self-loop, 0 - нет)\n",
    "            x_edges_np = np.ones((batch_size, num_nodes, num_nodes), dtype=np.int64)\n",
    "            # Заполняем диагональ специальным значением (например, 2, как в вашем GoogleTSPReader)\n",
    "            # Если ваша модель ожидает 0 на диагонали, измените это.\n",
    "            np.fill_diagonal(x_edges_np[0], 2)\n",
    "\n",
    "            # x_edges_values: Матрица расстояний\n",
    "            x_edges_values_np = np.expand_dims(dist_matrix, axis=0).astype(np.float32)\n",
    "\n",
    "            # x_nodes: Признаки узлов (просто единицы для TSP)\n",
    "            x_nodes_np = np.ones((batch_size, num_nodes), dtype=np.int64)\n",
    "\n",
    "            # x_nodes_coord: Координаты узлов\n",
    "            x_nodes_coord_np = np.expand_dims(nodes_coord, axis=0).astype(np.float32)\n",
    "\n",
    "            # Определяем устройство и типы тензоров PyTorch\n",
    "            device = next(self.gnn_model.parameters()).device # Получаем устройство модели\n",
    "            dtypeFloat = torch.cuda.FloatTensor if device.type == 'cuda' else torch.FloatTensor\n",
    "            dtypeLong = torch.cuda.LongTensor if device.type == 'cuda' else torch.LongTensor\n",
    "\n",
    "            # Конвертация в тензоры PyTorch и перемещение на device\n",
    "            x_edges = torch.from_numpy(x_edges_np).type(dtypeLong).to(device)\n",
    "            x_edges_values = torch.from_numpy(x_edges_values_np).type(dtypeFloat).to(device)\n",
    "            x_nodes = torch.from_numpy(x_nodes_np).type(dtypeLong).to(device)\n",
    "            x_nodes_coord_tensor = torch.from_numpy(x_nodes_coord_np).type(dtypeFloat).to(device)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during input tensor preparation in get_edge_probabilities: {e}\")\n",
    "            # Возвращаем пустой массив или поднимаем исключение\n",
    "            return np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "        # ---> Создание фиктивных y_edges и edge_cw <---\n",
    "        dummy_y_edges = torch.zeros(batch_size, num_nodes, num_nodes, dtype=torch.long, device=device)\n",
    "        # Веса [1.0, 1.0] т.к. два класса (ребро есть/нет)\n",
    "        dummy_edge_cw = torch.ones(2, dtype=torch.float, device=device)\n",
    "\n",
    "        # Get predictions from GNN\n",
    "        self.gnn_model.eval() # Убедимся, что модель в режиме оценки\n",
    "        with torch.no_grad():\n",
    "            # ---> Передаем фиктивные аргументы <---\n",
    "            try:\n",
    "                 result = self.gnn_model(x_edges, x_edges_values, x_nodes, x_nodes_coord_tensor,\n",
    "                                         dummy_y_edges, dummy_edge_cw)\n",
    "                 # Проверяем, что вернула модель\n",
    "                 if isinstance(result, tuple):\n",
    "                      edge_preds = result[0] # Берем предсказания (логиты)\n",
    "                 else:\n",
    "                      edge_preds = result\n",
    "            except Exception as e:\n",
    "                 print(f\"Error during model forward pass in get_edge_probabilities: {e}\")\n",
    "                 return np.zeros((num_nodes, num_nodes)) # Возвращаем нули при ошибке\n",
    "            # ---> КОНЕЦ ВЫЗОВА МОДЕЛИ <---\n",
    "\n",
    "\n",
    "        # Convert to probabilities using softmax\n",
    "        try:\n",
    "             # Убедимся, что edge_preds имеет правильную размерность [B, V, V, Voc]\n",
    "             if edge_preds.dim() == 4 and edge_preds.shape[0] == 1 and \\\n",
    "                edge_preds.shape[1] == num_nodes and edge_preds.shape[2] == num_nodes:\n",
    "                 # Берем вероятности для класса 1 (ребро есть)\n",
    "                 edge_probs_tensor = torch.softmax(edge_preds[0], dim=-1)[:, :, 1]\n",
    "                 edge_probs = edge_probs_tensor.cpu().numpy()\n",
    "             else:\n",
    "                 # Попытка обработать другие формы, если squeeze произошел где-то\n",
    "                 if edge_preds.dim()==3 and edge_preds.shape[0] == num_nodes and edge_preds.shape[1] == num_nodes:\n",
    "                      edge_probs_tensor = torch.softmax(edge_preds, dim=-1)[:, :, 1]\n",
    "                      edge_probs = edge_probs_tensor.cpu().numpy()\n",
    "                 else:\n",
    "                      raise ValueError(f\"Unexpected output shape from GNN model: {edge_preds.shape}. Expected 4D [1, N, N, Voc] or 3D [N, N, Voc].\")\n",
    "\n",
    "        except Exception as e_prob:\n",
    "             print(f\"Error processing edge_preds shape {edge_preds.shape} in get_edge_probabilities: {e_prob}\")\n",
    "             return np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "        return edge_probs\n",
    "\n",
    "    # Добавляем параметр optimal_edges_pure для сравнения\n",
    "    def solve_tsp(self, nodes_coord, optimal_edges_pure=None):\n",
    "        \"\"\"Solve TSP using GNN predictions and Branch & Cut.\n",
    "\n",
    "        Args:\n",
    "            nodes_coord: Coordinates of nodes (num_nodes, 2)\n",
    "            optimal_edges_pure: Optional set of tuples representing optimal edges from pure B&C for debugging\n",
    "\n",
    "        Returns:\n",
    "            tour: Optimal tour as list of node indices\n",
    "            tour_length: Length of optimal tour\n",
    "            solve_time: Time taken to solve ILP part (excluding GNN prediction)\n",
    "        \"\"\"\n",
    "        total_start_time = time.time() # Время для всего метода\n",
    "\n",
    "        num_nodes = nodes_coord.shape[0]\n",
    "        dist_matrix = squareform(pdist(nodes_coord, metric='euclidean'))\n",
    "\n",
    "        # --- Время GNN ---\n",
    "        gnn_start_time = time.time()\n",
    "        edge_probs = self.get_edge_probabilities(nodes_coord)\n",
    "        gnn_time = time.time() - gnn_start_time\n",
    "\n",
    "\n",
    "        # --- Время ILP ---\n",
    "        ilp_start_time = time.time()\n",
    "        model = LpProblem(\"TSP_GNN_BranchCut\", LpMinimize)\n",
    "        x = {}\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(i+1, num_nodes):\n",
    "                x[i, j] = LpVariable(f'x_{i}_{j}', cat='Binary')\n",
    "\n",
    "        def get_edge_var(i, j):\n",
    "            # Убедимся, что i < j для доступа к словарю x\n",
    "            idx1, idx2 = min(i, j), max(i, j)\n",
    "            if idx1 == idx2: return None # Не должно быть, но на всякий случай\n",
    "            return x.get((idx1, idx2))\n",
    "\n",
    "        model += lpSum([dist_matrix[i][j] * get_edge_var(i, j) for i in range(num_nodes) for j in range(i+1, num_nodes)])\n",
    "\n",
    "        for i in range(num_nodes):\n",
    "             # Используем get_edge_var для правильного суммирования\n",
    "            model += lpSum([get_edge_var(i, j) for j in range(num_nodes) if i != j]) == 2\n",
    "\n",
    "        fixed_edges_count = 0\n",
    "        eliminated_edges_count = 0\n",
    "\n",
    "        # --- Логирование Фиксации Ребер ---\n",
    "        if self.fixing_percentage > 0:\n",
    "            indices = [(i, j) for i in range(num_nodes) for j in range(i+1, num_nodes)]\n",
    "            edge_probs_triu = np.array([edge_probs[i, j] for i, j in indices])\n",
    "            sorted_indices = np.argsort(-edge_probs_triu) # Descending\n",
    "            num_edges_to_fix_total = len(indices)\n",
    "            num_edges_to_fix_limit = int(self.fixing_percentage * num_edges_to_fix_total)\n",
    "\n",
    "\n",
    "            for k, idx in enumerate(sorted_indices[:num_edges_to_fix_limit]):\n",
    "                i, j = indices[idx]\n",
    "                prob = edge_probs[i,j]\n",
    "                edge_tuple = tuple(sorted((i, j))) # Для сравнения с optimal_edges_pure\n",
    "\n",
    "                # Применяем порог threshold\n",
    "                if prob > self.threshold:\n",
    "                    model += get_edge_var(i, j) == 1\n",
    "                    fixed_edges_count += 1\n",
    "\n",
    "        # --- Логирование Удаления Ребер ---\n",
    "        if self.elimination_percentage > 0:\n",
    "            indices = [(i, j) for i in range(num_nodes) for j in range(i+1, num_nodes)]\n",
    "            edge_probs_triu = np.array([edge_probs[i, j] for i, j in indices])\n",
    "            sorted_indices = np.argsort(edge_probs_triu) # Ascending\n",
    "            num_edges_to_elim_total = len(indices)\n",
    "            num_edges_to_elim_limit = int(self.elimination_percentage * num_edges_to_elim_total)\n",
    "\n",
    "\n",
    "            for k, idx in enumerate(sorted_indices[:num_edges_to_elim_limit]):\n",
    "                i, j = indices[idx]\n",
    "                prob = edge_probs[i, j]\n",
    "                edge_tuple = tuple(sorted((i, j))) # Для сравнения\n",
    "\n",
    "                model += get_edge_var(i, j) == 0\n",
    "                eliminated_edges_count += 1\n",
    "\n",
    "        # --- Решение и Цикл Устранения Подтуров ---\n",
    "        subtour_elim_iterations = 0\n",
    "        total_pulp_solve_time = 0.0\n",
    "\n",
    "        while True:\n",
    "            iter_solve_start = time.time()\n",
    "            # Подавляем вывод PuLP, если не нужен\n",
    "            solver_kwargs = {'msg': False}\n",
    "            status = model.solve(pulp.PULP_CBC_CMD(**solver_kwargs)) # Используем встроенный или внешний решатель\n",
    "            iter_solve_time = time.time() - iter_solve_start\n",
    "            total_pulp_solve_time += iter_solve_time\n",
    "\n",
    "            if pulp.LpStatus[status] != 'Optimal':\n",
    "                 print(f\"[WARN GNN Solv] PuLP solver did not find an optimal solution (Status: {pulp.LpStatus[status]})\")\n",
    "                 # Можно вернуть None или поднять исключение\n",
    "                 ilp_time = time.time() - ilp_start_time\n",
    "                 total_time = time.time() - total_start_time\n",
    "                 print(f\"[DEBUG GNN Solv] ILP + Subtour time: {ilp_time:.4f}s (PuLP solve time: {total_pulp_solve_time:.4f}s)\")\n",
    "                 print(f\"[DEBUG GNN Solv] Total GNN solve method time: {total_time:.4f}s\")\n",
    "                 return [], float('inf'), total_time # Возвращаем индикатор ошибки\n",
    "\n",
    "\n",
    "            edges = []\n",
    "            for i in range(num_nodes):\n",
    "                for j in range(i+1, num_nodes):\n",
    "                    var = get_edge_var(i,j)\n",
    "                    if var is not None and value(var) > 0.5:\n",
    "                        edges.append((i, j))\n",
    "\n",
    "            G = nx.Graph()\n",
    "            G.add_edges_from(edges)\n",
    "            components = list(nx.connected_components(G))\n",
    "\n",
    "            if len(components) == 1:\n",
    "                break\n",
    "\n",
    "            subtour_elim_iterations += 1\n",
    "\n",
    "            # Добавляем ограничения только для компонент < N узлов\n",
    "            constraints_added_this_iter = 0\n",
    "            for component in components:\n",
    "                if len(component) < num_nodes:\n",
    "                    component_nodes = list(component)\n",
    "                    # Формируем ограничение: sum x(i,j) <= |S|-1 для i,j in S\n",
    "                    subtour_edges_sum = lpSum([get_edge_var(i, j)\n",
    "                                               for i in component_nodes\n",
    "                                               for j in component_nodes if i < j]) # Используем i < j\n",
    "                    # Имя ограничения для отладки\n",
    "                    constraint_name = f\"SubtourElim_{subtour_elim_iterations}_{constraints_added_this_iter}\"\n",
    "                    model += subtour_edges_sum <= len(component_nodes) - 1, constraint_name\n",
    "                    constraints_added_this_iter += 1\n",
    "            if constraints_added_this_iter == 0 and len(components)>1:\n",
    "                 print(\"[ERROR GNN Solv] Found multiple components but couldn't add elimination constraints!\")\n",
    "                 ilp_time = time.time() - ilp_start_time\n",
    "                 total_time = time.time() - total_start_time\n",
    "                 print(f\"[DEBUG GNN Solv] ILP + Subtour time: {ilp_time:.4f}s (PuLP solve time: {total_pulp_solve_time:.4f}s)\")\n",
    "                 print(f\"[DEBUG GNN Solv] Total GNN solve method time: {total_time:.4f}s\")\n",
    "                 return [], float('inf'), total_time\n",
    "\n",
    "\n",
    "        ilp_time = time.time() - ilp_start_time # Время на всю ILP часть\n",
    "        total_time = time.time() - total_start_time # Общее время метода\n",
    "\n",
    "\n",
    "        tour = self.edges_to_tour(edges) # Передаем num_nodes\n",
    "        tour_length = self.calculate_tour_length(tour, dist_matrix)\n",
    "\n",
    "        return tour, tour_length, total_time # Возвращаем общее время метода\n",
    "\n",
    "\n",
    "    def edges_to_tour(self, edges):\n",
    "        \"\"\"Convert edge list to tour.\n",
    "\n",
    "        Args:\n",
    "            edges: List of edges as (i,j) tuples\n",
    "\n",
    "        Returns:\n",
    "            tour: List of node indices in tour order\n",
    "        \"\"\"\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(edges)\n",
    "\n",
    "        # Если число вершин меньше, чем ожидается, добавим изолированные узлы\n",
    "        num_nodes = max(max(e) for e in edges) + 1 if edges else 0\n",
    "        for i in range(num_nodes):\n",
    "            if i not in G:\n",
    "                G.add_node(i)\n",
    "\n",
    "        # Пытаемся найти цикл через встроенную функцию\n",
    "        try:\n",
    "            cycle = nx.find_cycle(G, source=0)\n",
    "            # Формируем тур: начальный узел + последовательность второго элемента ребер\n",
    "            tour = [cycle[0][0]] + [j for (_, j) in cycle]\n",
    "            # Если цикл замкнут (начало = конец), убираем повторное появление начального узла\n",
    "            if tour[0] == tour[-1]:\n",
    "                tour.pop()\n",
    "            # Если тур охватывает не все узлы, пробуем дополнить DFS‑обходом\n",
    "            if len(tour) < num_nodes:\n",
    "                raise ValueError(\"Cycle does not cover all nodes\")\n",
    "            return tour\n",
    "        except Exception as e:\n",
    "            # Если nx.find_cycle не удалось или тур неполный, строим тур вручную через DFS\n",
    "            tour = []\n",
    "            visited = set()\n",
    "            def dfs(node, prev):\n",
    "                visited.add(node)\n",
    "                tour.append(node)\n",
    "                for nbr in G.neighbors(node):\n",
    "                    if nbr != prev and nbr not in visited:\n",
    "                        dfs(nbr, node)\n",
    "            dfs(0, None)\n",
    "            # Если тур не полный, дополним отсутствующими узлами (без гарантии гамильтонова цикла)\n",
    "            for node in range(num_nodes):\n",
    "                if node not in visited:\n",
    "                    tour.append(node)\n",
    "            return tour\n",
    "\n",
    "    def calculate_tour_length(self, tour, dist_matrix):\n",
    "        \"\"\"Calculate tour length.\n",
    "\n",
    "        Args:\n",
    "            tour: List of node indices\n",
    "            dist_matrix: Distance matrix\n",
    "\n",
    "        Returns:\n",
    "            tour_length: Total tour length\n",
    "        \"\"\"\n",
    "        tour_length = 0\n",
    "        for i in range(len(tour)):\n",
    "            j = (i + 1) % len(tour)\n",
    "            tour_length += dist_matrix[tour[i]][tour[j]]\n",
    "        return tour_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b41dc3",
   "metadata": {},
   "source": [
    "## Concorde TSP датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import networkx as nx\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "from typing import List, Tuple, Optional, Dict, Any, Iterator\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class ConcordeTSPInstance:\n",
    "    \"\"\"Структура для хранения данных одного экземпляра TSP.\"\"\"\n",
    "    def __init__(self, name: str, coordinates: np.ndarray, dist_matrix: np.ndarray,\n",
    "                 optimal_tour: Optional[List[int]] = None, optimal_length: Optional[float] = None):\n",
    "        self.name = name\n",
    "        self.coordinates = coordinates\n",
    "        self.dist_matrix = dist_matrix\n",
    "        self.optimal_tour = optimal_tour\n",
    "        self.optimal_length = optimal_length\n",
    "        self.num_nodes = coordinates.shape[0]\n",
    "\n",
    "    def get_nx_graph(self) -> nx.Graph:\n",
    "        \"\"\"Создает и возвращает граф NetworkX с весами ребер.\"\"\"\n",
    "        n = self.num_nodes\n",
    "        graph = nx.complete_graph(n)\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                graph.edges[i, j]['weight'] = self.dist_matrix[i, j]\n",
    "        return graph\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_tour_len(tour: List[int], dist_matrix: np.ndarray) -> float:\n",
    "        \"\"\"Статический метод для вычисления длины тура.\"\"\"\n",
    "        length = 0.0\n",
    "        n = len(tour)\n",
    "        if n == 0: return 0.0\n",
    "        for i in range(n):\n",
    "            u = tour[i]\n",
    "            v = tour[(i + 1) % n]\n",
    "            if 0 <= u < dist_matrix.shape[0] and 0 <= v < dist_matrix.shape[0]:\n",
    "                 length += dist_matrix[u, v]\n",
    "            else:\n",
    "                 print(f\"Warning: Invalid index in tour. u={u}, v={v}, matrix_shape={dist_matrix.shape}\")\n",
    "                 return float('inf')\n",
    "        return length\n",
    "\n",
    "\n",
    "class ConcordeTSPReader:\n",
    "    \"\"\"\n",
    "    Читает файлы датасета Concorde TSP и возвращает экземпляры задач.\n",
    "    Позволяет ограничить максимальное количество загружаемых экземпляров.\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath: str, name_prefix: str = \"instance\",\n",
    "                 max_instances: Optional[int] = None): # <--- Новый параметр\n",
    "        \"\"\"\n",
    "        Инициализация ридера.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Путь к файлу датасета (.txt).\n",
    "            name_prefix (str): Префикс для именования экземпляров.\n",
    "            max_instances (Optional[int]): Максимальное количество экземпляров\n",
    "                                           для загрузки из файла. Если None,\n",
    "                                           загружаются все.\n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.name_prefix = name_prefix\n",
    "        # Проверяем, что max_instances - положительное число или None\n",
    "        if max_instances is not None and max_instances <= 0:\n",
    "            print(\"Warning: max_instances must be positive. Loading all instances.\")\n",
    "            self.max_instances = None\n",
    "        else:\n",
    "            self.max_instances = max_instances\n",
    "        self.instances: List[ConcordeTSPInstance] = []\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Загружает и парсит данные из файла с отображением прогресса и лимитом.\"\"\"\n",
    "        try:\n",
    "            with open(self.filepath, \"r\") as f:\n",
    "                # Читаем все строки, чтобы правильно рассчитать total для tqdm\n",
    "                lines = f.readlines()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Dataset file not found at {self.filepath}\")\n",
    "            self.instances = []\n",
    "            return\n",
    "\n",
    "        if not lines:\n",
    "            print(f\"Warning: File is empty: {self.filepath}\")\n",
    "            self.instances = []\n",
    "            return\n",
    "\n",
    "        filename_short = os.path.basename(self.filepath)\n",
    "        print(f\"Parsing file: {filename_short}\")\n",
    "\n",
    "        # Определяем, сколько строк будем реально обрабатывать\n",
    "        num_lines_to_process = len(lines)\n",
    "        if self.max_instances is not None:\n",
    "            num_lines_to_process = min(len(lines), self.max_instances)\n",
    "            print(f\"Loading at most {self.max_instances} instances.\")\n",
    "\n",
    "        # Настраиваем tqdm\n",
    "        pbar = tqdm(enumerate(lines),\n",
    "                    desc=f\"Parsing {filename_short}\",\n",
    "                    total=num_lines_to_process, # Total - количество целевых экземпляров\n",
    "                    unit=\" lines\",\n",
    "                    ncols=1000,\n",
    "                    ascii=True)\n",
    "\n",
    "        for line_num, line in pbar:\n",
    "            # ---> Проверка лимита <---\n",
    "            if self.max_instances is not None and len(self.instances) >= self.max_instances:\n",
    "                pbar.close() # Закрываем прогресс-бар, т.к. достигли лимита\n",
    "                print(f\"\\nReached max_instances limit ({self.max_instances}). Stopping parsing.\")\n",
    "                break # Выходим из цикла\n",
    "\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "\n",
    "            parts = line.split()\n",
    "            try:\n",
    "                output_idx = parts.index('output')\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            # --- Координаты ---\n",
    "            num_coord_parts = output_idx\n",
    "            if num_coord_parts % 2 != 0: continue\n",
    "            num_nodes = num_coord_parts // 2\n",
    "            if num_nodes <= 1: continue\n",
    "\n",
    "            try:\n",
    "                coords_flat = [float(p) for p in parts[:num_coord_parts]]\n",
    "                coordinates = np.array(coords_flat).reshape(num_nodes, 2)\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "\n",
    "            # --- Матрица расстояний ---\n",
    "            dist_matrix = squareform(pdist(coordinates, metric='euclidean'))\n",
    "\n",
    "            # --- Оптимальный тур ---\n",
    "            tour_parts = parts[output_idx + 1:]\n",
    "            if tour_parts and tour_parts[-1] == '-1': tour_parts = tour_parts[:-1]\n",
    "\n",
    "            try:\n",
    "                optimal_tour_indices = [int(node) - 1 for node in tour_parts]\n",
    "            except ValueError: continue\n",
    "\n",
    "            if len(optimal_tour_indices) == num_nodes + 1 and optimal_tour_indices[0] == optimal_tour_indices[-1]:\n",
    "                optimal_tour_0based = optimal_tour_indices[:-1]\n",
    "            elif len(optimal_tour_indices) == num_nodes:\n",
    "                 optimal_tour_0based = optimal_tour_indices\n",
    "            else: continue\n",
    "\n",
    "            if len(set(optimal_tour_0based)) != num_nodes or len(optimal_tour_0based) != num_nodes: continue\n",
    "\n",
    "            # --- Оптимальная длина ---\n",
    "            optimal_length = ConcordeTSPInstance.calculate_tour_len(optimal_tour_0based, dist_matrix)\n",
    "            if optimal_length == float('inf'): continue\n",
    "\n",
    "            # --- Создание экземпляра ---\n",
    "            instance_name = f\"{self.name_prefix}_{line_num+1}\"\n",
    "            instance = ConcordeTSPInstance(\n",
    "                name=instance_name, coordinates=coordinates, dist_matrix=dist_matrix,\n",
    "                optimal_tour=optimal_tour_0based, optimal_length=optimal_length\n",
    "            )\n",
    "            self.instances.append(instance)\n",
    "            # Обновляем счетчик в tqdm только после успешного добавления экземпляра\n",
    "            pbar.update(1) # Обновляем на 1 успешно обработанный\n",
    "\n",
    "        # Закрываем pbar, если цикл завершился естественным образом\n",
    "        if not pbar.disable:\n",
    "             pbar.close()\n",
    "\n",
    "        # Корректируем total для pbar, если обработали меньше строк, чем ожидали\n",
    "        # (это может случиться, если многие строки были пропущены из-за ошибок)\n",
    "        # Это нужно, чтобы прогресс-бар дошел до 100%, если лимит не был достигнут явно\n",
    "        # pbar.total = len(self.instances)\n",
    "        # pbar.refresh() # Обновить отображение\n",
    "\n",
    "        print(f\"\\nFinished parsing. Loaded {len(self.instances)} instances from {self.filepath}\")\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Возвращает количество загруженных экземпляров.\"\"\"\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, index: int) -> ConcordeTSPInstance:\n",
    "        \"\"\"Возвращает экземпляр по индексу.\"\"\"\n",
    "        if 0 <= index < len(self.instances):\n",
    "            return self.instances[index]\n",
    "        else:\n",
    "            raise IndexError(\"Index out of range\")\n",
    "\n",
    "    def get_iterator(self, shuffle_data: bool = False) -> Iterator[ConcordeTSPInstance]:\n",
    "        \"\"\"Возвращает итератор по экземплярам (опционально перемешанным).\"\"\"\n",
    "        instance_list = self.instances\n",
    "        if shuffle_data:\n",
    "            # Используем копию, чтобы не изменять оригинальный порядок в ридере\n",
    "            instance_list = shuffle(list(self.instances))\n",
    "        return iter(instance_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf1adef",
   "metadata": {},
   "source": [
    "## Утилитарный бенчмарк класс для сравнения методов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fd7cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pulp import LpVariable, LpProblem, lpSum, LpMinimize, value\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "class TSPComparison:\n",
    "    \"\"\"\n",
    "    Utilities for comparing TSP solution methods\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def pure_branch_cut_solve(nodes_coord):\n",
    "        \"\"\"Pure Branch & Cut TSP solver without GNN guidance.\n",
    "\n",
    "        Args:\n",
    "            nodes_coord: Coordinates of nodes (num_nodes, 2)\n",
    "\n",
    "        Returns:\n",
    "            tour: Optimal tour as list of node indices\n",
    "            tour_length: Length of optimal tour\n",
    "            solve_time: Time taken to solve in seconds\n",
    "        \"\"\"\n",
    "        num_nodes = nodes_coord.shape[0]\n",
    "\n",
    "        # Get distance matrix\n",
    "        dist_matrix = squareform(pdist(nodes_coord, metric='euclidean'))\n",
    "\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Create ILP model\n",
    "        model = LpProblem(\"TSP_PureBranchCut\", LpMinimize)\n",
    "\n",
    "        # Create edge variables - x[i,j] = 1 if edge (i,j) is used\n",
    "        x = {}\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(i+1, num_nodes):  # Only need one direction for undirected graph\n",
    "                x[i, j] = LpVariable(f'x_{i}_{j}', cat='Binary')\n",
    "\n",
    "        # Helper function to access edge variable in either direction\n",
    "        def get_edge_var(i, j):\n",
    "            return x[min(i, j), max(i, j)]\n",
    "\n",
    "        # Objective: minimize total distance\n",
    "        model += lpSum([dist_matrix[i][j] * get_edge_var(i, j) for i in range(num_nodes) for j in range(i+1, num_nodes)])\n",
    "\n",
    "        # Constraints: each node must have exactly two edges\n",
    "        for i in range(num_nodes):\n",
    "            model += lpSum([get_edge_var(i, j) for j in range(num_nodes) if j != i]) == 2\n",
    "\n",
    "        # Solve with built-in Branch & Cut\n",
    "        model.solve()\n",
    "\n",
    "        # Extract solution\n",
    "        edges = []\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(i+1, num_nodes):\n",
    "                if value(get_edge_var(i, j)) > 0.5:\n",
    "                    edges.append((i, j))\n",
    "\n",
    "        # Build tour from edges\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from(edges)\n",
    "\n",
    "        # Check for subtours and add constraints until we get a valid tour\n",
    "        while True:\n",
    "            # Find connected components (subtours)\n",
    "            components = list(nx.connected_components(G))\n",
    "\n",
    "            if len(components) == 1:\n",
    "                # Valid tour found\n",
    "                break\n",
    "\n",
    "            # Add subtour elimination constraints and resolve\n",
    "            for component in components:\n",
    "                if len(component) < num_nodes:\n",
    "                    component = list(component)\n",
    "                    model += lpSum([get_edge_var(i, j) for i in component for j in component if i < j]) <= len(component) - 1\n",
    "\n",
    "            # Resolve\n",
    "            model.solve()\n",
    "\n",
    "            # Update graph\n",
    "            G.clear()\n",
    "            edges = []\n",
    "            for i in range(num_nodes):\n",
    "                for j in range(i+1, num_nodes):\n",
    "                    if value(get_edge_var(i, j)) > 0.5:\n",
    "                        edges.append((i, j))\n",
    "            G.add_edges_from(edges)\n",
    "\n",
    "        # End timing\n",
    "        solve_time = time.time() - start_time\n",
    "\n",
    "        # Convert edge list to tour\n",
    "        tour = []\n",
    "        current = 0  # Start from node 0\n",
    "        tour.append(current)\n",
    "\n",
    "        for _ in range(num_nodes-1):\n",
    "            neighbors = list(G.neighbors(current))\n",
    "            # Remove already visited nodes\n",
    "            unvisited = [n for n in neighbors if n not in tour]\n",
    "            if not unvisited:\n",
    "                break\n",
    "            current = unvisited[0]\n",
    "            tour.append(current)\n",
    "\n",
    "        # Calculate tour length\n",
    "        tour_length = 0\n",
    "        for i in range(len(tour)):\n",
    "            j = (i + 1) % len(tour)\n",
    "            tour_length += dist_matrix[tour[i]][tour[j]]\n",
    "\n",
    "        return tour, tour_length, solve_time, edges\n",
    "\n",
    "    @staticmethod\n",
    "    def pure_gnn_beam_search_solve(nodes_coord: np.ndarray,\n",
    "                                     gnn_model, config: Dict, beam_size: int,\n",
    "                                     dtypeFloat, dtypeLong\n",
    "                                     ) -> Tuple[Optional[List[int]], float, float]:\n",
    "        \"\"\"\n",
    "        Решает TSP, используя только GNN и Beam Search (выбирая кратчайший тур из луча).\n",
    "\n",
    "        Args:\n",
    "            nodes_coord: Координаты узлов (num_nodes, 2).\n",
    "            gnn_model: Обученная модель GNN (nn.Module).\n",
    "            config: Словарь конфигурации GNN.\n",
    "            beam_size: Размер луча для Beam Search.\n",
    "            dtypeFloat: Тип для float тензоров.\n",
    "            dtypeLong: Тип для long тензоров.\n",
    "\n",
    "        Returns:\n",
    "            tour: Найденный тур (список индексов 0-based) или None при ошибке.\n",
    "            tour_length: Длина найденного тура (или inf).\n",
    "            solve_time: Время выполнения (секунды).\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        num_nodes = nodes_coord.shape[0]\n",
    "        tour = None\n",
    "        tour_length = float('inf')\n",
    "\n",
    "        try:\n",
    "            # 1. Получаем предсказания GNN (y_pred_edges)\n",
    "            # Подготовка входов для GNN (аналогично get_edge_probabilities)\n",
    "            dist_matrix = squareform(pdist(nodes_coord, metric='euclidean'))\n",
    "            x_edges_np = np.ones((1, num_nodes, num_nodes))\n",
    "            np.fill_diagonal(x_edges_np[0], 2)\n",
    "            x_edges_values_np = np.expand_dims(dist_matrix, 0)\n",
    "            x_nodes_np = np.ones((1, num_nodes))\n",
    "            x_nodes_coord_np = np.expand_dims(nodes_coord, 0)\n",
    "\n",
    "            # Конвертация в тензоры\n",
    "            x_edges = torch.LongTensor(x_edges_np).type(dtypeLong)\n",
    "            x_edges_values = torch.FloatTensor(x_edges_values_np).type(dtypeFloat)\n",
    "            x_nodes = torch.LongTensor(x_nodes_np).type(dtypeLong)\n",
    "            x_nodes_coord_tensor = torch.FloatTensor(x_nodes_coord_np).type(dtypeFloat) # Переименовал\n",
    "            \n",
    "            device = x_nodes_coord_tensor.device # Определяем device\n",
    "            batch_size = 1\n",
    "\n",
    "            # ---> ДОБАВЛЕНИЕ: Создание фиктивных y_edges и edge_cw <---\n",
    "            dummy_y_edges = torch.zeros(batch_size, num_nodes, num_nodes, dtype=torch.long, device=device)\n",
    "            dummy_edge_cw = torch.ones(2, dtype=torch.float, device=device)\n",
    "\n",
    "            # Перемещаем на GPU, если доступно\n",
    "            if torch.cuda.is_available():\n",
    "                x_edges = x_edges.cuda()\n",
    "                x_edges_values = x_edges_values.cuda()\n",
    "                x_nodes = x_nodes.cuda()\n",
    "                x_nodes_coord_tensor = x_nodes_coord_tensor.cuda()\n",
    "\n",
    "            # Предсказание\n",
    "            gnn_model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Убрали передачу y_edges и edge_cw, т.к. loss не нужен\n",
    "                result = gnn_model(x_edges, x_edges_values, x_nodes, x_nodes_coord_tensor,\n",
    "                                   dummy_y_edges, dummy_edge_cw)\n",
    "                if isinstance(result, tuple): y_pred_edges = result[0]\n",
    "                else: y_pred_edges = result\n",
    "\n",
    "            # 2. Запускаем Beam Search (выбираем кратчайший)\n",
    "            # beamsearch_tour_nodes_shortest ожидает y_pred_edges и x_edges_values\n",
    "            predicted_tours_tensor = beamsearch_tour_nodes_shortest( # Или beamsearch_tour_nodes\n",
    "                y_pred_edges,           # Предсказания GNN\n",
    "                x_edges_values,         # Матрица расстояний (тензор)\n",
    "                beam_size=beam_size, batch_size=batch_size, num_nodes=num_nodes,\n",
    "                dtypeFloat=dtypeFloat, dtypeLong=dtypeLong,\n",
    "                probs_type='raw', random_start=False\n",
    "            )\n",
    "\n",
    "            # 3. Получаем лучший тур из батча (он у нас один)\n",
    "            if predicted_tours_tensor is not None and predicted_tours_tensor.shape[0] > 0:\n",
    "                tour_tensor = predicted_tours_tensor[0] # Берем первый (и единственный) тур\n",
    "                tour = tour_tensor.cpu().numpy().tolist() # Конвертируем в список Python\n",
    "                # Проверяем валидность тура\n",
    "                if is_valid_tour(tour, num_nodes):\n",
    "                    # Рассчитываем длину найденного тура\n",
    "                     tour_length = ConcordeTSPInstance.calculate_tour_len(tour, dist_matrix)\n",
    "                else:\n",
    "                     print(\"Warning: Beam Search produced an invalid tour.\")\n",
    "                     tour = None # Сбрасываем тур, если он невалидный\n",
    "                     tour_length = float('inf')\n",
    "            else:\n",
    "                 print(\"Warning: Beam Search did not return a tour.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Pure GNN + Beam Search solve: {e}\")\n",
    "            tour = None\n",
    "            tour_length = float('inf')\n",
    "\n",
    "        solve_time = time.time() - start_time\n",
    "        return tour, tour_length, solve_time\n",
    "\n",
    "    @staticmethod\n",
    "    def visualize_tours(nodes_coord, tour_pure, tour_gnn, title=\"TSP Tours Comparison\"):\n",
    "        \"\"\"Visualize and compare two TSP tours.\n",
    "\n",
    "        Args:\n",
    "            nodes_coord: Node coordinates (num_nodes, 2)\n",
    "            tour_pure: Tour from pure Branch & Cut\n",
    "            tour_gnn: Tour from GNN-guided Branch & Cut\n",
    "            title: Plot title\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # Plot pure Branch & Cut tour\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(nodes_coord[:, 0], nodes_coord[:, 1], c='blue', s=50)\n",
    "\n",
    "        # Connect tour points\n",
    "        for i in range(len(tour_pure)):\n",
    "            j = (i + 1) % len(tour_pure)\n",
    "            plt.plot([nodes_coord[tour_pure[i], 0], nodes_coord[tour_pure[j], 0]],\n",
    "                     [nodes_coord[tour_pure[i], 1], nodes_coord[tour_pure[j], 1]], 'r-')\n",
    "\n",
    "        # Number the nodes\n",
    "        for i, (x, y) in enumerate(nodes_coord):\n",
    "            plt.text(x, y, str(i), fontsize=12)\n",
    "\n",
    "        plt.title(\"Pure Branch & Cut Tour\")\n",
    "\n",
    "        # Plot GNN-guided Branch & Cut tour\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(nodes_coord[:, 0], nodes_coord[:, 1], c='blue', s=50)\n",
    "\n",
    "        # Connect tour points\n",
    "        for i in range(len(tour_gnn)):\n",
    "            j = (i + 1) % len(tour_gnn)\n",
    "            plt.plot([nodes_coord[tour_gnn[i], 0], nodes_coord[tour_gnn[j], 0]],\n",
    "                     [nodes_coord[tour_gnn[i], 1], nodes_coord[tour_gnn[j], 1]], 'r-')\n",
    "\n",
    "        # Number the nodes\n",
    "        for i, (x, y) in enumerate(nodes_coord):\n",
    "            plt.text(x, y, str(i), fontsize=12)\n",
    "\n",
    "        plt.title(\"GNN-guided Branch & Cut Tour\")\n",
    "\n",
    "        plt.suptitle(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def compare_methods(\n",
    "                        # Модели GNN\n",
    "                        gnn_solver_v1,\n",
    "                        gnn_solver_v2,\n",
    "                        gnn_model_v1: Optional[nn.Module],\n",
    "                        gnn_model_v2: Optional[nn.Module],\n",
    "                        config: Dict, # Общий конфиг GNN\n",
    "                        test_instances: List[np.ndarray],\n",
    "                        # Параметры для Pure GNN+BS\n",
    "                        beam_size_gnn_pure: int = 10,\n",
    "                        metrics=['tour_length', 'solving_time']\n",
    "                        ) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Сравнивает 5 методов:\n",
    "        1. Pure B&C\n",
    "        2. GNN+BS (v1)\n",
    "        3. GNN+BS (v2)\n",
    "        4. GNN+B&C (v1)\n",
    "        5. GNN+B&C (v2)\n",
    "\n",
    "        Args:\n",
    "            gnn_model_v1: Обученная модель GNN v1 (Transformer).\n",
    "            gnn_model_v2: Обученная модель GNN v2 (Linear).\n",
    "            config: Словарь конфигурации GNN.\n",
    "            test_instances: Список массивов координат узлов.\n",
    "            gnn_bc_params_v1: Словарь параметров {'threshold', 'fixing_percentage', 'elimination_percentage'} для гибрида с v1.\n",
    "            gnn_bc_params_v2: Словарь параметров для гибрида с v2.\n",
    "            beam_size_gnn_pure: Размер луча для Pure GNN + Beam Search.\n",
    "            metrics: Метрики для сравнения.\n",
    "\n",
    "        Returns:\n",
    "            Словарь с результатами для 5 методов.\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'pure_bc':         {'tour_lengths': [], 'solving_times': [], 'tour': []},\n",
    "            'gnn_bs_v1':       {'tour_lengths': [], 'solving_times': [], 'tour': []}, # Модель 1 + BS\n",
    "            'gnn_bs_v2':       {'tour_lengths': [], 'solving_times': [], 'tour': []}, # Модель 2 + BS\n",
    "            'gnn_bc_v1':       {'tour_lengths': [], 'solving_times': [], 'tour': []}, # Модель 1 + B&C\n",
    "            'gnn_bc_v2':       {'tour_lengths': [], 'solving_times': [], 'tour': []}, # Модель 2 + B&C\n",
    "            'simulated_annealing_2opt':         {'tour_lengths': [], 'solving_times': [], 'tour': []},\n",
    "        }\n",
    "\n",
    "        # Типы тензоров\n",
    "        dtypeFloat = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "        dtypeLong = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor\n",
    "\n",
    "        for i, nodes_coord in enumerate(test_instances):\n",
    "            print(f\"\\n===== Solving instance {i+1}/{len(test_instances)} =====\")\n",
    "\n",
    "            # --- 1. Pure Branch & Cut ---\n",
    "            print(\"  Running Pure B&C...\")\n",
    "            tour_pure, tour_length_pure, pure_time, _ = TSPComparison.pure_branch_cut_solve(nodes_coord)\n",
    "            results['pure_bc']['tour_lengths'].append(tour_length_pure)\n",
    "            results['pure_bc']['solving_times'].append(pure_time)\n",
    "            results['pure_bc']['tour'].append(tour_pure)\n",
    "            print(f\"    Pure B&C: Length={tour_length_pure:.4f}, Time={pure_time:.4f}s\")\n",
    "\n",
    "            # --- 2. Pure GNN + Beam Search (Модель 1) ---\n",
    "            if gnn_model_v1:\n",
    "                print(\"  Running Pure GNN + Beam Search (Model 1 - Transformer)...\")\n",
    "                tour_gnn_bs_v1, tour_length_gnn_bs_v1, gnn_bs_time_v1 = TSPComparison.pure_gnn_beam_search_solve(\n",
    "                    nodes_coord, gnn_model_v1, config, beam_size_gnn_pure, dtypeFloat, dtypeLong\n",
    "                )\n",
    "                results['gnn_bs_v1']['tour_lengths'].append(tour_length_gnn_bs_v1)\n",
    "                results['gnn_bs_v1']['solving_times'].append(gnn_bs_time_v1)\n",
    "                results['gnn_bs_v1']['tour'].append(tour_gnn_bs_v1)\n",
    "                print(f\"    GNN+BS(v1): Length={tour_length_gnn_bs_v1:.4f}, Time={gnn_bs_time_v1:.4f}s\")\n",
    "            else:\n",
    "                print(\"  Skipping Pure GNN + BS (Model 1 not loaded).\")\n",
    "                results['gnn_bs_v1']['tour_lengths'].append(float('inf'))\n",
    "                results['gnn_bs_v1']['solving_times'].append(float('inf'))\n",
    "                results['gnn_bs_v1']['tour'].append(None)\n",
    "\n",
    "            # --- 3. Pure GNN + Beam Search (Модель 2) ---\n",
    "            if gnn_model_v2:\n",
    "                print(\"  Running Pure GNN + Beam Search (Model 2 - Linear)...\")\n",
    "                tour_gnn_bs_v2, tour_length_gnn_bs_v2, gnn_bs_time_v2 = TSPComparison.pure_gnn_beam_search_solve(\n",
    "                    nodes_coord, gnn_model_v2, config, beam_size_gnn_pure, dtypeFloat, dtypeLong\n",
    "                )\n",
    "                results['gnn_bs_v2']['tour_lengths'].append(tour_length_gnn_bs_v2)\n",
    "                results['gnn_bs_v2']['solving_times'].append(gnn_bs_time_v2)\n",
    "                results['gnn_bs_v2']['tour'].append(tour_gnn_bs_v2)\n",
    "                print(f\"    GNN+BS(v2): Length={tour_length_gnn_bs_v2:.4f}, Time={gnn_bs_time_v2:.4f}s\")\n",
    "            else:\n",
    "                print(\"  Skipping Pure GNN + BS (Model 2 not loaded).\")\n",
    "                results['gnn_bs_v2']['tour_lengths'].append(float('inf'))\n",
    "                results['gnn_bs_v2']['solving_times'].append(float('inf'))\n",
    "                results['gnn_bs_v2']['tour'].append(None)\n",
    "\n",
    "            # --- 4. GNN-guided Branch & Cut (Модель 1) ---\n",
    "            if gnn_solver_v1:\n",
    "                print(\"  Running GNN-guided B&C (Model 1 - Transformer)...\")\n",
    "                tour_gnn_bc_v1, tour_length_gnn_bc_v1, gnn_bc_time_v1 = gnn_solver_v1.solve_tsp(nodes_coord)\n",
    "                results['gnn_bc_v1']['tour_lengths'].append(tour_length_gnn_bc_v1)\n",
    "                results['gnn_bc_v1']['solving_times'].append(gnn_bc_time_v1)\n",
    "                results['gnn_bc_v1']['tour'].append(tour_gnn_bc_v1)\n",
    "                print(f\"    GNN+B&C(v1): Length={tour_length_gnn_bc_v1:.4f}, Time={gnn_bc_time_v1:.4f}s\")\n",
    "            else:\n",
    "                print(\"  Skipping GNN-guided B&C (Model 1 not loaded or solver init failed).\")\n",
    "                results['gnn_bc_v1']['tour_lengths'].append(float('inf'))\n",
    "                results['gnn_bc_v1']['solving_times'].append(float('inf'))\n",
    "                results['gnn_bc_v1']['tour'].append(None)\n",
    "\n",
    "\n",
    "            # --- 5. GNN-guided Branch & Cut (Модель 2) ---\n",
    "            if gnn_solver_v2:\n",
    "                print(\"  Running GNN-guided B&C (Model 2 - Linear)...\")\n",
    "                tour_gnn_bc_v2, tour_length_gnn_bc_v2, gnn_bc_time_v2 = gnn_solver_v2.solve_tsp(nodes_coord)\n",
    "                results['gnn_bc_v2']['tour_lengths'].append(tour_length_gnn_bc_v2)\n",
    "                results['gnn_bc_v2']['solving_times'].append(gnn_bc_time_v2)\n",
    "                results['gnn_bc_v2']['tour'].append(tour_gnn_bc_v2)\n",
    "                print(f\"    GNN+B&C(v2): Length={tour_length_gnn_bc_v2:.4f}, Time={gnn_bc_time_v2:.4f}s\")\n",
    "            else:\n",
    "                print(\"  Skipping GNN-guided B&C (Model 2 not loaded or solver init failed).\")\n",
    "                results['gnn_bc_v2']['tour_lengths'].append(float('inf'))\n",
    "                results['gnn_bc_v2']['solving_times'].append(float('inf'))\n",
    "                results['gnn_bc_v2']['tour'].append(None)\n",
    "            \n",
    "            # --- 6. Simulated Annealing + 2-opt (outer) ---\n",
    "            print(\"  Running Simulated Annealing 2-opt (outer)...\")\n",
    "            distance_matrix = squareform(pdist(nodes_coord, metric='euclidean'))\n",
    "            simulated_annealing_2opt = TSP_2opt_SA_outer(distance_matrix)\n",
    "            \n",
    "            start = time.time()\n",
    "            tour_sa_2opt, tour_length_sa_2opt = simulated_annealing_2opt.solve()\n",
    "            time_sa_2opt = time.time() - start\n",
    "            results['simulated_annealing_2opt']['tour_lengths'].append(tour_length_sa_2opt)\n",
    "            results['simulated_annealing_2opt']['solving_times'].append(time_sa_2opt)\n",
    "            results['simulated_annealing_2opt']['tour'].append(tour_sa_2opt)\n",
    "            print(f\"    Simulated Annealing 2-opt (outer): Length={tour_length_sa_2opt:.4f}, Time={time_sa_2opt:.4f}s\")\n",
    "\n",
    "        # Вычисление средних значений\n",
    "        print(\"\\nCalculating average results...\")\n",
    "        for method in list(results.keys()):\n",
    "            # Инициализируем средние значения по умолчанию как inf\n",
    "            results[method]['avg_tour_lengths'] = float('inf')\n",
    "            results[method]['avg_solving_times'] = float('inf')\n",
    "\n",
    "            metrics_to_average = ['tour_lengths', 'solving_times']\n",
    "            for metric in metrics_to_average:\n",
    "                 # Проверяем, есть ли ключ и данные в нем\n",
    "                 if results[method].get(metric):\n",
    "                     # Фильтруем None и inf значения\n",
    "                     valid_values = [v for v in results[method][metric] if v is not None and v != float('inf')]\n",
    "                     if valid_values: # Если есть хотя бы одно валидное значение\n",
    "                         results[method][f'avg_{metric}'] = np.mean(valid_values)\n",
    "                     # else: оставляем значение inf по умолчанию\n",
    "                 # else: оставляем значение inf по умолчанию\n",
    "        # ---> КОНЕЦ ДОБАВЛЕННОГО КОДА УСРЕДНЕНИЯ <---\n",
    "\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_comparison_results(results: Dict[Tuple[str, str], Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Plot comparison results for multiple methods against Pure B&C.\n",
    "\n",
    "        Args:\n",
    "            results: Dictionary mapping (instance_name, solver_name) to benchmark results.\n",
    "                     Assumes structure includes keys like 'pure_bc', 'gnn_bs_v1', etc.\n",
    "                     and sub-keys 'avg_tour_lengths', 'avg_solving_times'.\n",
    "        \"\"\"\n",
    "        # --- Извлечение средних данных ---\n",
    "        method_names = ['Pure B&C', 'GNN+BS(v1)', 'GNN+BS(v2)', 'GNN+B&C(v1)', 'GNN+B&C(v2)', \"Simulated Annealing + 2-opt (outer)\"]\n",
    "        method_keys = ['pure_bc', 'gnn_bs_v1', 'gnn_bs_v2', 'gnn_bc_v1', 'gnn_bc_v2', \"simulated_annealing_2opt\"]\n",
    "\n",
    "        avg_lengths = []\n",
    "        avg_times = []\n",
    "\n",
    "        for key in method_keys:\n",
    "            avg_len = results.get(key, {}).get('avg_tour_lengths', float('inf'))\n",
    "            avg_time = results.get(key, {}).get('avg_solving_times', float('inf'))\n",
    "\n",
    "            # Заменяем inf на NaN для корректной отрисовки (или можно оставить inf и matplotlib их проигнорирует)\n",
    "            avg_lengths.append(avg_len if avg_len != float('inf') else np.nan)\n",
    "            avg_times.append(avg_time if avg_time != float('inf') else np.nan)\n",
    "\n",
    "        # --- Построение графиков ---\n",
    "        plt.figure(figsize=(16, 7)) # Немного шире\n",
    "\n",
    "        # График длин\n",
    "        plt.subplot(1, 2, 1)\n",
    "        bars1 = plt.bar(method_names, avg_lengths, color=['blue', 'orange', 'green', 'red', 'purple'])\n",
    "        plt.title('Average Tour Length Comparison')\n",
    "        plt.ylabel('Average Length')\n",
    "        plt.xticks(rotation=25, ha='right') # Поворот подписей для читаемости\n",
    "        # Добавляем значения над столбцами\n",
    "        for bar in bars1:\n",
    "             yval = bar.get_height()\n",
    "             if not np.isnan(yval):\n",
    "                 plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.2f}', va='bottom', ha='center') # Показываем значение\n",
    "\n",
    "        # График времени\n",
    "        plt.subplot(1, 2, 2)\n",
    "        bars2 = plt.bar(method_names, avg_times, color=['blue', 'orange', 'green', 'red', 'purple'])\n",
    "        plt.title('Average Solving Time Comparison')\n",
    "        plt.ylabel('Average Time (s)')\n",
    "        plt.xticks(rotation=25, ha='right')\n",
    "        # Добавляем значения над столбцами\n",
    "        for bar in bars2:\n",
    "             yval = bar.get_height()\n",
    "             if not np.isnan(yval):\n",
    "                 plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.3f}', va='bottom', ha='center') # Больше знаков для времени\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd4f1af",
   "metadata": {},
   "source": [
    "## Пример использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d19484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap # Для переноса длинных названий методов\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Инициализация и обучение модели GNN\n",
    "def train_gnn_model(config, num_epochs=25, version=1):\n",
    "    \"\"\"Train GNN model for TSP with validation and testing.\n",
    "\n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "        num_epochs: Number of epochs to train\n",
    "\n",
    "    Returns:\n",
    "        net: Trained GNN model\n",
    "        losses: Dictionary containing training, validation and test losses\n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    if version == 1:\n",
    "        net = nn.DataParallel(ResidualGatedGCNModel(config, torch.cuda.FloatTensor, torch.cuda.LongTensor))\n",
    "    else:\n",
    "        net = nn.DataParallel(ResidualGatedGCNModel_v2(config, torch.cuda.FloatTensor, torch.cuda.LongTensor))\n",
    "    net.cuda()\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=variables['learning_rate'], weight_decay=1e-5)\n",
    "    val_loss_old = None\n",
    "\n",
    "    # Prepare for tracking losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train one epoch\n",
    "        train_time, train_loss = train_one_epoch(net, optimizer, config)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        print(f\"Epoch: {epoch}, Train Loss: {train_loss:.4f}, Time: {train_time:.2f}s\")\n",
    "\n",
    "        # Validation phase\n",
    "        if epoch % config[\"val_every\"] == 0 or epoch == num_epochs-1:\n",
    "            val_time, val_loss = test(net, config, mode='val')\n",
    "            val_losses.append(val_loss)\n",
    "            print(f\"Epoch: {epoch}, Val Loss: {val_loss:.4f}, Time: {val_time:.2f}s\")\n",
    "\n",
    "            # Update learning rate based on validation performance\n",
    "            if val_loss_old is not None and val_loss > 0.99 * val_loss_old:\n",
    "                config[\"learning_rate\"] /= config[\"decay_rate\"]\n",
    "                optimizer = update_learning_rate(optimizer, config[\"learning_rate\"])\n",
    "                print(f\"Learning rate updated to: {config['learning_rate']:.6f}\")\n",
    "\n",
    "            val_loss_old = val_loss  # Update old validation loss\n",
    "\n",
    "        # Testing phase\n",
    "        if epoch % config[\"test_every\"] == 0 or epoch == num_epochs-1:\n",
    "            test_time, test_loss = test(net, config, mode='test')\n",
    "            test_losses.append(test_loss)\n",
    "            print(f\"Epoch: {epoch}, Test Loss: {test_loss:.4f}, Time: {test_time:.2f}s\\n\")\n",
    "        \n",
    "        if epoch % config[\"save_every\"] == 0 or epoch == num_epochs-1:\n",
    "            _, test_loss_save = test(net, config, mode='test')\n",
    "            save_model(net, f\"./tsp_gnn_model_epoch_{epoch}_test_loss_{test_loss_save}.pt\")\n",
    "            print(f\"Epoch: {epoch}, Test Loss: {test_loss_save:.4f}, Model Saved...\") \n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    # Return model and losses for further analysis\n",
    "    losses = {\n",
    "        'train': train_losses,\n",
    "        'validation': val_losses,\n",
    "        'test': test_losses\n",
    "    }\n",
    "\n",
    "    return net, losses\n",
    "\n",
    "# Генерация тестовых примеров\n",
    "def generate_test_instances(num_instances=10, num_nodes=20):\n",
    "    \"\"\"Generate random TSP test instances.\n",
    "\n",
    "    Args:\n",
    "        num_instances: Number of instances to generate\n",
    "        num_nodes: Number of nodes per instance\n",
    "\n",
    "    Returns:\n",
    "        instances: List of node coordinate arrays\n",
    "    \"\"\"\n",
    "    instances = []\n",
    "    for _ in range(num_instances):\n",
    "        # Generate random node coordinates in [0, 1] x [0, 1]\n",
    "        nodes_coord = np.random.rand(num_nodes, 2)\n",
    "        instances.append(nodes_coord)\n",
    "    return instances\n",
    "\n",
    "# --- Константы и Конфигурация ---\n",
    "NUM_TEST_INSTANCES = 1 # Уменьшим для скорости отладки\n",
    "NUM_NODES = 500\n",
    "NUM_RUNS = 1\n",
    "BEAM_SIZE = 5 # Размер луча для Beam Search #1, 10\n",
    "\n",
    "GNN_BC_PARAMS = {\n",
    "    \"threshold\": 0.4, # 0.0, 0.3, 0.5, 0.0, 0.5, 0.9\n",
    "    \"fixing_percentage\": 0.0,\n",
    "    \"elimination_percentage\": 0.60 # 0.45, 0.45, 0.45, 0.7, 0.7, 0.7\n",
    "}\n",
    "\n",
    "RESULTS_FILENAME = f\"hyperparameter_tuning_results_bs{str(BEAM_SIZE)}_ep{str(GNN_BC_PARAMS[\"elimination_percentage\"])}_th{str(GNN_BC_PARAMS[\"threshold\"])}.csv\"\n",
    "FILE_EXISTS = os.path.exists(RESULTS_FILENAME)\n",
    "\n",
    "# Открываем файл для добавления ('a') или создаем новый ('w') с заголовком\n",
    "csv_file = open(RESULTS_FILENAME, 'a', newline='', encoding='utf-8')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "\n",
    "if not FILE_EXISTS:\n",
    "    header = [\n",
    "        \"Beam Size\", \"Elimination %\", \"Threshold\", # Параметры GNN\n",
    "        \"Instance\", \"Dimension\",                   # Параметры инстанса\n",
    "        \"Method\",                                  # Название метода\n",
    "        \"Avg Length\", \"Avg Time (s)\",              # Метрики\n",
    "        \"Length Diff\", \"Speedup vs Pure\"           # Сравнение с Pure B&C\n",
    "    ]\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "\n",
    "# Пути к моделям\n",
    "MODEL_PATH_V1 = \"./tsp_gnn_model_naive.pt\" # Модель с TransformerConv\n",
    "MODEL_PATH_V2 = \"./tsp_gnn_model_epoch_499_test_loss_0.12400513887405396.pt\" # Модель без TransformerConv\n",
    "\n",
    "# Initialize configuration\n",
    "config = {\n",
    "    'train_filepath': './tsp_data/tsp20_train_concorde.txt',\n",
    "    'val_filepath': './tsp_data/tsp20_val_concorde.txt',\n",
    "    'test_filepath': './tsp_data/tsp20_test_concorde.txt',\n",
    "    'num_nodes': 20,\n",
    "    'num_neighbors': -1,\n",
    "    'node_dim': 2,\n",
    "    'voc_nodes_in': 2,\n",
    "    'voc_nodes_out': 2,\n",
    "    'voc_edges_in': 3,\n",
    "    'voc_edges_out': 2,\n",
    "    'hidden_dim': 300,\n",
    "    'num_layers': 5,\n",
    "    'mlp_layers': 2,\n",
    "    'aggregation': 'mean',\n",
    "    'max_epochs': 50,\n",
    "    'batches_per_epoch': 256,  # Reduced for example\n",
    "    'accumulation_steps': 1,\n",
    "    'learning_rate': 0.001,\n",
    "    'decay_rate': 1.01,\n",
    "    'val_every': 3,\n",
    "    'test_every': 3,\n",
    "    'save_every': 50,\n",
    "    'batch_size': 64\n",
    "\n",
    "}\n",
    "\n",
    "# Типы тензоров\n",
    "dtypeFloat = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "dtypeLong = torch.cuda.LongTensor if torch.cuda.is_available() else torch.LongTensor\n",
    "\n",
    "# --- Загрузка Моделей ---\n",
    "print(\"Loading GNN Models...\")\n",
    "net_v1 = None\n",
    "net_v2 = None\n",
    "\n",
    "if os.path.exists(MODEL_PATH_V1):\n",
    "    net_v1 = load_model(ResidualGatedGCNModel, config, MODEL_PATH_V1, dtypeFloat, dtypeLong)\n",
    "    print(f\"Model 1 ({MODEL_PATH_V1}) loaded successfully.\")\n",
    "else:\n",
    "    print(f\"Model file not found: {MODEL_PATH_V1}\")\n",
    "    # Опционально: Запустить обучение V1, если файл не найден\n",
    "    print(\"Training Model V1...\")\n",
    "    net_v1, losses_v1 = train_gnn_model(config, num_epochs=50)\n",
    "    save_model(net_v1, MODEL_PATH_V1)\n",
    "\n",
    "if os.path.exists(MODEL_PATH_V2):\n",
    "    net_v2 = load_model(ResidualGatedGCNModel_v2, config, MODEL_PATH_V2, dtypeFloat, dtypeLong)\n",
    "    print(f\"Model 2 ({MODEL_PATH_V2}) loaded successfully.\")\n",
    "else:\n",
    "    print(f\"Model file not found: {MODEL_PATH_V2}\")\n",
    "    # Опционально: Запустить обучение V2, если файл не найден\n",
    "    print(\"Training Model V2...\")\n",
    "    net_v2, losses_v2 = train_gnn_model(config, num_epochs=500)\n",
    "    save_model(net_v2, MODEL_PATH_V2)\n",
    "\n",
    "\n",
    "\n",
    "# --- Генерация Тестовых Экземпляров ---\n",
    "test_instances = generate_test_instances(num_instances=NUM_TEST_INSTANCES, num_nodes=NUM_NODES)\n",
    "\n",
    "# --- Запуск Сравнения ---\n",
    "print(\"\\nRunning Comparison: Pure B&C vs GNN+BS(v1) vs GNN+BS(v2) vs GNN+B&C(v1) vs GNN+B&C(v2)...\")\n",
    "current_gnn_bc_params_v1 = GNN_BC_PARAMS.copy()\n",
    "current_gnn_bc_params_v2 = GNN_BC_PARAMS.copy()\n",
    "# current_gnn_bc_params_v1[\"elimination_percentage\"] = elimination_perc\n",
    "# current_gnn_bc_params_v1[\"threshold\"] = threshold\n",
    "# current_gnn_bc_params_v2[\"elimination_percentage\"] = elimination_perc\n",
    "# current_gnn_bc_params_v2[\"threshold\"] = threshold\n",
    "\n",
    "print(f\"\\n--- Testing Params: Beam={BEAM_SIZE}, Elim={current_gnn_bc_params_v1[\"elimination_percentage\"]:.1f}, Thresh={current_gnn_bc_params_v1[\"threshold\"]:.1f} ---\")\n",
    "\n",
    "# --- Инициализация Солверов ---\n",
    "gnn_solver_v1_bc = None\n",
    "if net_v1:\n",
    "    gnn_solver_v1_bc = GNNBranchCutSolver(gnn_model=net_v1, **current_gnn_bc_params_v1)\n",
    "\n",
    "gnn_solver_v2_bc = None\n",
    "if net_v2:\n",
    "    gnn_solver_v2_bc = GNNBranchCutSolver(gnn_model=net_v2, **current_gnn_bc_params_v2)\n",
    "\n",
    "for run_num in range(NUM_RUNS):\n",
    "    all_results = {}\n",
    "\n",
    "    # Передаем None для моделей/солверов, если они не загрузились\n",
    "    all_results = TSPComparison.compare_methods(\n",
    "        # Передаем актуальные солверы\n",
    "        gnn_solver_v1=gnn_solver_v1_bc,\n",
    "        gnn_solver_v2=gnn_solver_v2_bc,\n",
    "        gnn_model_v1=net_v1,\n",
    "        gnn_model_v2=net_v2,\n",
    "        config=config,\n",
    "        test_instances=test_instances, # Используем ОДИН И ТОТ ЖЕ НАБОР\n",
    "        # Передаем текущий beam_size\n",
    "        beam_size_gnn_pure=BEAM_SIZE\n",
    "    )\n",
    "\n",
    "                # --- Логирование результатов этой итерации в CSV ---\n",
    "    # if all_results:\n",
    "    #     # Получаем базовые результаты Pure B&C (предполагаем, что они не меняются от параметров GNN)\n",
    "    #     pure_res = all_results.get('pure_bc', {})\n",
    "    #     pure_avg_len = pure_res.get('avg_tour_lengths', float('inf'))\n",
    "    #     pure_avg_time = pure_res.get('avg_solving_times', float('inf'))\n",
    "\n",
    "    #     methods_to_log = {\n",
    "    #         \"GNN+BS (v1)\": 'gnn_bs_v1', \"GNN+BS (v2)\": 'gnn_bs_v2',\n",
    "    #         \"GNN+B&C (v1)\": 'gnn_bc_v1', \"GNN+B&C (v2)\": 'gnn_bc_v2'\n",
    "    #     }\n",
    "\n",
    "    #     # Извлекаем размерность (предполагаем, что она одинакова для всех инстансов в test_instances)\n",
    "    #     dimension = NUM_NODES # Берем из конфига, если test_instances могут быть разного размера\n",
    "\n",
    "    #     for name, key in methods_to_log.items():\n",
    "    #         if key in all_results and all_results[key]:\n",
    "    #             res = all_results[key]\n",
    "    #             avg_len = res.get('avg_tour_lengths', float('inf'))\n",
    "    #             avg_time = res.get('avg_solving_times', float('inf'))\n",
    "\n",
    "    #             len_diff = (avg_len - pure_avg_len) if avg_len != float('inf') and pure_avg_len != float('inf') else float('inf')\n",
    "    #             speedup = (pure_avg_time / avg_time) if avg_time > 0 and pure_avg_time != float('inf') else float('inf')\n",
    "\n",
    "    #             # Записываем строку в CSV\n",
    "    #             csv_writer.writerow([\n",
    "    #                 BEAM_SIZE, current_gnn_bc_params_v1[\"elimination_percentage\"], current_gnn_bc_params_v1[\"threshold\"],\n",
    "    #                 f\"Avg_{len(test_instances)}x{dimension}\", dimension, # Имя инстанса теперь обобщенное\n",
    "    #                 name,\n",
    "    #                 avg_len, avg_time,\n",
    "    #                 len_diff, speedup\n",
    "    #             ])\n",
    "\n",
    "    # csv_file.close() # Обязательно закрыть файл\n",
    "    # print(f\"\\nHyperparameter tuning results saved to {RESULTS_FILENAME}\")\n",
    "\n",
    "    print(\"\\n===== Overall Method Comparison =====\")\n",
    "    print(\"Method               | Avg Length     | Length Diff | Avg Time (s) | Speedup vs Pure\")\n",
    "    print(\"---------------------|----------------|-------------|--------------|-----------------\")\n",
    "\n",
    "    if all_results:\n",
    "        pure_avg_len = all_results['pure_bc'].get('avg_tour_lengths', float('inf'))\n",
    "        pure_avg_time = all_results['pure_bc'].get('avg_solving_times', float('inf'))\n",
    "        if pure_avg_len != float('inf') and pure_avg_time != float('inf'):\n",
    "            print(f\"Pure B&C           | {pure_avg_len:<14.4f} | +0.0000     | {pure_avg_time:<12.4f} | 1.00x\")\n",
    "        else:\n",
    "            print(\"Pure B&C results could not be calculated.\")\n",
    "\n",
    "        # Определяем порядок и ключи для вывода\n",
    "        methods_to_print = {\n",
    "            \"GNN+BS (v1)\": 'gnn_bs_v1',\n",
    "            \"GNN+BS (v2)\": 'gnn_bs_v2',\n",
    "            \"GNN+B&C (v1)\": 'gnn_bc_v1',\n",
    "            \"GNN+B&C (v2)\": 'gnn_bc_v2',\n",
    "            \"Simulated Annealing + 2-opt (outer)\": \"simulated_annealing_2opt\"\n",
    "        }\n",
    "\n",
    "        for name, key in methods_to_print.items():\n",
    "            if key in all_results and all_results[key]: # Проверяем наличие ключа и данных\n",
    "                res = all_results[key]\n",
    "                avg_len = res.get('avg_tour_lengths', float('inf'))\n",
    "                avg_time = res.get('avg_solving_times', float('inf'))\n",
    "                # speedup = (pure_avg_time / avg_time) if avg_time > 0 and pure_avg_time != float('inf') else float('inf')\n",
    "                # len_diff = (avg_len - pure_avg_len) if avg_len != float('inf') and pure_avg_len != float('inf') else float('inf')\n",
    "                # print(f\"{name:<20} | {avg_len:<14.4f} | {len_diff:<+11.4f} | {avg_time:<12.4f} | {speedup:.2f}x\")\n",
    "            else:\n",
    "                print(f\"{name:<20} | N/A            | N/A         | N/A          | N/A\")\n",
    "\n",
    "    print(\"-------------------------------------------------------------------------------\")\n",
    "\n",
    "    # --- Визуализация (Пример: Сравнение Pure B&C и GNN+BS(v1)) ---\n",
    "    method_to_compare_1 = 'pure_bc'\n",
    "    method_to_compare_2 = 'gnn_bs_v1'\n",
    "    method_to_compare_3 = 'gnn_bs_v2'\n",
    "    method_to_compare_4 = 'gnn_bc_v1'\n",
    "    method_to_compare_5 = 'gnn_bc_v2'\n",
    "    method_to_compare_6 = 'simulated_annealing_2opt'\n",
    "\n",
    "    # for instance_idx_viz in range(5):\n",
    "    #     if instance_idx_viz < len(test_instances) and method_to_compare_1 in all_results and method_to_compare_2 in all_results:\n",
    "    #         coords_viz = test_instances[instance_idx_viz]\n",
    "    #         tour_pure_viz = all_results['pure_bc']['tour'][instance_idx_viz]\n",
    "    #         len_pure_viz = all_results['pure_bc']['tour_lengths'][instance_idx_viz]\n",
    "    #         tour_gnn_viz = all_results['gnn_bs_v1']['tour'][instance_idx_viz]\n",
    "    #         len_gnn_viz = all_results['gnn_bs_v1']['tour_lengths'][instance_idx_viz]\n",
    "\n",
    "    #         if tour_pure_viz and tour_gnn_viz:\n",
    "    #             plot_comparison(\n",
    "    #                 nodes_coord=coords_viz,\n",
    "    #                 found_tour=tour_gnn_viz,\n",
    "    #                 found_length=len_gnn_viz,\n",
    "    #                 optimal_tour=tour_pure_viz,\n",
    "    #                 optimal_length=len_pure_viz,\n",
    "    #                 instance_name=f\"Instance {instance_idx_viz+1}\",\n",
    "    #                 method_name=\"GNN+BS (v1)\"\n",
    "    #             )\n",
    "\n",
    "\n",
    "    # for instance_idx_viz in range(5):\n",
    "    #     if instance_idx_viz < len(test_instances) and method_to_compare_1 in all_results and method_to_compare_2 in all_results:\n",
    "    #         coords_viz = test_instances[instance_idx_viz]\n",
    "    #         tour_pure_viz = all_results['pure_bc']['tour'][instance_idx_viz]\n",
    "    #         len_pure_viz = all_results['pure_bc']['tour_lengths'][instance_idx_viz]\n",
    "    #         tour_gnn_viz = all_results['gnn_bc_v1']['tour'][instance_idx_viz]\n",
    "    #         len_gnn_viz = all_results['gnn_bc_v1']['tour_lengths'][instance_idx_viz]\n",
    "\n",
    "    #         if tour_pure_viz and tour_gnn_viz:\n",
    "    #             plot_comparison(\n",
    "    #                 nodes_coord=coords_viz,\n",
    "    #                 found_tour=tour_gnn_viz,\n",
    "    #                 found_length=len_gnn_viz,\n",
    "    #                 optimal_tour=tour_pure_viz,\n",
    "    #                 optimal_length=len_pure_viz,\n",
    "    #                 instance_name=f\"Instance {instance_idx_viz+1}\",\n",
    "    #                 method_name=\"GNN+BC (v1)\"\n",
    "    #             )\n",
    "\n",
    "    # for instance_idx_viz in range(5):\n",
    "    #     if instance_idx_viz < len(test_instances) and method_to_compare_1 in all_results and method_to_compare_2 in all_results:\n",
    "    #         coords_viz = test_instances[instance_idx_viz]\n",
    "    #         tour_pure_viz = all_results['pure_bc']['tour'][instance_idx_viz]\n",
    "    #         len_pure_viz = all_results['pure_bc']['tour_lengths'][instance_idx_viz]\n",
    "    #         tour_gnn_viz = all_results['gnn_bc_v2']['tour'][instance_idx_viz]\n",
    "    #         len_gnn_viz = all_results['gnn_bc_v2']['tour_lengths'][instance_idx_viz]\n",
    "\n",
    "    #         if tour_pure_viz and tour_gnn_viz:\n",
    "    #             plot_comparison(\n",
    "    #                 nodes_coord=coords_viz,\n",
    "    #                 found_tour=tour_gnn_viz,\n",
    "    #                 found_length=len_gnn_viz,\n",
    "    #                 optimal_tour=tour_pure_viz,\n",
    "    #                 optimal_length=len_pure_viz,\n",
    "    #                 instance_name=f\"Instance {instance_idx_viz+1}\",\n",
    "    #                 method_name=\"GNN+BC (v2)\"\n",
    "    #             )\n",
    "\n",
    "    # 5. Print results\n",
    "    print(\"\\nResults Summary:\")\n",
    "    print(f\"Pure Branch & Cut - Avg Tour Length: {all_results['pure_bc']['avg_tour_lengths']:.4f}, Avg Time: {all_results['pure_bc'].get('avg_solving_times', float('inf')):.4f}s\")\n",
    "    print(f\"GNN Branch & Cut (v1) - Avg Tour Length: {all_results['gnn_bc_v1']['avg_tour_lengths']:.4f}, Avg Time: {all_results['gnn_bc_v1'].get('avg_solving_times', float('inf')):.4f}s\")\n",
    "    print(f\"GNN Branch & Cut (v2) - Avg Tour Length: {all_results['gnn_bc_v2']['avg_tour_lengths']:.4f}, Avg Time: {all_results['gnn_bc_v2'].get('avg_solving_times', float('inf')):.4f}s\")\n",
    "    print(f\"GNN+BS (v1) - Avg Tour Length: {all_results['gnn_bs_v1']['avg_tour_lengths']:.4f}, Avg Time: {all_results['gnn_bs_v1'].get('avg_solving_times', float('inf')):.4f}s\")\n",
    "    print(f\"GNN+BS (v2) - Avg Tour Length: {all_results['gnn_bs_v2']['avg_tour_lengths']:.4f}, Avg Time: {all_results['gnn_bs_v2'].get('avg_solving_times', float('inf')):.4f}s\")\n",
    "    print(f\"Simulated Annealing + 2-opt (outer) - Avg Tour Length: {all_results['simulated_annealing_2opt']['avg_tour_lengths']:.4f}, Avg Time: {all_results['simulated_annealing_2opt'].get('avg_solving_times', float('inf')):.4f}s\")\n",
    "    \n",
    "    # Calculate improvements\n",
    "    #time_improvement_v1 = (all_results['pure_bc'].get('avg_solving_times', float('inf')) - all_results['gnn_bc_v1'].get('avg_solving_times', float('inf'))) / all_results['pure_bc'].get('avg_solving_times', float('inf')) * 100\n",
    "    #time_improvement_v2 = (all_results['pure_bc'].get('avg_solving_times', float('inf')) - all_results['gnn_bc_v2'].get('avg_solving_times', float('inf'))) / all_results['pure_bc'].get('avg_solving_times', float('inf')) * 100\n",
    "    #quality_diff_v1 = (all_results['pure_bc']['avg_tour_lengths'] - all_results['gnn_bc_v1']['avg_tour_lengths']) / all_results['pure_bc']['avg_tour_lengths'] * 100\n",
    "    #quality_diff_v2 = (all_results['pure_bc']['avg_tour_lengths'] - all_results['gnn_bc_v2']['avg_tour_lengths']) / all_results['pure_bc']['avg_tour_lengths'] * 100\n",
    "\n",
    "    #print(f\"\\nTime improvement (bc v1 vs pure bc): {time_improvement_v1:.2f}%\")\n",
    "    #print(f\"Solution quality difference (bc v1 vs pure bc): {quality_diff_v1:.2f}% ({'better' if quality_diff_v1 >= 0 else 'worse'})\")\n",
    "\n",
    "    # print(f\"\\nTime improvement (bc v2 vs pure bc): {time_improvement_v2:.2f}%\")\n",
    "    # print(f\"Solution quality difference (bc v2 vs pure bc): {quality_diff_v2:.2f}% ({'better' if quality_diff_v2 >= 0 else 'worse'})\")\n",
    "\n",
    "    # 6. Visualize comparison\n",
    "    print(\"Results:\\n\", all_results)\n",
    "\n",
    "    aggregated_results = {}\n",
    "    method_keys_to_aggregate = ['pure_bc', 'gnn_bs_v1', 'gnn_bs_v2', 'gnn_bc_v1', 'gnn_bc_v2', 'simulated_annealing_2opt']\n",
    "\n",
    "    for method_key in method_keys_to_aggregate:\n",
    "        if method_key in all_results:\n",
    "            # Просто копируем уже вычисленные средние\n",
    "            aggregated_results[method_key] = {\n",
    "                'avg_tour_lengths': all_results[method_key].get('avg_tour_lengths', float('inf')),\n",
    "                'avg_solving_times': all_results[method_key].get('avg_solving_times', float('inf'))\n",
    "            }\n",
    "        else:\n",
    "            # Если метод не тестировался, добавляем N/A (или inf)\n",
    "            aggregated_results[method_key] = {\n",
    "                'avg_tour_lengths': float('inf'),\n",
    "                'avg_solving_times': float('inf')\n",
    "            }\n",
    "\n",
    "    if 'TSPComparison' in locals() and hasattr(TSPComparison, 'plot_comparison_results'):\n",
    "        TSPComparison.plot_comparison_results(aggregated_results)\n",
    "    else:\n",
    "        print(\"Warning: TSPComparison.plot_comparison_results not found or TSPComparison not defined.\")\n",
    "\n",
    "    TSPComparison.plot_comparison_results(aggregated_results)\n",
    "\n",
    "    # 7. Detailed visualization of one instance\n",
    "    instance_idx = 0  # Choose first instance\n",
    "    print(f\"\\nVisualizing detailed comparison for test instance {instance_idx + 1}...\")\n",
    "\n",
    "    # # Solve with pure Branch & Cut\n",
    "    # tour_pure, tour_length_pure, _, _ = TSPComparison.pure_branch_cut_solve(test_instances[instance_idx])\n",
    "\n",
    "    # # Solve with GNN-guided Branch & Cut (v1)\n",
    "    # tour_gnn, tour_length_gnn, _ = gnn_solver_v1_bc.solve_tsp(test_instances[instance_idx])\n",
    "\n",
    "    # # Visualize\n",
    "    # TSPComparison.visualize_tours(\n",
    "    #     test_instances[instance_idx],\n",
    "    #     tour_pure,\n",
    "    #     tour_gnn,\n",
    "    #     f\"TSP Instance Comparison - Pure: {tour_length_pure:.4f}, GNN: {tour_length_gnn:.4f}\"\n",
    "    # )\n",
    "    \n",
    "    print(\"\\nRaw Results:\", all_results)\n",
    "\n",
    "    # # Конвертируем туры numpy/list в списки для JSON\n",
    "    # def make_json_serializable(obj):\n",
    "    #     if isinstance(obj, np.ndarray): return obj.tolist()\n",
    "    #     if isinstance(obj, list): return [make_json_serializable(item) for item in obj]\n",
    "    #     if isinstance(obj, dict): return {k: make_json_serializable(v) for k, v in obj.items()}\n",
    "    #     if isinstance(obj, tuple): return tuple(make_json_serializable(item) for item in obj)\n",
    "    #     # Добавьте другие типы по необходимости (Path и т.д.)\n",
    "    #     if isinstance(obj, Path): return str(obj)\n",
    "    #     if isinstance(obj, (int, float, str, bool)) or obj is None: return obj\n",
    "    #     return repr(obj) # Запасной вариант\n",
    "\n",
    "    # # Преобразуем ключи-кортежи в строки\n",
    "    # string_key_results = {str(k): make_json_serializable(v) for k, v in all_results.items()}\n",
    "\n",
    "    BASE_PATH = './benchmark_data/'\n",
    "\n",
    "    try:\n",
    "        with open(f\"{BASE_PATH}concorde_tsp_outputs/benchmark_results_nodes_{NUM_NODES}_instances_{NUM_TEST_INSTANCES}_run_{run_num}.json\", \"w\") as f:\n",
    "            json.dump(all_results, f, indent=4)\n",
    "        print(\"\\nBenchmark results saved to benchmark_results.json\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving results to JSON: {e}\")\n",
    "\n",
    "# 8. Analyze GNN predictions\n",
    "# print(\"\\nAnalyzing GNN edge predictions...\")\n",
    "# edge_probs = gnn_solver_v1_bc.get_edge_probabilities(test_instances[instance_idx])\n",
    "\n",
    "# # Calculate prediction accuracy\n",
    "# # Create ground truth edge mask from optimal tour\n",
    "# true_edges = np.zeros((config['num_nodes'], config['num_nodes']))\n",
    "# for i in range(len(tour_pure)):\n",
    "#     j = (i + 1) % len(tour_pure)\n",
    "#     true_edges[tour_pure[i], tour_pure[j]] = 1\n",
    "#     true_edges[tour_pure[j], tour_pure[i]] = 1\n",
    "\n",
    "# # Convert edge probabilities to binary predictions using threshold\n",
    "# pred_edges = (edge_probs > 0.5).astype(int)\n",
    "\n",
    "# # Calculate accuracy metrics\n",
    "# true_positives = np.sum((pred_edges == 1) & (true_edges == 1))\n",
    "# false_positives = np.sum((pred_edges == 1) & (true_edges == 0))\n",
    "# false_negatives = np.sum((pred_edges == 0) & (true_edges == 1))\n",
    "# true_negatives = np.sum((pred_edges == 0) & (true_edges == 0))\n",
    "\n",
    "# precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "# recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "# f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "# print(f\"Edge prediction precision: {precision:.4f}\")\n",
    "# print(f\"Edge prediction recall: {recall:.4f}\")\n",
    "# print(f\"Edge prediction F1 score: {f1_score:.4f}\")\n",
    "\n",
    "# # 9. Analyze effect of different GNN parameters\n",
    "# print(\"\\nAnalyzing effect of different GNN threshold parameters...\")\n",
    "\n",
    "# thresholds = [0.3, 0.5, 0.7, 0.9]\n",
    "# fixing_percentages = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# elimination_percentages = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# # Create test instance\n",
    "# test_instance = test_instances[0]\n",
    "\n",
    "# # Get baseline result\n",
    "# _, baseline_length, baseline_time, _ = TSPComparison.pure_branch_cut_solve(test_instance)\n",
    "# print(f\"Baseline - Length: {baseline_length:.4f}, Time: {baseline_time:.4f}s\")\n",
    "\n",
    "# # Test different parameters\n",
    "# for threshold in thresholds:\n",
    "#     for fixing_percentage in fixing_percentages:\n",
    "#       for elimination_percentage in elimination_percentages:\n",
    "#         # Update solver parameters\n",
    "#         gnn_solver_v1_bc.threshold = threshold\n",
    "#         gnn_solver_v1_bc.fixing_percentage = fixing_percentage\n",
    "#         # Задаем elimination_percentage отдельно (например, фиксируем на 10%)\n",
    "#         #gnn_solver.elimination_percentage = 0.1\n",
    "#         gnn_solver_v1_bc.elimination_percentage = elimination_percentage # вернул обратно\n",
    "\n",
    "#         # Solve\n",
    "#         start_time = time.time()\n",
    "#         _, tour_length, _ = gnn_solver_v1_bc.solve_tsp(test_instance)\n",
    "#         solve_time = time.time() - start_time\n",
    "\n",
    "#         # Print results\n",
    "#         print(f\"Threshold: {threshold}, Fixing: {fixing_percentage*100}%, Elimination: {elimination_percentage*100}% - Length: {tour_length:.4f}, Time: {solve_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779e6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nRaw Results:\", all_results)\n",
    "\n",
    "# import json\n",
    "# # # Конвертируем туры numpy/list в списки для JSON\n",
    "# # def make_json_serializable(obj):\n",
    "# #     if isinstance(obj, np.ndarray): return obj.tolist()\n",
    "# #     if isinstance(obj, list): return [make_json_serializable(item) for item in obj]\n",
    "# #     if isinstance(obj, dict): return {k: make_json_serializable(v) for k, v in obj.items()}\n",
    "# #     if isinstance(obj, tuple): return tuple(make_json_serializable(item) for item in obj)\n",
    "# #     # Добавьте другие типы по необходимости (Path и т.д.)\n",
    "# #     if isinstance(obj, Path): return str(obj)\n",
    "# #     if isinstance(obj, (int, float, str, bool)) or obj is None: return obj\n",
    "# #     return repr(obj) # Запасной вариант\n",
    "\n",
    "# # # Преобразуем ключи-кортежи в строки\n",
    "# # string_key_results = {str(k): make_json_serializable(v) for k, v in all_results.items()}\n",
    "\n",
    "# BASE_PATH = './benchmark_data/'\n",
    "\n",
    "# try:\n",
    "#     with open(f\"{BASE_PATH}concorde_tsp_outputs/benchmark_results_{NUM_NODES}_{NUM_TEST_INSTANCES}.json\", \"w\") as f:\n",
    "#         json.dump(string_key_results, f, indent=4)\n",
    "#     print(\"\\nBenchmark results saved to benchmark_results.json\")\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nError saving results to JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0065a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "RESULTS_FILENAME = \"hyperparameter_tuning_results_all.csv\"\n",
    "print(f\"\\n--- Analyzing Results from {RESULTS_FILENAME} ---\")\n",
    "\n",
    "# --- Загрузка данных из CSV ---\n",
    "try:\n",
    "    df_results = pd.read_csv(RESULTS_FILENAME)\n",
    "    # Заменяем inf на NaN для удобства\n",
    "    df_results.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    # Преобразуем типы, если нужно\n",
    "    df_results['Beam Size'] = pd.to_numeric(df_results['Beam Size'], errors='coerce').fillna(0).astype(int)\n",
    "    df_results['Elimination %'] = pd.to_numeric(df_results['Elimination %'], errors='coerce')\n",
    "    df_results['Threshold'] = pd.to_numeric(df_results['Threshold'], errors='coerce')\n",
    "    df_results['Avg Length'] = pd.to_numeric(df_results['Avg Length'], errors='coerce')\n",
    "    df_results['Avg Time (s)'] = pd.to_numeric(df_results['Avg Time (s)'], errors='coerce')\n",
    "    df_results['Length Diff'] = pd.to_numeric(df_results['Length Diff'], errors='coerce')\n",
    "    df_results['Speedup vs Pure'] = pd.to_numeric(df_results['Speedup vs Pure'], errors='coerce')\n",
    "\n",
    "    print(\"Loaded DataFrame:\")\n",
    "    print(df_results.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Results file '{RESULTS_FILENAME}' not found.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or processing results file: {e}\")\n",
    "    exit()\n",
    "\n",
    "if df_results.empty:\n",
    "    print(\"No data to analyze.\")\n",
    "else:\n",
    "    # --- Считаем СРЕДНЕЕ время Pure B&C по всем записям ---\n",
    "    # pure_bc_times = df_results[df_results['Method'] == 'Pure B&C']['Avg Time (s)'].dropna()\n",
    "    # avg_pure_bc_time = pure_bc_times.mean() if not pure_bc_times.empty else np.nan\n",
    "    avg_pure_bc_time = 6.9507\n",
    "    print(f\"\\nAverage Pure B&C time across all runs: {avg_pure_bc_time:.4f}s\")\n",
    "\n",
    "    # --- Пересчитываем Speedup, используя СРЕДНЕЕ время Pure B&C ---\n",
    "    if not np.isnan(avg_pure_bc_time) and avg_pure_bc_time > 0:\n",
    "        # Создаем новую колонку или перезаписываем старую\n",
    "        df_results['Speedup vs Avg Pure'] = df_results.apply(\n",
    "            lambda row: avg_pure_bc_time / row['Avg Time (s)'] if pd.notna(row['Avg Time (s)']) and row['Avg Time (s)'] > 0 else np.nan,\n",
    "            axis=1\n",
    "        )\n",
    "        # Удаляем старую колонку Speedup, если она была\n",
    "        if 'Speedup vs Pure' in df_results.columns:\n",
    "             df_results.drop(columns=['Speedup vs Pure'], inplace=True)\n",
    "        # Переименовываем новую колонку для консистентности\n",
    "        df_results.rename(columns={'Speedup vs Avg Pure': 'Speedup vs Pure'}, inplace=True)\n",
    "        print(\"Recalculated 'Speedup vs Pure' using average Pure B&C time.\")\n",
    "    else:\n",
    "        print(\"Could not calculate average Pure B&C time, Speedup calculation skipped.\")\n",
    "    # --- Визуализация зависимостей ---\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # Выбираем методы для анализа зависимостей\n",
    "    # Например, сравним GNN+BS(v2) и GNN+B&C(v2)\n",
    "    methods_to_analyze = [\"GNN+BS (v2)\", \"GNN+B&C (v2)\"]\n",
    "    df_filtered = df_results[df_results['Method'].isin(methods_to_analyze)]\n",
    "\n",
    "    # --- График 1: Зависимость Длины от Beam Size (для GNN+BS) ---\n",
    "    df_bs = df_filtered[df_filtered['Method'].str.contains(\"GNN\\+BS\")]\n",
    "    if not df_bs.empty:\n",
    "         plt.figure(figsize=(10, 6))\n",
    "         sns.lineplot(data=df_bs, x='Beam Size', y='Avg Length', hue='Method', marker='o')\n",
    "         plt.title('Влияние Beam Size на Качество GNN+BS')\n",
    "         plt.xlabel('Beam Size (K)')\n",
    "         plt.ylabel('Средняя Длина Тура')\n",
    "         plt.grid(True, linestyle='--', alpha=0.7)\n",
    "         plt.legend(title='Метод')\n",
    "         plt.show()\n",
    "\n",
    "    # --- График 2: Зависимость Времени от Beam Size (для GNN+BS) ---\n",
    "    if not df_bs.empty:\n",
    "         plt.figure(figsize=(10, 6))\n",
    "         sns.lineplot(data=df_bs, x='Beam Size', y='Avg Time (s)', hue='Method', marker='o')\n",
    "         plt.title('Влияние Beam Size на Время GNN+BS')\n",
    "         plt.xlabel('Beam Size (K)')\n",
    "         plt.ylabel('Среднее Время (с)')\n",
    "         plt.yscale('log') # Время лучше смотреть в лог. масштабе\n",
    "         plt.grid(True, linestyle='--', alpha=0.7)\n",
    "         plt.legend(title='Метод')\n",
    "         plt.show()\n",
    "\n",
    "    # --- График 3: Зависимость Качества GNN+B&C от Elimination % (при фикс. Threshold) ---\n",
    "    df_bc = df_filtered[df_filtered['Method'].str.contains(\"GNN\\+B\\&C\")]\n",
    "    fixed_threshold = 0.5 # Выберите порог для анализа\n",
    "    df_bc_thresh = df_bc[np.isclose(df_bc['Threshold'], fixed_threshold)]\n",
    "    if not df_bc_thresh.empty:\n",
    "         plt.figure(figsize=(10, 6))\n",
    "         sns.lineplot(data=df_bc_thresh, x='Elimination %', y='Length Diff', hue='Method', marker='o')\n",
    "         plt.title(f'Влияние Elimination % на Отклонение GNN+B&C (Threshold={fixed_threshold})')\n",
    "         plt.xlabel('Elimination Percentage')\n",
    "         plt.ylabel('Среднее Отклонение от Оптимума (Length Diff)')\n",
    "         plt.xticks(np.arange(0, 1, 0.1)) # Метки от 0 до 0.9\n",
    "         plt.grid(True, linestyle='--', alpha=0.7)\n",
    "         plt.legend(title='Метод')\n",
    "         plt.axhline(0, color='grey', linestyle=':', linewidth=1)\n",
    "         plt.show()\n",
    "\n",
    "    # --- График 4: Зависимость Времени GNN+B&C от Elimination % (при фикс. Threshold) ---\n",
    "    if not df_bc_thresh.empty:\n",
    "         plt.figure(figsize=(10, 6))\n",
    "         sns.lineplot(data=df_bc_thresh, x='Elimination %', y='Avg Time (s)', hue='Method', marker='o')\n",
    "         plt.title(f'Влияние Elimination % на Время GNN+B&C (Threshold={fixed_threshold})')\n",
    "         plt.xlabel('Elimination Percentage')\n",
    "         plt.ylabel('Среднее Время (с)')\n",
    "         plt.xticks(np.arange(0, 1, 0.1))\n",
    "         plt.yscale('log')\n",
    "         plt.grid(True, linestyle='--', alpha=0.7)\n",
    "         plt.legend(title='Метод')\n",
    "         plt.show()\n",
    "\n",
    "    # --- Таблица Лучших Параметров (Пример) ---\n",
    "    print(\"\\n--- Лучшие результаты для каждого метода ---\")\n",
    "    # Находим строку с минимальной средней длиной для каждого метода\n",
    "    best_results = df_results.loc[df_results.groupby('Method')['Avg Length'].idxmin()]\n",
    "    print(best_results[['Method', 'Beam Size', 'Elimination %', 'Threshold', 'Avg Length', 'Length Diff', 'Avg Time (s)', 'Speedup vs Pure']].round(4).to_string(index=False))\n",
    "\n",
    "    print(\"\\n--- Самые быстрые результаты (с приемлемым качеством, например, Diff < 0.1) ---\")\n",
    "    fast_good_results = df_results[(df_results['Length Diff'].fillna(float('inf')) < 0.1)].sort_values('Avg Time (s)')\n",
    "    print(fast_good_results[['Method', 'Beam Size', 'Elimination %', 'Threshold', 'Avg Length', 'Length Diff', 'Avg Time (s)', 'Speedup vs Pure']].round(4).to_string(index=False))\n",
    "\n",
    "    # --- Таблица Лучших по ВРЕМЕНИ для каждого метода ---\n",
    "    print(\"\\n--- Лучшие результаты по ВРЕМЕНИ для каждого метода ---\")\n",
    "    # Находим строку с минимальным средним временем для каждого метода\n",
    "    # Обрабатываем NaN перед поиском минимума\n",
    "    df_results_valid_time = df_results.dropna(subset=['Speedup vs Pure'])\n",
    "    if not df_results_valid_time.empty:\n",
    "        best_time_results = df_results_valid_time.loc[df_results_valid_time.groupby('Method')['Speedup vs Pure'].idxmax()]\n",
    "        print(best_time_results[['Method', 'Beam Size', 'Elimination %', 'Threshold', 'Avg Length', 'Length Diff', 'Avg Time (s)', 'Speedup vs Pure']].round(4).to_string(index=False))\n",
    "    else:\n",
    "        print(\"Нет валидных данных по времени для поиска лучших.\")\n",
    "\n",
    "\n",
    "    # --- Таблица Быстрых (по времени) результатов с приемлемым качеством ---\n",
    "    print(\"\\n--- Самые БЫСТРЫЕ результаты с приемлемым качеством (например, Diff < 0.1) ---\")\n",
    "    quality_threshold = 0.1 # Порог для Length Diff\n",
    "    # Фильтруем по качеству и сортируем по времени\n",
    "    fast_good_results_by_time = df_results[(df_results['Length Diff'].fillna(float('inf')) < quality_threshold)].sort_values('Speedup vs Pure', ascending=False)\n",
    "    if not fast_good_results_by_time.empty:\n",
    "        print(fast_good_results_by_time[['Method', 'Beam Size', 'Elimination %', 'Threshold', 'Avg Length', 'Length Diff', 'Avg Time (s)', 'Speedup vs Pure']].round(4).to_string(index=False))\n",
    "    else:\n",
    "        print(f\"Нет результатов с Length Diff < {quality_threshold}.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 10. ВИЗУАЛИЗАЦИЯ РАСПРЕДЕЛЕНИЯ ДЛИН ТУРОВ (VIOLIN PLOT)\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import textwrap # Уже импортирован выше, но для ясности\n",
    "import numpy as np # Убедитесь, что numpy импортирован\n",
    "import matplotlib.pyplot as plt # Убедитесь, что pyplot импортирован\n",
    "\n",
    "print(\"\\nGenerating Violin Plot for Tour Length Distribution...\")\n",
    "\n",
    "# --- Подготовка данных для графика ---\n",
    "plot_data_list_violin = []\n",
    "# Имена методов, как они хранятся в ключах all_results\n",
    "method_keys_for_plot = ['pure_bc', 'gnn_bs_v1', 'gnn_bs_v2', 'gnn_bc_v1', 'gnn_bc_v2']\n",
    "# Красивые имена для отображения на графике\n",
    "method_display_names = {\n",
    "    'pure_bc': 'Pure B&C',\n",
    "    'gnn_bs_v1': 'GNN+BS (v1)',\n",
    "    'gnn_bs_v2': 'GNN+BS (v2)',\n",
    "    'gnn_bc_v1': 'GNN+B&C (v1)',\n",
    "    'gnn_bc_v2': 'GNN+B&C (v2)'\n",
    "}\n",
    "\n",
    "# --- Получаем размерности инстансов ---\n",
    "# Нам нужен способ сопоставить каждую длину из списка с размерностью инстанса.\n",
    "# Предположим, что порядок инстансов в test_instances соответствует порядку\n",
    "# результатов в списках tour_lengths/solving_times/tour внутри all_results.\n",
    "instance_dimensions = [inst.shape[0] for inst in test_instances] # Список размерностей [N1, N2, ...]\n",
    "\n",
    "if not all_results:\n",
    "     print(\"Error: 'all_results' dictionary is empty.\")\n",
    "else:\n",
    "    # Итерируем по методам\n",
    "    for method_key in method_keys_for_plot:\n",
    "        if method_key in all_results and all_results[method_key]:\n",
    "            method_data = all_results[method_key]\n",
    "            display_name = method_display_names.get(method_key, method_key)\n",
    "\n",
    "            # Получаем список длин для этого метода\n",
    "            lengths_list = method_data.get('tour_lengths', [])\n",
    "\n",
    "            # Проверяем соответствие длины списков\n",
    "            if len(lengths_list) != len(instance_dimensions):\n",
    "                print(f\"Warning: Mismatch between number of lengths ({len(lengths_list)}) and instances ({len(instance_dimensions)}) for method '{display_name}'. Skipping this method for violin plot.\")\n",
    "                continue\n",
    "\n",
    "            # Создаем записи для DataFrame\n",
    "            for i, length_val in enumerate(lengths_list):\n",
    "                # Пропускаем невалидные длины (inf)\n",
    "                if length_val is None or length_val == float('inf'):\n",
    "                    continue\n",
    "\n",
    "                dimension_val = instance_dimensions[i] # Берем размерность из списка\n",
    "                plot_data_list_violin.append({\n",
    "                    'solver_name': display_name,\n",
    "                    'dimension': dimension_val,\n",
    "                    'length': length_val\n",
    "                })\n",
    "        else:\n",
    "             print(f\"Warning: No data found for method '{method_key}' in all_results.\")\n",
    "\n",
    "\n",
    "    # --- Создание DataFrame ---\n",
    "    plot_df_violin = pd.DataFrame(plot_data_list_violin)\n",
    "\n",
    "    if plot_df_violin.empty:\n",
    "        print(\"Error: No valid data available to generate violin plot after processing.\")\n",
    "    else:\n",
    "        print(\"\\nDataFrame for violin plot prepared:\")\n",
    "        print(plot_df_violin.head())\n",
    "        print(f\"\\nDimensions in violin plot data: {sorted(plot_df_violin['dimension'].unique())}\")\n",
    "        print(f\"Solvers in violin plot data: {plot_df_violin['solver_name'].unique()}\")\n",
    "\n",
    "        # --- Визуализация Violin Plot ---\n",
    "        print(\"\\nGenerating violin plot...\")\n",
    "\n",
    "        # Перенос длинных названий методов\n",
    "        wrap_width = 18\n",
    "        plot_df_violin['solver_wrapped'] = plot_df_violin['solver_name'].apply(\n",
    "            lambda x: textwrap.fill(str(x).replace(\"_\", \" \"), wrap_width)\n",
    "        )\n",
    "\n",
    "        # Получаем уникальные размерности и сортируем их\n",
    "        dimensions_found = sorted(plot_df_violin['dimension'].unique())\n",
    "        dimensions_found = [d for d in dimensions_found if not np.isnan(d)]\n",
    "\n",
    "        if not dimensions_found:\n",
    "             print(\"Error: No valid dimensions found in the data.\")\n",
    "        else:\n",
    "            solver_order = plot_df_violin.groupby('solver_wrapped')['length'].median().sort_values().index\n",
    "\n",
    "            # --- ИЗМЕНЕНИЕ: Уменьшаем height и aspect ---\n",
    "            num_dims = len(dimensions_found)\n",
    "            col_wrap_val = min(num_dims, 3)\n",
    "            # Сделаем высоту меньше, например, фиксированной или меньше зависящей от числа солверов\n",
    "            plot_height = 6 # Попробуйте фиксированную высоту\n",
    "            # Аспектное соотношение можно сделать ближе к квадратному или чуть шире\n",
    "            plot_aspect = 1.2 # Попробуйте значение > 1\n",
    "\n",
    "            g = sns.FacetGrid(plot_df_violin, col=\"dimension\", col_order=dimensions_found,\n",
    "                              height=plot_height, # Используем новую высоту\n",
    "                              aspect=plot_aspect, # Используем новый аспект\n",
    "                              sharex=False, col_wrap=col_wrap_val)\n",
    "            # --- КОНЕЦ ИЗМЕНЕНИЯ ---\n",
    "\n",
    "            # Рисуем скрипичные диаграммы\n",
    "            g.map_dataframe(sns.violinplot, x=\"length\", y=\"solver_wrapped\",\n",
    "                            palette=\"viridis\", orient='h', cut=0, inner=None, linewidth=1.0,\n",
    "                            order=solver_order)\n",
    "\n",
    "            # Рисуем точки поверх\n",
    "            g.map_dataframe(sns.stripplot, x=\"length\", y=\"solver_wrapped\",\n",
    "                            color=\"black\", alpha=0.4, size=3, orient='h', jitter=0.15,\n",
    "                            order=solver_order)\n",
    "\n",
    "            # Настройка графиков (остается без изменений)\n",
    "            g.fig.suptitle('Длины решений', y=1.03, fontsize=16)\n",
    "            g.set_titles(\"N = {col_name:.0f}\")\n",
    "            g.set_axis_labels(\"Длина Найденного Тура\", \"Алгоритм\")\n",
    "            for ax in g.axes.flat:\n",
    "                ax.tick_params(axis='y', labelsize=9)\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "            plt.show()\n",
    "\n",
    "            # Создаем FacetGrid\n",
    "            num_dims = len(dimensions_found)\n",
    "            col_wrap_val = min(num_dims, 3)\n",
    "            g = sns.FacetGrid(plot_df_violin, col=\"dimension\", col_order=dimensions_found,\n",
    "                              height=max(5, len(solver_order) * 0.8), # Адаптируем высоту\n",
    "                              aspect= max(0.5, 6 / col_wrap_val / max(4, len(solver_order)*0.8) * 5), # Адаптируем аспект\n",
    "                              sharex=False, col_wrap=col_wrap_val)\n",
    "\n",
    "            # Рисуем скрипичные диаграммы\n",
    "            g.map_dataframe(sns.violinplot, x=\"length\", y=\"solver_wrapped\",\n",
    "                            palette=\"viridis\", orient='h', cut=0, inner=None, linewidth=1.0,\n",
    "                            order=solver_order) # Используем отсортированный порядок\n",
    "\n",
    "            # Рисуем точки поверх (stripplot)\n",
    "            g.map_dataframe(sns.stripplot, x=\"length\", y=\"solver_wrapped\",\n",
    "                            color=\"black\", alpha=0.4, size=3, orient='h', jitter=0.15,\n",
    "                            order=solver_order) # Тот же порядок\n",
    "\n",
    "            # Настройка графиков\n",
    "            g.fig.suptitle('Длины решений', y=1.03, fontsize=16)\n",
    "            g.set_titles(\"N = {col_name:.0f}\")\n",
    "            g.set_axis_labels(\"Длина Найденного Тура\", \"Алгоритм\")\n",
    "\n",
    "            # Улучшаем читаемость оси Y\n",
    "            for ax in g.axes.flat:\n",
    "                ax.tick_params(axis='y', labelsize=9)\n",
    "                # Опционально: добавить линии средних/медиан, если нужно\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc22d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 11. ВИЗУАЛИЗАЦИЯ ВРЕМЯ vs КАЧЕСТВО (SCATTER PLOT)\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from adjustText import adjust_text\n",
    "\n",
    "print(\"\\nGenerating Time vs Quality Scatter Plot...\")\n",
    "\n",
    "# --- Преобразуем aggregated_results в DataFrame ---\n",
    "scatter_data_list = []\n",
    "# Имена методов и ключи (те же, что и раньше)\n",
    "method_keys_for_plot = ['pure_bc', 'gnn_bs_v1', 'gnn_bs_v2', 'gnn_bc_v1', 'gnn_bc_v2']\n",
    "method_display_names = {\n",
    "    'pure_bc': 'Pure B&C', 'gnn_bs_v1': 'GNN+BS (v1)', 'gnn_bs_v2': 'GNN+BS (v2)',\n",
    "    'gnn_bc_v1': 'GNN+B&C (v1)', 'gnn_bc_v2': 'GNN+B&C (v2)'\n",
    "}\n",
    "\n",
    "# Получаем оптимальную длину (если она есть и консистентна)\n",
    "optimal_len = aggregated_results.get('pure_bc', {}).get('avg_tour_lengths', np.nan)\n",
    "\n",
    "for method_key in method_keys_for_plot:\n",
    "    if method_key in aggregated_results:\n",
    "        res = aggregated_results[method_key]\n",
    "        avg_len = res.get('avg_tour_lengths', np.nan)\n",
    "        avg_time = res.get('avg_solving_times', np.nan)\n",
    "        display_name = method_display_names.get(method_key, method_key)\n",
    "\n",
    "        # Рассчитываем Gap (%)\n",
    "        gap = np.nan\n",
    "        if not np.isnan(optimal_len) and optimal_len > 0 and not np.isnan(avg_len):\n",
    "            gap = (avg_len - optimal_len) / optimal_len * 100\n",
    "\n",
    "        scatter_data_list.append({\n",
    "            'Method': display_name,\n",
    "            'Average Time (s)': avg_time,\n",
    "            'Average Length': avg_len,\n",
    "            'Gap (%)': gap\n",
    "        })\n",
    "\n",
    "scatter_df = pd.DataFrame(scatter_data_list)\n",
    "\n",
    "if scatter_df.empty or scatter_df[['Average Time (s)', 'Gap (%)']].dropna().empty:\n",
    "    print(\"Error: Not enough valid data to generate scatter plot.\")\n",
    "else:\n",
    "    print(\"\\nDataFrame for Scatter Plot:\")\n",
    "    print(scatter_df)\n",
    "\n",
    "    # --- Построение графика ---\n",
    "    plt.figure(figsize=(12, 8)) # Можно сделать чуть больше\n",
    "    ax = plt.gca() # Получаем оси для передачи в adjust_text\n",
    "\n",
    "    # Используем seaborn scatterplot\n",
    "    scatter_plot = sns.scatterplot(\n",
    "        data=scatter_df,\n",
    "        x='Average Time (s)',\n",
    "        y='Gap (%)',\n",
    "        hue='Method',\n",
    "        style='Method',\n",
    "        s=220,\n",
    "        palette='viridis',\n",
    "        legend='full', # Оставим легенду на случай, если adjust_text не сработает\n",
    "        ax=ax # Указываем оси\n",
    "    )\n",
    "\n",
    "    texts = []\n",
    "    for i in range(scatter_df.shape[0]):\n",
    "         row = scatter_df.iloc[i]\n",
    "         # Проверяем, что координаты валидны перед созданием текста\n",
    "         if not pd.isna(row['Average Time (s)']) and not pd.isna(row['Gap (%)']):\n",
    "             texts.append(ax.text(row['Average Time (s)'], row['Gap (%)'], row['Method'], fontsize=11))\n",
    "\n",
    "    # Вызываем adjust_text для автоматического размещения\n",
    "    if texts:\n",
    "        adjust_text(texts, ax=ax, # Передаем оси\n",
    "                    # Опции для настройки расталкивания:\n",
    "                    expand_points=(1.2, 1.2), # Увеличить расстояние от точек\n",
    "                    force_points=(0.2, 0.2), # Сила отталкивания от точек\n",
    "                    force_text=(0.3, 0.5),    # Сила отталкивания текстов друг от друга\n",
    "                    arrowprops=dict(arrowstyle=\"-\", color='gray', lw=0.5, alpha=0.7) # Стрелки к точкам\n",
    "                   )\n",
    "    else:\n",
    "        print(\"No valid points found to label.\")\n",
    "\n",
    "    # Настройка осей и заголовка\n",
    "    plt.title('Компромисс Время-Качество для Разных Методов TSP', fontsize=16)\n",
    "    plt.xlabel('Среднее Время Решения (s)')\n",
    "    plt.ylabel('Среднее Отклонение от Оптимума (%)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.axhline(0, color='grey', linestyle=':', linewidth=1)\n",
    "\n",
    "    # Опционально: логарифмическая шкала\n",
    "    # plt.xscale('log')\n",
    "    # plt.xlabel('Среднее Время Решения (s, log scale)')\n",
    "\n",
    "    # Перемещаем легенду seaborn, если она перекрывает что-то\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    if handles: # Проверяем, есть ли что показывать в легенде\n",
    "         # Убираем 'Method' из заголовка легенды, если он есть\n",
    "         if labels[0].lower() == 'method':\n",
    "              handles = handles[1:]\n",
    "              labels = labels[1:]\n",
    "         ax.legend(handles=handles, labels=labels, title=\"Методы\", bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1]) # Оставляем место справа для легенды\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d50a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Generate test instances\n",
    "test_instances = generate_test_instances(num_instances=5, num_nodes=20) # Можете изменить количество\n",
    "\n",
    "# --- ЗАПУСК ТЕСТОВ С РАЗНЫМИ СТРАТЕГИЯМИ GNN ---\n",
    "\n",
    "# --- Тест 1: GNN с отключенной фиксацией ---\n",
    "print(\"\\n===== ТЕСТ 1: GNN-guided B&C (Fixing OFF, Elim ON) =====\")\n",
    "gnn_solver_elim_only = GNNBranchCutSolver(\n",
    "    gnn_model=net,\n",
    "    threshold=0.6,          # Порог для фиксации здесь не используется\n",
    "    fixing_percentage=0.0,  # <--- ОТКЛЮЧАЕМ ФИКСАЦИЮ\n",
    "    elimination_percentage=0.2 # Оставляем удаление 20%\n",
    ")\n",
    "print(\"Comparing Pure B&C with GNN-guided (Elimination Only)...\")\n",
    "results_elim_only = TSPComparison.compare_methods(gnn_solver_elim_only, test_instances)\n",
    "\n",
    "print(\"\\nResults Summary (Elimination Only):\")\n",
    "print(f\"Pure Branch & Cut - Avg Tour Length: {results_elim_only['pure_bc']['avg_tour_lengths']:.4f}, Avg Time: {results_elim_only['pure_bc']['avg_solving_times']:.4f}s\")\n",
    "print(f\"GNN (Elim Only) - Avg Tour Length: {results_elim_only['gnn_bc']['avg_tour_lengths']:.4f}, Avg Time: {results_elim_only['gnn_bc']['avg_solving_times']:.4f}s\")\n",
    "print(\"---------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# --- Тест 2: GNN с очень осторожной фиксацией ---\n",
    "print(\"\\n===== ТЕСТ 2: GNN-guided B&C (Conservative Fixing, Elim ON) =====\")\n",
    "gnn_solver_conservative_fix = GNNBranchCutSolver(\n",
    "    gnn_model=net,\n",
    "    threshold=0.95,         # <--- ОЧЕНЬ ВЫСОКИЙ ПОРОГ\n",
    "    fixing_percentage=0.1,  # <--- ФИКСИРУЕМ МЕНЬШИЙ ПРОЦЕНТ (только самые уверенные)\n",
    "    elimination_percentage=0.2 # Оставляем удаление 20%\n",
    ")\n",
    "print(\"Comparing Pure B&C with GNN-guided (Conservative Fixing)...\")\n",
    "# Для этого теста можно включить debug_level=1, чтобы увидеть, сколько ребер реально фиксируется\n",
    "results_conservative_fix = TSPComparison.compare_methods(gnn_solver_conservative_fix, test_instances)\n",
    "\n",
    "print(\"\\nResults Summary (Conservative Fixing):\")\n",
    "print(f\"Pure Branch & Cut - Avg Tour Length: {results_conservative_fix['pure_bc']['avg_tour_lengths']:.4f}, Avg Time: {results_conservative_fix['pure_bc']['avg_solving_times']:.4f}s\")\n",
    "print(f\"GNN (Cons. Fix) - Avg Tour Length: {results_conservative_fix['gnn_bc']['avg_tour_lengths']:.4f}, Avg Time: {results_conservative_fix['gnn_bc']['avg_solving_times']:.4f}s\")\n",
    "print(\"---------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# --- Тест 3: GNN с отключенным удалением ---\n",
    "print(\"\\n===== ТЕСТ 3: GNN-guided B&C (Fixing ON, Elim OFF) =====\")\n",
    "gnn_solver_fix_only = GNNBranchCutSolver(\n",
    "    gnn_model=net,\n",
    "    threshold=0.6,          # Возвращаем исходный порог\n",
    "    fixing_percentage=0.2,  # Возвращаем исходный процент\n",
    "    elimination_percentage=0.0 # <--- ОТКЛЮЧАЕМ УДАЛЕНИЕ\n",
    ")\n",
    "print(\"Comparing Pure B&C with GNN-guided (Fixing Only)...\")\n",
    "# Включаем debug_level=1 для анализа фиксации\n",
    "results_fix_only = TSPComparison.compare_methods(gnn_solver_fix_only, test_instances)\n",
    "\n",
    "print(\"\\nResults Summary (Fixing Only):\")\n",
    "print(f\"Pure Branch & Cut - Avg Tour Length: {results_fix_only['pure_bc']['avg_tour_lengths']:.4f}, Avg Time: {results_fix_only['pure_bc']['avg_solving_times']:.4f}s\")\n",
    "print(f\"GNN (Fix Only)  - Avg Tour Length: {results_fix_only['gnn_bc']['avg_tour_lengths']:.4f}, Avg Time: {results_fix_only['gnn_bc']['avg_solving_times']:.4f}s\")\n",
    "print(\"---------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# --- (Опционально) Исходный вариант для сравнения ---\n",
    "# print(\"\\n===== ИСХОДНЫЙ ВАРИАНТ GNN-guided B&C =====\")\n",
    "# gnn_solver_original = GNNBranchCutSolver(\n",
    "#     gnn_model=net,\n",
    "#     threshold=0.6,\n",
    "#     fixing_percentage=0.2,\n",
    "#     elimination_percentage=0.2\n",
    "# )\n",
    "# print(\"Comparing Pure B&C with Original GNN-guided...\")\n",
    "# results_original = TSPComparison.compare_methods(gnn_solver_original, test_instances, debug_level=0)\n",
    "#\n",
    "# print(\"\\nResults Summary (Original):\")\n",
    "# print(f\"Pure Branch & Cut - Avg Tour Length: {results_original['pure_bc']['avg_tour_lengths']:.4f}, Avg Time: {results_original['pure_bc']['avg_solving_times']:.4f}s\")\n",
    "# print(f\"GNN (Original)  - Avg Tour Length: {results_original['gnn_bc']['avg_tour_lengths']:.4f}, Avg Time: {results_original['gnn_bc']['avg_solving_times']:.4f}s\")\n",
    "# print(\"---------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# --- Анализ и выводы по результатам тестов ---\n",
    "print(\"\\n===== ОБЩИЕ ВЫВОДЫ ПО ТЕСТАМ =====\")\n",
    "# Сравниваем результаты results_elim_only, results_conservative_fix, results_fix_only\n",
    "# (и results_original, если запускали) с результатами Pure B&C из любого из них (они должны быть одинаковы)\n",
    "\n",
    "pure_avg_len = results_elim_only['pure_bc']['avg_tour_lengths']\n",
    "pure_avg_time = results_elim_only['pure_bc']['avg_solving_times']\n",
    "\n",
    "print(f\"Pure B&C:           Avg Len={pure_avg_len:.4f}, Avg Time={pure_avg_time:.4f}s\")\n",
    "\n",
    "elim_only_avg_len = results_elim_only['gnn_bc']['avg_tour_lengths']\n",
    "elim_only_avg_time = results_elim_only['gnn_bc']['avg_solving_times']\n",
    "print(f\"GNN (Elim Only):    Avg Len={elim_only_avg_len:.4f} (Diff: {elim_only_avg_len-pure_avg_len:+.4f}), Avg Time={elim_only_avg_time:.4f}s (Speedup: {pure_avg_time/elim_only_avg_time:.2f}x)\")\n",
    "\n",
    "cons_fix_avg_len = results_conservative_fix['gnn_bc']['avg_tour_lengths']\n",
    "cons_fix_avg_time = results_conservative_fix['gnn_bc']['avg_solving_times']\n",
    "print(f\"GNN (Conserv. Fix): Avg Len={cons_fix_avg_len:.4f} (Diff: {cons_fix_avg_len-pure_avg_len:+.4f}), Avg Time={cons_fix_avg_time:.4f}s (Speedup: {pure_avg_time/cons_fix_avg_time:.2f}x)\")\n",
    "\n",
    "fix_only_avg_len = results_fix_only['gnn_bc']['avg_tour_lengths']\n",
    "fix_only_avg_time = results_fix_only['gnn_bc']['avg_solving_times']\n",
    "print(f\"GNN (Fix Only):     Avg Len={fix_only_avg_len:.4f} (Diff: {fix_only_avg_len-pure_avg_len:+.4f}), Avg Time={fix_only_avg_time:.4f}s (Speedup: {pure_avg_time/fix_only_avg_time:.2f}x)\")\n",
    "\n",
    "# Дополнительные выводы на основе сравнения\n",
    "if elim_only_avg_len <= pure_avg_len and elim_only_avg_time < pure_avg_time:\n",
    "    print(\"\\n-> Стратегия 'Только Удаление' выглядит перспективной: сохраняет качество и ускоряет.\")\n",
    "elif cons_fix_avg_len <= pure_avg_len and cons_fix_avg_time < pure_avg_time:\n",
    "     print(\"\\n-> Стратегия 'Консервативная Фиксация' выглядит перспективной: сохраняет качество и ускоряет.\")\n",
    "elif fix_only_avg_len != float('inf') and fix_only_avg_len > pure_avg_len:\n",
    "     print(\"\\n-> Стратегия 'Только Фиксация' (даже без удаления) приводит к субоптимальным решениям. Фиксация - основная проблема.\")\n",
    "elif elim_only_avg_len > pure_avg_len:\n",
    "     print(\"\\n-> Стратегия 'Только Удаление' ухудшает качество. Возможно, удаляются нужные ребра или GNN плохо их ранжирует.\")\n",
    "\n",
    "print(\"\\nРекомендация: Проанализируйте подробные результаты тестов. Если 'Только Удаление' или 'Консервативная Фиксация' показывают хорошие результаты, используйте их. Если нет - основное внимание на улучшение GNN модели.\")\n",
    "\n",
    "# 8. Visualize comparison (можно выбрать лучшие результаты для визуализации)\n",
    "# Например, сравнить Pure B&C и GNN (Elim Only)\n",
    "# TSPComparison.plot_comparison_results(results_elim_only)\n",
    "\n",
    "# 9. Detailed visualization of one instance (сравнить Pure и лучший GNN вариант)\n",
    "# instance_idx = 0\n",
    "# tour_pure, tour_length_pure, _ , _= TSPComparison.pure_branch_cut_solve(test_instances[instance_idx])\n",
    "# tour_gnn_best, tour_length_gnn_best, _ = gnn_solver_elim_only.solve_tsp(test_instances[instance_idx]) # Пример\n",
    "# TSPComparison.visualize_tours(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diploma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
